#!/usr/bin/env python
import contextlib as __stickytape_contextlib

@__stickytape_contextlib.contextmanager
def __stickytape_temporary_dir():
    import tempfile
    import shutil
    dir_path = tempfile.mkdtemp()
    try:
        yield dir_path
    finally:
        shutil.rmtree(dir_path)

with __stickytape_temporary_dir() as __stickytape_working_dir:
    def __stickytape_write_module(path, contents):
        import os, os.path

        def make_package(path):
            parts = path.split("/")
            partial_path = __stickytape_working_dir
            for part in parts:
                partial_path = os.path.join(partial_path, part)
                if not os.path.exists(partial_path):
                    os.mkdir(partial_path)
                    with open(os.path.join(partial_path, "__init__.py"), "wb") as f:
                        f.write(b"\n")

        make_package(os.path.dirname(path))

        full_path = os.path.join(__stickytape_working_dir, path)
        with open(full_path, "wb") as module_file:
            module_file.write(contents)

    import sys as __stickytape_sys
    __stickytape_sys.path.insert(0, __stickytape_working_dir)

    __stickytape_write_module('range_analysis_app/__init__.py', b'')
    __stickytape_write_module('range_analysis_app/components/__init__.py', b'')
    __stickytape_write_module('range_analysis_app/components/streamlit_nested_layout.py', b'# Implementation from streamlit-nested-layout => https://github.com/joy13975/streamlit-nested-layout\n# This package is not available in the Snowflake Anaconda Channel => https://repo.anaconda.com/pkgs/snowflake/\n\nfrom streamlit.delta_generator import *\nfrom streamlit.delta_generator import _enqueue_message\n\n\ndef _nestable_block(\n    self: "DeltaGenerator",\n    block_proto: Block_pb2.Block = Block_pb2.Block(),\n) -> "DeltaGenerator":\n    # Operate on the active DeltaGenerator, in case we\'re in a `with` block.\n    dg = self._active_dg\n\n    # Prevent nested columns & expanders by checking all parents.\n    block_type = block_proto.WhichOneof("type")\n    # Convert the generator to a list, so we can use it multiple times.\n    # parent_block_types = frozenset(dg._parent_block_types)\n    # if block_type == "column" and block_type in parent_block_types:\n    #     raise StreamlitAPIException(\n    #         "Columns may not be nested inside other columns."\n    #     )\n    # if block_type == "expandable" and block_type in parent_block_types:\n    #     raise StreamlitAPIException(\n    #         "Expanders may not be nested inside other expanders."\n    #     )\n\n    if dg._root_container is None or dg._cursor is None:\n        return dg\n\n    msg = ForwardMsg_pb2.ForwardMsg()\n    msg.metadata.delta_path[:] = dg._cursor.delta_path\n    msg.delta.add_block.CopyFrom(block_proto)\n\n    # Normally we\'d return a new DeltaGenerator that uses the locked cursor\n    # below. But in this case we want to return a DeltaGenerator that uses\n    # a brand new cursor for this new block we\'re creating.\n    block_cursor = cursor.RunningCursor(\n        root_container=dg._root_container,\n        parent_path=dg._cursor.parent_path + (dg._cursor.index,),\n    )\n    block_dg = DeltaGenerator(\n        root_container=dg._root_container,\n        cursor=block_cursor,\n        parent=dg,\n        block_type=block_type,\n    )\n    # Blocks inherit their parent form ids.\n    # NOTE: Container form ids aren\'t set in proto.\n    block_dg._form_data = FormData(current_form_id(dg))\n\n    # Must be called to increment this cursor\'s index.\n    dg._cursor.get_locked_cursor(last_index=None)\n    _enqueue_message(msg)\n\n    caching.save_block_message(\n        block_proto,\n        invoked_dg_id=self.id,\n        used_dg_id=dg.id,\n        returned_dg_id=block_dg.id,\n    )\n\n    return block_dg\n\n\nDeltaGenerator._block = _nestable_block\n')
    __stickytape_write_module('app_pages/__init__.py', b'')
    __stickytape_write_module('app_pages/about_page.py', b'import streamlit as st\n\nfrom app_pages.page import Page\n\n\nclass AboutPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "About Maxa"\n\n    def run(self) -> None:\n        st.markdown(\n            """\n            # Maxa\n            \n            https://www.maxa.ai\n            """\n        )\n')
    __stickytape_write_module('app_pages/page.py', b'import abc\n\n\nclass Page(abc.ABC):\n    @property\n    @abc.abstractmethod\n    def page_description(self) -> str:\n        pass\n\n    @abc.abstractmethod\n    def run(self) -> None:\n        pass\n')
    __stickytape_write_module('app_pages/dashboard.py', b'import streamlit as st\n\nfrom app_pages.page import Page\nfrom data.helpers import repository\nfrom utils import chunks\n\n\nclass DashboardPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "Dashboard of saved visualizations."\n\n    def run(self) -> None:\n        repo = repository()\n\n        if not repo.visualizations:\n            st.warning("Add visualizations to the dashboard in the Business Use Cases page.")\n            return\n\n        for pair in chunks(list(enumerate(repo.visualizations.items())), 2):\n            for (index, (viz_key, visualization)), col in zip(pair, st.columns([6, 6])):\n                with col:\n                    action_buttons(index, viz_key)\n                    visualization.visualize()\n\n\ndef action_buttons(index: int, viz_key: str):\n    """Action buttons: Remove, Move-up, Move-down."""\n    repo = repository()\n    cols = st.columns([1] * 12)\n\n    last_col = cols[-1].empty()\n\n    if not last_col.button("\xe2\x80\xa6", help="Open menu", key=f"open-menu-{index}", use_container_width=True):\n        return\n\n    if last_col.button("\xe2\x9c\x95", help="Close menu", key=f"close-menu-{index}", use_container_width=True):\n        return\n\n    def remove_viz(key: int) -> None:\n        return lambda: repo.visualizations.pop(key)\n\n    cols[-2].button(\n        "\xf0\x9f\x97\x91\xef\xb8\x8f",\n        help="Remove",\n        key=f"remove-{index}-{viz_key}",\n        on_click=remove_viz(viz_key),\n        use_container_width=True,\n    )\n\n    def move_viz(key: int, index: int) -> None:\n        return lambda: move_visualization(key, index)\n\n    cols[-4].button(\n        "\xe2\xac\x86\xef\xb8\x8f" if index % 2 == 0 else "\xe2\xac\x85\xef\xb8\x8f",\n        help="Move up" if index % 2 == 0 else "Move left",\n        key=f"move-up-{index}-{viz_key}",\n        on_click=move_viz(viz_key, index - 1),\n        disabled=index == 0,\n        use_container_width=True,\n    )\n    cols[-3].button(\n        "\xe2\xac\x87\xef\xb8\x8f" if index % 2 == 1 else "\xe2\x9e\xa1\xef\xb8\x8f",\n        help="Move down" if index % 2 == 1 else "Move right",\n        key=f"move-down-{index}-{viz_key}",\n        on_click=move_viz(viz_key, index + 1),\n        disabled=index == len(repo.visualizations) - 1,\n        use_container_width=True,\n    )\n\n\ndef move_visualization(key: int, index: int) -> None:\n    """Move the visualization with the given key to the given index."""\n    visualizations = repository().visualizations\n    viz = visualizations.pop(key)\n\n    keys = list(visualizations.keys())\n    values = list(visualizations.values())\n\n    keys.insert(index, key)\n    values.insert(index, viz)\n\n    repository().visualizations = dict(zip(keys, values))\n')
    __stickytape_write_module('data/__init__.py', b'')
    __stickytape_write_module('data/helpers.py', b'from typing import TYPE_CHECKING, List, Optional, Tuple\n\nimport streamlit as st\nfrom inflection import pluralize as inflection_pluralize\nfrom snowflake.snowpark.exceptions import SnowparkSQLException\nfrom snowflake.snowpark.types import StringType, StructField, StructType\n\nfrom range_analysis_app.db import snowflake_session\nfrom utils import pluralize\n\nif TYPE_CHECKING:\n    from data.models import DomaBaseModel\n    from data.repository import Repository\n\n\ndef create_table_if_not_exists() -> None:\n    session = snowflake_session()\n    try:\n        session.table("doma_entities").collect()\n    except SnowparkSQLException as e:\n        schema = StructType([StructField("json_data", StringType())])\n        df = session.create_dataframe([], schema=schema)\n        df.write.save_as_table("doma_entities", mode="overwrite")\n\n\ndef repository() -> "Repository":\n    if "repository" not in st.session_state:\n        from data.repository import Repository\n\n        create_table_if_not_exists()\n        row = snowflake_session().table("doma_entities").first()\n        if row is None:\n            st.session_state.repository = Repository()\n        else:\n            data = row.JSON_DATA\n            st.session_state["prev_repository_json"] = data\n            st.session_state.repository = Repository.parse_raw(data)\n\n    return st.session_state.repository\n\n\ndef save_repo_button(next_page: Optional[str] = None) -> None:\n    """Component for saving the repository and going to the next page."""\n    raw_json = repository().json()\n    prev_raw_json = st.session_state.get("prev_repository_json")\n\n    if prev_raw_json == raw_json:\n        return\n\n    message_col, btn_col = st.columns([10, 2])\n\n    with btn_col:\n        st.markdown("")\n        placeholder = st.empty()\n\n    if placeholder.button("Save"):\n        placeholder.empty()\n        with message_col, st.spinner():\n            schema = StructType([StructField("json_data", StringType())])\n            (\n                snowflake_session()\n                .create_dataframe([raw_json], schema=schema)\n                .write.save_as_table("doma_entities", mode="overwrite")\n            )\n\n        st.session_state["prev_repository_json"] = raw_json\n\n        if next_page:\n            message_col.info(f"Saved. Please go to next page {next_page}.", icon="\xe2\x9c\x94\xef\xb8\x8f")\n        else:\n            message_col.info("Saved.", icon="\xe2\x9c\x94\xef\xb8\x8f")\n\n    else:\n        message_col.warning("You have unsaved changes.", icon="\xe2\x9a\xa0\xef\xb8\x8f")\n\n\ndef validate_state(\n    models: List["DomaBaseModel"],\n    entity_name: str,\n    permit_empty: bool = False,\n    validate_each: bool = True,\n) -> Tuple[bool, str]:\n    """Return a pair (is_valid, message) describing the validation state of the given models."""\n\n    if len(models) == 0 and not permit_empty:\n        return False, f"{entity_name} cannot be empty"\n\n    if validate_each:\n        try:\n            success_message = [f"{pluralize(len(models), entity_name)} has been set"]\n            for model in models:\n                model.validate_entity()\n                success_message.append(f"- {model.validation_item_name()}")\n\n            return True, "\\n".join(success_message)\n        except ValueError as e:\n            return False, str(e)\n\n    success_message = f"All set for the {inflection_pluralize(entity_name)}"\n    return True, "\\n".join([success_message] + [f"- {x.validation_item_name()}" for x in models])\n\n\ndef render_validation_component(is_valid: bool, message: str):\n    if is_valid:\n        st.success(message, icon="\xe2\x9c\x94\xef\xb8\x8f")\n    else:\n        st.error(message, icon="\xe2\x9c\x96\xef\xb8\x8f")\n')
    __stickytape_write_module('range_analysis_app/db.py', b'import json\nfrom typing import Dict, Iterable, List, Literal, TypedDict, Union, get_args\n\nimport pandas as pd\nimport snowflake.snowpark.functions as f\nimport streamlit as st\nfrom pydantic import Field\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.exceptions import SnowparkSessionException\nfrom typing_extensions import NotRequired\n\nfrom utils import BaseModelCacheKey\n\n\n@st.cache_resource\ndef snowflake_session() -> Session:\n    try:\n        return get_active_session()\n    except SnowparkSessionException:\n        return Session.builder.configs(st.secrets["snowflake"]).create()\n\n\n@st.cache_data\ndef fetch_tables(schema: str = "") -> List[str]:\n    return [row.name for row in snowflake_session().sql(f"show tables in schema {schema}").collect()]\n\n\nDatetimeType = Literal["DATE", "TIME", "TIMESTAMP", "TIMESTAMP_LTZ", "TIMESTAMP_NTZ"]\nDataType = Union[Literal["TEXT", "FIXED", "REAL", "BINARY", "BOOLEAN", "ARRAY", "OBJECT"], DatetimeType]\nExtendedDataType = Union[DataType, Literal["INTEGER"]]\n\nDATETIME_TYPES: Iterable[DatetimeType] = get_args(DatetimeType)\nID_TYPES: Iterable[ExtendedDataType] = ["TEXT", "INTEGER", "BINARY", "ARRAY"]\n\n\n@st.cache_data\ndef fetch_columns(table: str) -> Dict[str, DataType]:\n    return {\n        row.column_name: parse_data_type(json.loads(row.data_type))\n        for row in snowflake_session().sql(f"show columns in table {table}").collect()\n    }\n\n\nclass DataTypeDict(TypedDict):\n    """Represent the `data_type` value in Snowflake `SHOW COLUMNS IN TABLE`."""\n\n    type: DataType\n    nullable: bool\n\n    # for type FIXED and TIME/TIMESTAMP\n    precision: NotRequired[int]\n    scale: NotRequired[int]\n\n    # for type TEXT and BINARY\n    length: NotRequired[int]\n    bytesLength: NotRequired[int]\n    fixed: NotRequired[bool]\n\n\ndef parse_data_type(data_type: DataTypeDict) -> ExtendedDataType:\n    if data_type["type"] == "FIXED" and data_type["scale"] == 0:\n        return "INTEGER"\n\n    return data_type["type"]\n\n\n@st.cache_data\ndef preview_table(table: str, limit: int = 5) -> pd.DataFrame:\n    return pd.DataFrame(snowflake_session().table(table).limit(limit).collect())\n\n\nclass Filters(BaseModelCacheKey):\n    mapping: Dict[str, List[str]] = Field(default_factory=dict)\n\n    def add(self, key: str, values: List[str]) -> None:\n        self.mapping[key] = values\n\n    def to_condition(self) -> f.Column:\n        """Return a Snowpark filter condition to be used in the `DataFrame.filter` or `DataFrame.where` methods."""\n        condition = f.lit(True)\n        for column, values in self.mapping.items():\n            condition &= f.col(column).in_(values)\n        return condition\n\n    def is_empty(self) -> bool:\n        return len(self.mapping) == 0\n\n\ndef drop_temporary_tables() -> None:\n    for table in fetch_tables.execute_without_cache():\n        if table.startswith("TMP_"):\n            snowflake_session().table(table).drop_table()\n\n    fetch_tables.clear_cache()\n')
    __stickytape_write_module('utils.py', b'from collections import defaultdict\nfrom typing import Callable, Dict, Iterable, List, Optional, Tuple, Type, TypeVar\n\nimport streamlit as st\nfrom inflection import pluralize as inflection_pluralize\nfrom pydantic import BaseModel\n\nT = TypeVar("T")\nK = TypeVar("K")\n\n\ndef pluralize(count: int, singular: str) -> str:\n    return f"{count} {inflection_pluralize(singular) if count > 1 else singular}"\n\n\ndef chunks(values: List[T], chunk_size: int) -> Iterable[List[T]]:\n    """Split the given values into multiple chunks of the given size."""\n    for index in range(0, len(values), chunk_size):\n        yield values[index : index + chunk_size]\n\n\nclass BaseModelCacheKey(BaseModel):\n    """Model that can be used as caching key in st.cache_data."""\n\n    def __reduce__(self) -> Tuple[Type, Tuple[str, ...]]:\n        """Function from pickle used by the hasher of st.cache_data."""\n        return (self.parse_raw, (self.json(),))\n\n\ndef groupby(values: List[T], key: Callable[[T], K]) -> Dict[K, List[T]]:\n    result = defaultdict(list)\n\n    for value in values:\n        result[key(value)].append(value)\n\n    return result\n')
    __stickytape_write_module('data/models.py', b'from abc import ABC, abstractmethod\nfrom typing import TYPE_CHECKING, Dict, List, Literal, NewType, Optional, Union\nfrom uuid import uuid4\n\nfrom pydantic import Field\n\nfrom data.helpers import repository\nfrom range_analysis_app import db\nfrom range_analysis_app.db import snowflake_session\nfrom utils import BaseModelCacheKey\n\nif TYPE_CHECKING:\n    from data.metric import Metric\n\n\nId = NewType("Id", str)\n\nEntityType = Literal["dimension", "activity", "metric"]\n\n\ndef get_uuid4() -> Id:\n    return str(uuid4())\n\n\nclass DomaBaseModel(BaseModelCacheKey, ABC):\n    @abstractmethod\n    def validate_entity(self) -> None:\n        """Raise ValueError if the current entity is invalid."""\n\n    @abstractmethod\n    def validation_item_name(self) -> str:\n        """Name of the item to be validated"""\n\n\nclass Entity(DomaBaseModel, ABC):\n    uid: Id = Field(default_factory=get_uuid4)\n    name: str\n\n    @classmethod\n    @abstractmethod\n    def get_type(cls) -> EntityType:\n        ...\n\n    @abstractmethod\n    def delete(self) -> None:\n        ...\n\n    def validate_entity(self) -> None:\n        if not bool(self.name):\n            raise ValueError(f"An entity name must be set.")\n\n    def validation_item_name(self) -> str:\n        return self.name\n\n\nclass DimensionActivityMapping(DomaBaseModel):\n    dim_id: Id\n    act_id: Id\n\n    main_table_id: Optional[Id]\n    joined_table_id: Optional[Id]\n\n    main_column: Optional[str]\n    joined_column: Optional[str]\n\n    @property\n    def dimension(self) -> Optional["Dimension"]:\n        return repository().get_entity("dimension", self.dim_id)\n\n    @property\n    def activity(self) -> Optional["Activity"]:\n        return repository().get_entity("activity", self.act_id)\n\n    @property\n    def main_table(self) -> Optional["DbTable"]:\n        return self._find_table(self.main_table_id)\n\n    @property\n    def joined_table(self) -> Optional["DbTable"]:\n        return self._find_table(self.joined_table_id)\n\n    @staticmethod\n    def _find_table(uid: Id) -> Optional["DbTable"]:\n        for table in repository().selected_tables:\n            if uid == table.uid:\n                return table\n\n    def validate_entity(self) -> None:\n        if not bool(self.main_table_id) or not bool(self.main_column):\n            raise ValueError(f"A main table must be set.")\n\n        if not bool(self.joined_table_id) or not bool(self.joined_column):\n            raise ValueError(f"A joined table must be set.")\n\n    def validation_item_name(self) -> str:\n        return f"{self.dimension.name} <=> {self.activity.name}"\n\n\nclass Dimension(Entity):\n    @classmethod\n    def get_type(cls) -> EntityType:\n        return "dimension"\n\n    @staticmethod\n    def create(name: str) -> "Dimension":\n        dim = Dimension(name=name)\n        repository().dimensions.append(dim)\n        return dim\n\n    def delete(self) -> None:\n        repo = repository()\n        for activity in self.activities:\n            activity.unassign_dimension(self)\n\n        if table := self.db_table:\n            table.delete()\n\n        repo.dimensions.remove(self)\n\n    @property\n    def activities(self) -> List["Activity"]:\n        act_ids = [mapping.act_id for mapping in repository().dim_act_mappings if mapping.dim_id == self.uid]\n        return [act for act in repository().activities if act.uid in act_ids]\n\n    @property\n    def db_table(self) -> Optional["DbTable"]:\n        for table in repository().selected_tables:\n            if table.entity_uid == self.uid and table.entity_type == "dimension":\n                return table\n\n    def validate_entity(self) -> None:\n        super().validate_entity()\n\n        if self.db_table is None:\n            raise ValueError(f"Missing table for dimension {self.name}")\n\n\nclass Activity(Entity):\n    @classmethod\n    def get_type(cls) -> EntityType:\n        return "activity"\n\n    @staticmethod\n    def create(name: str) -> "Activity":\n        act = Activity(name=name)\n        repository().activities.append(act)\n        return act\n\n    def delete(self) -> None:\n        for dimension in self.dimensions:\n            self.unassign_dimension(dimension)\n\n        if table := self.db_table:\n            table.delete()\n\n        for metric in self.metrics:\n            metric.delete()\n\n        repository().activities.remove(self)\n\n    @property\n    def dimensions(self) -> List[Dimension]:\n        dim_ids = [mapping.dim_id for mapping in repository().dim_act_mappings if mapping.act_id == self.uid]\n        return [dim for dim in repository().dimensions if dim.uid in dim_ids]\n\n    def assign_dimension(self, dim_id: Id) -> None:\n        mapping = DimensionActivityMapping(act_id=self.uid, dim_id=dim_id)\n        repository().dim_act_mappings.append(mapping)\n\n    def unassign_dimension(self, dim: Dimension) -> None:\n        repository().dim_act_mappings = [\n            mapping\n            for mapping in repository().dim_act_mappings\n            if mapping.act_id != self.uid or mapping.dim_id != dim.uid\n        ]\n        for metric in self.metrics:\n            metric.unassign_dimension(dim)\n\n    @property\n    def metrics(self) -> List["Metric"]:\n        return [metric for metric in repository().metrics if metric.act_id == self.uid]\n\n    @property\n    def mappings(self) -> List[DimensionActivityMapping]:\n        return [mapping for mapping in repository().dim_act_mappings if mapping.act_id == self.uid]\n\n    @property\n    def db_table(self) -> Optional["DbTable"]:\n        for table in repository().selected_tables:\n            if table.entity_uid == self.uid and table.entity_type == "activity":\n                return table\n\n    def validate_tables_and_mapping(self) -> List[str]:\n        """Return a list of error messages for each missing tables or mappings in the activity."""\n        errors = []\n        if self.db_table is None:\n            errors.append(f"Missing table for activity {self.name}")\n\n        for dim in self.dimensions:\n            if dim.db_table is None:\n                errors.append(f"Missing table for dimension {dim.name}")\n\n        for mapping in self.mappings:\n            try:\n                mapping.validate_entity()\n            except ValueError:\n                errors.append(f"Missing mapping: {mapping.validation_item_name()}")\n\n        return errors\n\n\nclass DbTable(DomaBaseModel):\n    uid: Id = Field(default_factory=get_uuid4)\n\n    table_name: str\n    table_schema: str\n    table_database: str\n\n    entity_uid: Id\n    entity_type: EntityType\n\n    columns: Dict[str, str] = {}\n\n    @staticmethod\n    def create(\n        table_name: str,\n        table_schema: str,\n        table_database: str,\n        columns: Dict[str, str],\n        entity: Entity,\n    ) -> "DbTable":\n        entity_uid = entity.uid\n        entity_type = entity.get_type()\n        tbl = DbTable(\n            table_name=table_name,\n            table_schema=table_schema,\n            table_database=table_database,\n            columns=columns,\n            entity_uid=entity_uid,\n            entity_type=entity_type,\n        )\n        repository().selected_tables.append(tbl)\n        return tbl\n\n    @property\n    def full_name(self) -> str:\n        return f"{self.table_database}.{self.table_schema}.{self.table_name}"\n\n    @property\n    def entity(self) -> Union[Activity, Dimension]:\n        return repository().get_entity(self.entity_type, self.entity_uid)\n\n    def delete(self) -> None:\n        repository().selected_tables.remove(self)\n        repository().dim_act_mappings = [\n            mapping\n            for mapping in repository().dim_act_mappings\n            if self.uid in [mapping.main_table_id, mapping.joined_table_id]\n        ]\n\n        # Remove table columns from metrics\n        entity = self.entity\n\n        if isinstance(entity, Activity):\n            for metric in entity.metrics:\n                metric.columns = None\n\n        if isinstance(entity, Dimension):\n            for activity in entity.activities:\n                for metric in activity.metrics:\n                    metric.unassign_dimension(entity)\n\n    def validate_entity(self) -> None:\n        if snowflake_session().table(self.full_name).first() is None:\n            raise ValueError(f"Table \'{self.full_name}\' is empty or does not exist.")\n\n    def validation_item_name(self) -> str:\n        return self.full_name\n\n    def columns_of_type(self, data_types: List[db.DataType]) -> List[str]:\n        """Return the columns having one of the given types."""\n        return [col for col, data_type in self.columns.items() if data_type in data_types]\n')
    __stickytape_write_module('data/metric.py', b'from typing import Dict, List, Literal, Optional, Set, get_args\n\nimport pandas as pd\nimport snowflake.snowpark.functions as f\nimport streamlit as st\nfrom pydantic import Field\nfrom snowflake import snowpark\n\nfrom data.helpers import repository\nfrom data.models import Activity, Dimension, DimensionActivityMapping, Entity, EntityType, Id\nfrom range_analysis_app import db\nfrom std_graphlib import TopologicalSorter\nfrom utils import BaseModelCacheKey, groupby\n\nAggMethod = Literal["sum", "count", "avg"]\nAGG_METHODS = get_args(AggMethod)\n\n\nclass MetricColumns(BaseModelCacheKey):\n    dimensions: List[str]\n    metric_column: str\n    timestamp: str\n    agg_method: Literal["sum", "count", "avg"]\n\n    labels: Dict[str, str] = Field(default_factory=dict)\n\n    def remove_dimension_columns(self, dim_columns: Set[str]) -> None:\n        for dim_col in set(self.dimensions) & dim_columns:\n            self.dimensions.remove(dim_col)\n            self.labels.pop(dim_col, None)\n\n\nclass Metric(Entity):\n    act_id: Optional[Id]\n    columns: Optional[MetricColumns]\n\n    @classmethod\n    def get_type(cls) -> EntityType:\n        return "metric"\n\n    def validate_entity(self) -> None:\n        if not bool(self.columns):\n            raise ValueError(f"Metric columns must be set.")\n\n    @staticmethod\n    def create(name: str, act_id: Optional[str] = None) -> "Metric":\n        met = Metric(name=name, act_id=act_id)\n        repository().metrics.append(met)\n        return met\n\n    def delete(self) -> None:\n        repository().metrics.remove(self)\n\n    @property\n    def activity(self) -> Optional[Activity]:\n        return repository().get_entity("activity", self.act_id)\n\n    def unassign_dimension(self, dimension: Dimension):\n        dim_table = dimension.db_table\n        if not self.columns or not dim_table:\n            return\n\n        dim_columns = set(dim_table.columns.keys())\n        self.columns.remove_dimension_columns(dim_columns)\n\n    def dataframe(self) -> snowpark.DataFrame:\n        """Return a dataframe based on the metric activity joined with its dimensions."""\n        if not self.columns:\n            raise ValueError(f"No columns in metric {self.name}.")\n\n        if self.activity.validate_tables_and_mapping():\n            raise ValueError(f"Missing tables and mappings in activity {self.activity.name}.")\n\n        mappings = sorted_mappings(self.activity.mappings)\n\n        snowpark_tables: Dict[str, snowpark.Table] = {}\n        for mapping in mappings:\n            for table in [mapping.main_table, mapping.joined_table]:\n                if table.uid not in snowpark_tables:\n                    snowpark_tables[table.uid] = db.snowflake_session().table(table.full_name)\n\n        act_table = snowpark_tables[self.activity.db_table.uid]\n\n        result_dataframe = act_table\n\n        for mapping in sorted_mappings(self.activity.mappings):\n            main_table = snowpark_tables[mapping.main_table_id]\n            joined_table = snowpark_tables[mapping.joined_table_id]\n\n            result_dataframe = result_dataframe.join(\n                joined_table,\n                how="left",\n                on=main_table.col(mapping.main_column) == joined_table.col(mapping.joined_column),\n            )\n\n        def col_ref(column_name: str) -> snowpark.Column:\n            """Return a reference for the column in one of the given tables eg. my_table.my_column"""\n            for table in snowpark_tables.values():\n                if column_name in table.columns:\n                    return table.col(column_name)\n\n        cols = self.columns\n\n        return result_dataframe.select(\n            [\n                *(col_ref(col).alias(col) for col in cols.dimensions),\n                f.to_date(col_ref(cols.timestamp)).alias(cols.timestamp),\n                col_ref(cols.metric_column).alias(cols.metric_column),\n            ]\n        )\n\n    @st.cache_data\n    def dimension_values(self, dimension: str, limit: int = 1000) -> List[str]:\n        assert dimension in self.columns.dimensions\n\n        rows = (\n            self.dataframe()\n            .group_by(dimension)\n            .agg(f.sum(self.columns.metric_column).alias("total"))\n            .sort(f.col("total").desc_nulls_last())\n            .limit(limit)\n            .collect()\n        )\n        return [row[dimension.upper()] for row in rows if row[dimension.upper()] is not None]\n\n    @st.cache_data\n    def preview_dataframe(self, limit: int = 10) -> pd.DataFrame:\n        data = pd.DataFrame(self.dataframe().limit(limit).collect())\n\n        columns = [self.columns.metric_column] + self.columns.dimensions\n        renames = {col: f"{col} ({label})" for col, label in self.columns.labels.items()}\n        return data.set_index(self.columns.timestamp)[columns].rename(columns=renames)\n\n\ndef sorted_mappings(mappings: List[DimensionActivityMapping]) -> List[DimensionActivityMapping]:\n    """Return the mappings sorted by the order in which tables must be joined in SQL."""\n    # A graph is a dict of { node => preceding nodes }, so here we have { table to join => tables to load beforehand }\n    tables_graph = {\n        joined_table_id: [mapping.main_table_id for mapping in mappings]\n        for joined_table_id, mappings in groupby(mappings, key=lambda mapping: mapping.joined_table_id).items()\n    }\n    sorted_table_ids = list(TopologicalSorter(tables_graph).static_order())\n\n    def sort_by_main_table(mapping: DimensionActivityMapping) -> int:\n        return sorted_table_ids.index(mapping.main_table_id)\n\n    return sorted(mappings, key=sort_by_main_table)\n')
    __stickytape_write_module('std_graphlib.py', b'# Backport of the `graphlib` library that is available in python 3.9:\n# => https://docs.python.org/3/library/graphlib.html\n#\n# TODO: replace by `graphlib` from std once in python 3.9\n\n\n__all__ = ["TopologicalSorter", "CycleError"]\n\n_NODE_OUT = -1\n_NODE_DONE = -2\n\n\nclass _NodeInfo:\n    __slots__ = "node", "npredecessors", "successors"\n\n    def __init__(self, node):\n        # The node this class is augmenting.\n        self.node = node\n\n        # Number of predecessors, generally >= 0. When this value falls to 0,\n        # and is returned by get_ready(), this is set to _NODE_OUT and when the\n        # node is marked done by a call to done(), set to _NODE_DONE.\n        self.npredecessors = 0\n\n        # List of successor nodes. The list can contain duplicated elements as\n        # long as they\'re all reflected in the successor\'s npredecessors attribute.\n        self.successors = []\n\n\nclass CycleError(ValueError):\n    """Subclass of ValueError raised by TopologicalSorter.prepare if cycles\n    exist in the working graph.\n\n    If multiple cycles exist, only one undefined choice among them will be reported\n    and included in the exception. The detected cycle can be accessed via the second\n    element in the *args* attribute of the exception instance and consists in a list\n    of nodes, such that each node is, in the graph, an immediate predecessor of the\n    next node in the list. In the reported list, the first and the last node will be\n    the same, to make it clear that it is cyclic.\n    """\n\n    pass\n\n\nclass TopologicalSorter:\n    """Provides functionality to topologically sort a graph of hashable nodes"""\n\n    def __init__(self, graph=None):\n        self._node2info = {}\n        self._ready_nodes = None\n        self._npassedout = 0\n        self._nfinished = 0\n\n        if graph is not None:\n            for node, predecessors in graph.items():\n                self.add(node, *predecessors)\n\n    def _get_nodeinfo(self, node):\n        if (result := self._node2info.get(node)) is None:\n            self._node2info[node] = result = _NodeInfo(node)\n        return result\n\n    def add(self, node, *predecessors):\n        """Add a new node and its predecessors to the graph.\n\n        Both the *node* and all elements in *predecessors* must be hashable.\n\n        If called multiple times with the same node argument, the set of dependencies\n        will be the union of all dependencies passed in.\n\n        It is possible to add a node with no dependencies (*predecessors* is not provided)\n        as well as provide a dependency twice. If a node that has not been provided before\n        is included among *predecessors* it will be automatically added to the graph with\n        no predecessors of its own.\n\n        Raises ValueError if called after "prepare".\n        """\n        if self._ready_nodes is not None:\n            raise ValueError("Nodes cannot be added after a call to prepare()")\n\n        # Create the node -> predecessor edges\n        nodeinfo = self._get_nodeinfo(node)\n        nodeinfo.npredecessors += len(predecessors)\n\n        # Create the predecessor -> node edges\n        for pred in predecessors:\n            pred_info = self._get_nodeinfo(pred)\n            pred_info.successors.append(node)\n\n    def prepare(self):\n        """Mark the graph as finished and check for cycles in the graph.\n\n        If any cycle is detected, "CycleError" will be raised, but "get_ready" can\n        still be used to obtain as many nodes as possible until cycles block more\n        progress. After a call to this function, the graph cannot be modified and\n        therefore no more nodes can be added using "add".\n        """\n        if self._ready_nodes is not None:\n            raise ValueError("cannot prepare() more than once")\n\n        self._ready_nodes = [i.node for i in self._node2info.values() if i.npredecessors == 0]\n        # ready_nodes is set before we look for cycles on purpose:\n        # if the user wants to catch the CycleError, that\'s fine,\n        # they can continue using the instance to grab as many\n        # nodes as possible before cycles block more progress\n        cycle = self._find_cycle()\n        if cycle:\n            raise CycleError(f"nodes are in a cycle", cycle)\n\n    def get_ready(self):\n        """Return a tuple of all the nodes that are ready.\n\n        Initially it returns all nodes with no predecessors; once those are marked\n        as processed by calling "done", further calls will return all new nodes that\n        have all their predecessors already processed. Once no more progress can be made,\n        empty tuples are returned.\n\n        Raises ValueError if called without calling "prepare" previously.\n        """\n        if self._ready_nodes is None:\n            raise ValueError("prepare() must be called first")\n\n        # Get the nodes that are ready and mark them\n        result = tuple(self._ready_nodes)\n        n2i = self._node2info\n        for node in result:\n            n2i[node].npredecessors = _NODE_OUT\n\n        # Clean the list of nodes that are ready and update\n        # the counter of nodes that we have returned.\n        self._ready_nodes.clear()\n        self._npassedout += len(result)\n\n        return result\n\n    def is_active(self):\n        """Return ``True`` if more progress can be made and ``False`` otherwise.\n\n        Progress can be made if cycles do not block the resolution and either there\n        are still nodes ready that haven\'t yet been returned by "get_ready" or the\n        number of nodes marked "done" is less than the number that have been returned\n        by "get_ready".\n\n        Raises ValueError if called without calling "prepare" previously.\n        """\n        if self._ready_nodes is None:\n            raise ValueError("prepare() must be called first")\n        return self._nfinished < self._npassedout or bool(self._ready_nodes)\n\n    def __bool__(self):\n        return self.is_active()\n\n    def done(self, *nodes):\n        """Marks a set of nodes returned by "get_ready" as processed.\n\n        This method unblocks any successor of each node in *nodes* for being returned\n        in the future by a call to "get_ready".\n\n        Raises :exec:`ValueError` if any node in *nodes* has already been marked as\n        processed by a previous call to this method, if a node was not added to the\n        graph by using "add" or if called without calling "prepare" previously or if\n        node has not yet been returned by "get_ready".\n        """\n\n        if self._ready_nodes is None:\n            raise ValueError("prepare() must be called first")\n\n        n2i = self._node2info\n\n        for node in nodes:\n            # Check if we know about this node (it was added previously using add()\n            if (nodeinfo := n2i.get(node)) is None:\n                raise ValueError(f"node {node!r} was not added using add()")\n\n            # If the node has not being returned (marked as ready) previously, inform the user.\n            stat = nodeinfo.npredecessors\n            if stat != _NODE_OUT:\n                if stat >= 0:\n                    raise ValueError(f"node {node!r} was not passed out (still not ready)")\n                elif stat == _NODE_DONE:\n                    raise ValueError(f"node {node!r} was already marked done")\n                else:\n                    assert False, f"node {node!r}: unknown status {stat}"\n\n            # Mark the node as processed\n            nodeinfo.npredecessors = _NODE_DONE\n\n            # Go to all the successors and reduce the number of predecessors, collecting all the ones\n            # that are ready to be returned in the next get_ready() call.\n            for successor in nodeinfo.successors:\n                successor_info = n2i[successor]\n                successor_info.npredecessors -= 1\n                if successor_info.npredecessors == 0:\n                    self._ready_nodes.append(successor)\n            self._nfinished += 1\n\n    def _find_cycle(self):\n        n2i = self._node2info\n        stack = []\n        itstack = []\n        seen = set()\n        node2stacki = {}\n\n        for node in n2i:\n            if node in seen:\n                continue\n\n            while True:\n                if node in seen:\n                    # If we have seen already the node and is in the\n                    # current stack we have found a cycle.\n                    if node in node2stacki:\n                        return stack[node2stacki[node] :] + [node]\n                    # else go on to get next successor\n                else:\n                    seen.add(node)\n                    itstack.append(iter(n2i[node].successors).__next__)\n                    node2stacki[node] = len(stack)\n                    stack.append(node)\n\n                # Backtrack to the topmost stack entry with\n                # at least another successor.\n                while stack:\n                    try:\n                        node = itstack[-1]()\n                        break\n                    except StopIteration:\n                        del node2stacki[stack.pop()]\n                        itstack.pop()\n                else:\n                    break\n        return None\n\n    def static_order(self):\n        """Returns an iterable of nodes in a topological order.\n\n        The particular order that is returned may depend on the specific\n        order in which the items were inserted in the graph.\n\n        Using this method does not require to call "prepare" or "done". If any\n        cycle is detected, :exc:`CycleError` will be raised.\n        """\n        self.prepare()\n        while self.is_active():\n            node_group = self.get_ready()\n            yield from node_group\n            self.done(*node_group)\n')
    __stickytape_write_module('data/repository.py', b'from typing import TYPE_CHECKING, Dict, List, Optional, Union\n\nfrom pydantic import Field\nfrom pydantic.main import BaseModel\nfrom typing_extensions import Annotated\n\nfrom data.metric import Metric\nfrom data.models import Activity, DbTable, Dimension, DimensionActivityMapping, Entity, EntityType, Id\nfrom range_analysis_app.exploration.range_comparison_exploration import RangeComparisonVisualization\nfrom range_analysis_app.exploration.time_series_exploration import TimeSeriesVisualization\n\nENTITY_TYPE_TO_COLLECTION: Dict[EntityType, str] = {\n    "dimension": "dimensions",\n    "activity": "activities",\n    "metric": "metrics",\n}\n\nVisualization = Annotated[\n    Union[TimeSeriesVisualization, RangeComparisonVisualization],\n    Field(..., discriminator="viz_type"),\n]\n\n\nclass Repository(BaseModel):\n    dimensions: List[Dimension] = []\n    activities: List[Activity] = []\n    metrics: List[Metric] = []\n    selected_tables: List[DbTable] = []\n    dim_act_mappings: List[DimensionActivityMapping] = []\n    visualizations: Dict[int, Visualization] = {}\n\n    def get_entity(self, entity_type: EntityType, uid: Id) -> Optional[Entity]:\n        collection_name = ENTITY_TYPE_TO_COLLECTION.get(entity_type)\n        if collection_name is None:\n            return None\n\n        for e in getattr(self, collection_name):\n            if e.uid == uid:\n                return e\n        return None\n')
    __stickytape_write_module('range_analysis_app/exploration/__init__.py', b'')
    __stickytape_write_module('range_analysis_app/exploration/range_comparison_exploration.py', b'import calendar\nfrom ast import Dict\nfrom typing import Literal\n\nimport pandas as pd\nimport plotly.graph_objects as go\nimport snowflake.snowpark.functions as f\nimport streamlit as st\nfrom streamlit.delta_generator import DeltaGenerator\n\nfrom data.metric import Metric\nfrom range_analysis_app.definitions.period import Period\nfrom range_analysis_app.definitions.range_comparison import RANGE_PERIODICITIES, RangeComparison, RangePeriodicity\nfrom range_analysis_app.exploration.plotly_styles import (\n    BOXPLOT_STYLE,\n    COLORS,\n    PLOT_LAYOUT,\n    PLOTLY_CONFIG,\n    SCATTER_STYLE,\n)\nfrom range_analysis_app.exploration.time_series_exploration import multiselect_dimension_filters\nfrom range_analysis_app.exploration.visualization import VisualizationModel\n\n\ndef range_comparison_exploration(metric: Metric, content_col: DeltaGenerator, side_col: DeltaGenerator) -> None:\n    """Component to create a range comparison and visualize it."""\n    with side_col:\n        comparison = generate_range_comparison(metric)\n\n        viz = RangeComparisonVisualization(comparison=comparison)\n        viz.button_add_to_dashboard()\n\n    with content_col:\n        viz.visualize()\n\n\ndef generate_range_comparison(metric: Metric) -> RangeComparison:\n    range_periodicity = st.selectbox("Range Periodicity", RANGE_PERIODICITIES, index=RANGE_PERIODICITIES.index("month"))\n\n    dates = fetch_dates(metric)\n    last_year = pd.Period(dates.end.year, freq="Y") - 1\n\n    current_period_date = st.date_input(\n        "Current Period Date",\n        value=dates.end,\n        min_value=dates.start,\n        max_value=dates.end,\n    )\n    historical_start, historical_end = st.date_input(\n        "Historical Dates",\n        value=(last_year.start_time.date(), last_year.end_time.date()),\n        min_value=dates.start,\n        max_value=dates.end,\n    )\n\n    filters = multiselect_dimension_filters(metric)\n\n    return RangeComparison(\n        metric=metric,\n        range_periodicity=range_periodicity,\n        historical_dates=Period(start=pd.Timestamp(historical_start), end=pd.Timestamp(historical_end)),\n        current_period_date=pd.Timestamp(current_period_date),\n        filters=filters,\n    )\n\n\n@st.cache_data\ndef fetch_ranges(comparison: RangeComparison) -> pd.DataFrame:\n    return pd.DataFrame(\n        comparison.dataframe()\n        .select(\n            "range_period",\n            "current_value",\n            "minimum",\n            "lower_quartile",\n            "median",\n            "upper_quartile",\n            "maximum",\n        )\n        .sort("range_period")\n        .collect()\n    ).rename(columns=str.lower)\n\n\n@st.cache_data\ndef fetch_dates(metric: Metric) -> Period:\n    row = (\n        metric.dataframe()\n        .select(\n            f.min(metric.columns.timestamp).alias("min_date"),\n            f.max(metric.columns.timestamp).alias("max_date"),\n        )\n        .first()\n    )\n\n    return Period(start=row.MIN_DATE, end=row.MAX_DATE)\n\n\nclass RangeComparisonVisualization(VisualizationModel):\n    viz_type: Literal["range_comparison"] = "range_comparison"\n    comparison: RangeComparison\n\n    def visualize(self) -> None:\n        """Visualize the range comparison with a boxplots of historical ranges and line of current range."""\n        ranges = fetch_ranges(self.comparison)\n\n        if len(ranges) == 0:\n            st.text("No result on this date range")\n            return\n\n        range_periodicity = self.comparison.range_periodicity\n        filters = self.comparison.filters\n\n        PERIOD_NAMES = {\n            "day_of_week": [\n                "Sunday",\n                "Monday",\n                "Tuesday",\n                "Wednesday",\n                "Thursday",\n                "Friday",\n                "Saturday",\n            ],\n            "month": calendar.month_name,\n        }\n        if range_periodicity in PERIOD_NAMES:\n            ranges = ranges.assign(\n                range_period=ranges["range_period"].map(dict(enumerate(PERIOD_NAMES[range_periodicity])))\n            )\n\n        metric_label = self.comparison.metric.name\n        current_period_label = current_period_text(range_periodicity, self.comparison.current_period())\n\n        title = f"{metric_label} in {current_period_label} compared to historical"\n        if not filters.is_empty():\n            title += "".join(" - " + ", ".join(values) for values in filters.mapping.values())\n\n        fig = go.Figure(\n            layout=dict(\n                title=title,\n                xaxis_title="",\n                yaxis_title=metric_label,\n            ),\n        )\n        fig.update_xaxes(type="category")\n        fig.add_trace(\n            go.Box(\n                name="Historical",\n                x=ranges["range_period"],\n                lowerfence=ranges["minimum"],\n                q1=ranges["lower_quartile"],\n                median=ranges["median"],\n                q3=ranges["upper_quartile"],\n                upperfence=ranges["maximum"],\n                **BOXPLOT_STYLE,\n            )\n        )\n        fig.add_trace(\n            go.Scatter(\n                name=current_period_label.capitalize(),\n                x=ranges["range_period"],\n                y=ranges["current_value"],\n                **SCATTER_STYLE,\n                marker_size=10 if len(ranges) < 100 else 6,\n                mode="markers",\n                line_color=COLORS["highlight"],\n            )\n        )\n        fig.update_layout(PLOT_LAYOUT)\n\n        st.plotly_chart(fig, config=PLOTLY_CONFIG, use_container_width=True)\n\n\ndef current_period_text(periodicity: RangePeriodicity, current_period: Period):\n    PERIODICITY_FORMATS: Dict[RangePeriodicity, str] = {\n        "day_of_week": "week %W of %Y",\n        "day_of_month": "days of %b %Y",\n        "day_of_year": "days of %Y",\n        "month": "months of %Y",\n        "quarter": "quarters of %Y",\n    }\n    if periodicity == "week":\n        return f"weeks of {current_period.start.isocalendar()[0]}"\n        #                  ^ when the first week is in the end of the previous year\n\n    return current_period.start.strftime(PERIODICITY_FORMATS[periodicity])\n')
    __stickytape_write_module('range_analysis_app/definitions/__init__.py', b'')
    __stickytape_write_module('range_analysis_app/definitions/period.py', b'import datetime\nfrom typing import Literal\n\nimport pandas as pd\nfrom pydantic import BaseModel\n\nPeriodFrequency = Literal["W", "M", "Y", "year-weeks"]\n\n\nclass Period(BaseModel):\n    """Period of time between two dates."""\n\n    start: datetime.date\n    end: datetime.date\n\n    @staticmethod\n    def from_pandas(period: pd.Period) -> "Period":\n        return Period(start=period.start_time, end=period.end_time)\n\n    @staticmethod\n    def from_date(date: datetime.date, freq: PeriodFrequency) -> "Period":\n        if freq == "year-weeks":\n            return period_of_year_weeks(date.year)\n\n        return Period.from_pandas(pd.Timestamp(date).to_period(freq))\n\n\ndef period_of_year_weeks(year: int) -> Period:\n    """Return the start and end date of the first and last weeks of the given year."""\n    year_period = pd.Period(year, freq="Y")\n\n    first_week = year_period.start_time.to_period("W")\n    last_week = year_period.end_time.to_period("W")\n\n    if first_week.week != 1:  # the last week of previous year\n        first_week += 1\n\n    if last_week.week == 1:  # the first week of next year\n        last_week -= 1\n\n    return Period(start=first_week.start_time.floor("D"), end=last_week.end_time.floor("D"))\n')
    __stickytape_write_module('range_analysis_app/definitions/range_comparison.py', b'from datetime import date\nfrom typing import Any, Dict, List, Literal, get_args\n\nimport pandas as pd\nimport snowflake.snowpark as snowpark\nimport snowflake.snowpark.functions as f\n\nfrom data.metric import Metric\nfrom range_analysis_app import db\nfrom range_analysis_app.definitions.definition import DataframeDefinition\nfrom range_analysis_app.definitions.period import Period, PeriodFrequency\nfrom range_analysis_app.definitions.time_series import TimeSeries\n\nRangePeriodicity = Literal["day_of_month", "day_of_week", "day_of_year", "week", "month", "quarter"]\nRANGE_PERIODICITIES = get_args(RangePeriodicity)\n\n\nclass RangeComparison(DataframeDefinition):\n    metric: Metric\n    range_periodicity: RangePeriodicity\n    historical_dates: Period\n    current_period_date: date\n    filters: db.Filters\n\n    def dataframe(self) -> snowpark.DataFrame:\n        metric_col = self.metric.columns.metric_column\n        historical_ranges = aggregate_over_ranges(\n            self.metric,\n            self.range_periodicity,\n            date_range=self.historical_dates,\n            aggregations=[\n                f.min(metric_col).alias("minimum"),\n                f.percentile_cont(0.25).within_group(metric_col).alias("lower_quartile"),\n                f.median(metric_col).alias("median"),\n                f.percentile_cont(0.75).within_group(metric_col).alias("upper_quartile"),\n                f.max(metric_col).alias("maximum"),\n            ],\n            filters=self.filters,\n        )\n\n        current_ranges = aggregate_over_ranges(\n            self.metric,\n            self.range_periodicity,\n            date_range=self.current_period(),\n            aggregations=[\n                f.any_value(metric_col).alias("current_value"),\n            ],\n            filters=self.filters,\n        )\n\n        return historical_ranges.join(current_ranges, on="range_period", how="left")\n\n    def current_period(self) -> Period:\n        PERIOD_FREQUENCIES: Dict[RangePeriodicity, PeriodFrequency] = {\n            "day_of_week": "W",\n            "day_of_month": "M",\n            "day_of_year": "Y",\n            "week": "year-weeks",\n            "month": "Y",\n            "quarter": "Y",\n        }\n        return Period.from_date(self.current_period_date, freq=PERIOD_FREQUENCIES[self.range_periodicity])\n\n\ndef aggregate_over_ranges(\n    metric: Metric,\n    range_periodicity: RangePeriodicity,\n    date_range: Period,\n    aggregations: List[f.Column],\n    filters: db.Filters,\n) -> snowpark.DataFrame:\n    """Aggregate the given metric over the periods of a periodicity.\n\n    For example, to aggregate each month between 2010 and 2020, outputting 12 rows.\n    """\n    RANGE_PERIODICITIES_CONFIGS: Dict[RangePeriodicity, Dict[str, Any]] = {\n        "day_of_week": {"period_function": f.dayofweek, "grain": "day"},\n        "day_of_month": {"period_function": f.dayofmonth, "grain": "day"},\n        "day_of_year": {"period_function": f.dayofyear, "grain": "day"},\n        "week": {"period_function": f.weekofyear, "grain": "week"},\n        "month": {"period_function": f.month, "grain": "month"},\n        "quarter": {"period_function": f.quarter, "grain": "quarter"},\n    }\n\n    range_config = RANGE_PERIODICITIES_CONFIGS[range_periodicity]\n    period_column = range_config["period_function"]("period").alias("range_period")\n\n    cols = metric.columns\n    return (\n        TimeSeries(metric=metric, grain=range_config["grain"])\n        .dataframe()\n        .filter(filters.to_condition())\n        .filter(f.col("period").between(date_range.start, date_range.end))\n        .select([cols.metric_column, period_column])\n        .group_by([period_column.get_name()])\n        .agg(aggregations)\n        .sort(period_column.get_name())\n    )\n')
    __stickytape_write_module('range_analysis_app/definitions/definition.py', b'import hashlib\nfrom abc import abstractmethod\nfrom typing import Generic, Literal, TypeVar\n\nimport snowflake.snowpark as snowpark\nfrom pydantic import BaseModel\n\nimport range_analysis_app.db as db\nfrom utils import BaseModelCacheKey\n\n\ndef hash_json(model: BaseModel) -> int:\n    """Return a constant hash representing the given Pydantic model."""\n    return int(hashlib.sha1(model.json().encode()).hexdigest()[:10], 16)\n\n\nclass DataframeDefinition(BaseModelCacheKey):\n    """Represent a Snowpark calculation resulting in a dataframe.\n\n    This definition is hashable to be able to cache the result in Streamlit.\n    """\n\n    __hash__ = hash_json\n\n    @abstractmethod\n    def dataframe(self) -> snowpark.DataFrame:\n        ...\n\n\nTDefinition = TypeVar("TDefinition", bound=DataframeDefinition)\n\n\nclass ResultTable(Generic[TDefinition], BaseModel):\n    """Table stored from a Snowpark Dataframe."""\n\n    __hash__ = hash_json\n\n    table_name: str\n    definition: TDefinition\n\n    def table(self) -> snowpark.Table:\n        return db.snowflake_session().table(self.table_name)\n\n\ndef generate_table(\n    definition: TDefinition,\n    table_name: str,\n    table_type: Literal["", "temporary"] = "",\n) -> ResultTable[TDefinition]:\n    """Generate a table for the given dataframe and return the table name."""\n    definition.dataframe().write.save_as_table(table_name, table_type=table_type, mode="overwrite")\n    return ResultTable(table_name=table_name, definition=definition)\n')
    __stickytape_write_module('range_analysis_app/definitions/time_series.py', b'from typing import Literal, get_args\n\nimport snowflake.snowpark as snowpark\nimport snowflake.snowpark.functions as f\n\nfrom data.metric import Metric\nfrom range_analysis_app.definitions.definition import DataframeDefinition\n\nGrain = Literal["day", "week", "month", "quarter", "year"]\nGRAINS = get_args(Grain)\n\n\nclass TimeSeries(DataframeDefinition):\n    metric: Metric\n    grain: Grain\n\n    def dataframe(self) -> snowpark.DataFrame:\n        cols = self.metric.columns\n        period_column = f.date_trunc(self.grain, cols.timestamp).alias("period")\n\n        return (\n            self.metric.dataframe()\n            .select(cols.dimensions + [cols.metric_column, period_column])\n            .group_by(cols.dimensions + [period_column.get_name()])\n            .agg(getattr(f, cols.agg_method)(cols.metric_column).alias(cols.metric_column))\n        )\n')
    __stickytape_write_module('range_analysis_app/exploration/plotly_styles.py', b'COLORS = {\n    "text": "#3e49a7",\n    "background": "#ffffff",\n    "line": "#00a9de",\n    "lineBackground": "#9cdcf7",\n    "highlight": "#ff2255",\n}\n\n# Doc: https://plotly.com/python/configuration-options\nPLOTLY_CONFIG = {\n    "displayModeBar": False,\n}\n\n# Doc: https://plotly.com/python-api-reference/generated/plotly.graph_objects.layout.html#plotly.graph_objects.layout.Shape\nPLOT_BORDER = {\n    "type": "rect",\n    "xref": "paper",\n    "yref": "paper",\n    "x0": 0,\n    "y0": 0,\n    "x1": 1,\n    "y1": 1,\n    "line": {"width": 1, "color": COLORS["text"]},\n}\n\n# Doc: https://plotly.com/python-api-reference/generated/plotly.graph_objects.Layout.html\nPLOT_LAYOUT = {\n    "height": 420,\n    "margin": {"t": 40, "b": 0, "l": 0, "r": 0},\n    "font_family": "Arial",\n    "font_color": COLORS["text"],\n    "showlegend": True,\n    "paper_bgcolor": COLORS["background"],\n    "title": {\n        "font": {"color": COLORS["text"]},\n        "x": 0,\n        "y": 0.98,\n        "xanchor": "left",\n        "yanchor": "top",\n    },\n    "xaxis": {\n        "title": {"font": {"color": COLORS["text"], "size": 14}, "standoff": 10},\n        "showgrid": True,\n        "color": COLORS["text"],\n        "ticks": "inside",\n        "tickcolor": COLORS["text"],\n        "tickfont": {"size": 12, "color": COLORS["text"]},\n        "dtick": "M12",\n    },\n    "yaxis": {\n        "title": {"font": {"color": COLORS["text"]}, "standoff": 10},\n        "showgrid": True,\n        "color": COLORS["text"],\n        "ticks": "inside",\n        "tickcolor": COLORS["text"],\n        "tickfont": {"size": 14, "color": COLORS["text"]},\n    },\n    "legend": {\n        "font": {"color": COLORS["text"]},\n        "x": 1,\n        "y": 1.125,\n        "xanchor": "right",\n        "yanchor": "top",\n        "orientation": "h",\n    },\n    "shapes": [PLOT_BORDER],\n}\n\n\n# Doc: https://plotly.com/python-api-reference/generated/plotly.graph_objects.Box.html\nBOXPLOT_STYLE = {\n    "fillcolor": COLORS["lineBackground"],\n    "line": {"color": COLORS["line"]},\n}\n\n\n# Doc: https://plotly.com/python-api-reference/generated/plotly.graph_objects.Scatter.html\nSCATTER_STYLE = {\n    "hovertemplate": "<b>%{y}</b><br>%{x}<extra></extra>",\n    "marker": {"line": {"width": 0}, "size": 6},\n    "line": {"width": 1, "color": COLORS["line"]},\n}\n')
    __stickytape_write_module('range_analysis_app/exploration/time_series_exploration.py', b'from typing import Literal\n\nimport pandas as pd\nimport plotly.graph_objects as go\nimport snowflake.snowpark.functions as f\nimport streamlit as st\nfrom streamlit.delta_generator import DeltaGenerator\n\nimport range_analysis_app.db as db\nfrom data.metric import Metric\nfrom range_analysis_app.definitions.time_series import GRAINS, TimeSeries\nfrom range_analysis_app.exploration.range_comparison_exploration import PLOT_LAYOUT, PLOTLY_CONFIG, SCATTER_STYLE\nfrom range_analysis_app.exploration.visualization import VisualizationModel\n\n\ndef time_series_exploration(metric: Metric, content_col: DeltaGenerator, side_col: DeltaGenerator) -> None:\n    """Component to create a time series and visualize it."""\n    with side_col:\n        grain = st.selectbox("Periodicity", GRAINS, index=GRAINS.index("month"))\n        time_series = TimeSeries(metric=metric, grain=grain)\n\n        filters = multiselect_dimension_filters(metric)\n        viz = TimeSeriesVisualization(time_series=time_series, filters=filters)\n        viz.button_add_to_dashboard()\n\n    with content_col:\n        viz.visualize()\n\n\nclass TimeSeriesVisualization(VisualizationModel):\n    viz_type: Literal["time_series"] = "time_series"\n    time_series: TimeSeries\n    filters: db.Filters\n\n    def visualize(self) -> None:\n        df = fetch_time_series(self.time_series, self.filters)\n\n        metric = self.time_series.metric\n        metric_column = metric.columns.metric_column.lower()\n\n        title = f"{metric.name} per {self.time_series.grain}"\n        if not self.filters.is_empty():\n            title += "".join(" - " + ", ".join(values) for values in self.filters.mapping.values())\n\n        y_axis_title = metric.name\n        x_axis_title = ""\n\n        fig = (\n            go.Figure(\n                layout=dict(\n                    title=title,\n                    xaxis_title=x_axis_title,\n                    yaxis_title=y_axis_title,\n                )\n            )\n            .add_trace(\n                go.Scatter(\n                    name="Values",\n                    x=df["period"],\n                    y=df[metric_column],\n                    **SCATTER_STYLE,\n                    mode="lines+markers",\n                )\n            )\n            .update_layout(PLOT_LAYOUT)\n        )\n\n        st.plotly_chart(fig, config=PLOTLY_CONFIG, use_container_width=True)\n\n\n@st.cache_data\ndef fetch_time_series(time_series: TimeSeries, filters: db.Filters) -> pd.DataFrame:\n    metric_column = time_series.metric.columns.metric_column\n    return pd.DataFrame(\n        time_series.dataframe()\n        .filter(filters.to_condition())\n        .group_by("period")\n        .agg(f.sum(metric_column).alias(metric_column))\n        .sort("period")\n        .collect()\n    ).rename(columns=str.lower)\n\n\ndef multiselect_dimension_filters(metric: Metric) -> db.Filters:\n    st.markdown("##### Filters")\n\n    filters = db.Filters()\n    for dimension in metric.columns.dimensions:\n        label = metric.columns.labels.get(dimension, dimension)\n\n        values = st.multiselect(label, metric.dimension_values(dimension))\n        if values:\n            filters.add(dimension, values)\n\n    return filters\n')
    __stickytape_write_module('range_analysis_app/exploration/visualization.py', b'from abc import abstractmethod\n\nimport streamlit as st\nfrom pydantic import BaseModel\n\nfrom data.helpers import repository\nfrom range_analysis_app.definitions.definition import hash_json\n\n\nclass VisualizationModel(BaseModel):\n    __hash__ = hash_json\n\n    @abstractmethod\n    def visualize():\n        ...\n\n    def button_add_to_dashboard(self) -> None:\n        """button to add the current visualization to the dashboard."""\n        if hash(self) in repository().visualizations:\n            st.info("Added in dashboard")\n            return\n\n        def add_to_dashboard():\n            repository().visualizations[hash(self)] = self\n\n        st.button("Add to dashboard", on_click=add_to_dashboard)\n')
    __stickytape_write_module('app_pages/data_analysis.py', b'import streamlit as st\n\nfrom app_pages.page import Page\nfrom data.helpers import repository\nfrom range_analysis_app.exploration.range_comparison_exploration import range_comparison_exploration\nfrom range_analysis_app.exploration.time_series_exploration import time_series_exploration\n\n\nclass DataAnalysisPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "This page performs the required data analysis based on the metrics defined by the user and displays the results using data visualization tools like Plotly or Matplotlib."\n\n    def run(self) -> None:\n        metrics = [metric for metric in repository().metrics if metric.columns is not None]\n\n        if len(metrics) == 0:\n            st.warning("You don\'t have defined metrics. Please go to the Metric Creation page and create some.")\n            return\n\n        content_col, side_col = st.columns([10, 2])\n        selected_metric = side_col.selectbox("Metric", metrics, format_func=lambda metric: metric.name)\n\n        VISUALIZATIONS = {\n            "Time series": time_series_exploration,\n            "Range comparison": range_comparison_exploration,\n        }\n        selected_visualization = side_col.selectbox("Visualization", VISUALIZATIONS)\n        VISUALIZATIONS[selected_visualization](selected_metric, content_col, side_col)\n')
    __stickytape_write_module('app_pages/help_page.py', b'from typing import Dict, Type\n\nimport streamlit as st\n\nfrom app_pages.data_loading import DataLoadingPage\nfrom app_pages.doma_entities_page import DomaEntitiesPage\nfrom app_pages.mapping import MappingPage\nfrom app_pages.metrics_creation import MetricsCreationPage\nfrom app_pages.page import Page\nfrom app_pages.summary_settings_page import SummarySettingsPage\n\n\nclass HelpPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "Help"\n\n    def run(self) -> None:\n        st.title("Help")\n')
    __stickytape_write_module('app_pages/data_loading.py', b'from typing import Dict, List\n\nimport streamlit as st\nfrom streamlit.runtime.state import WidgetCallback\n\nfrom app_pages.page import Page\nfrom data.db import fetch_databases, fetch_schemas, fetch_tables\nfrom data.helpers import repository\nfrom data.models import DbTable, Entity, Id\nfrom data.repository import Repository\nfrom range_analysis_app import db\nfrom range_analysis_app.components.collapse import collapse\n\n\nclass DataLoadingPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "This page allows users to select the tables and map they to DOMA entities"\n\n    def run(self) -> None:\n        repo = repository()\n\n        dimensions_labels = {str(dim.uid): f\'Dimension: "{dim.name}"\' for dim in repo.dimensions}\n        activities_labels = {str(act.uid): f\'Activity: "{act.name}"\' for act in repo.activities}\n        labels = {**dimensions_labels, **activities_labels}\n        entities = repo.dimensions + repo.activities\n        entities_map = {str(e.uid): e for e in entities}\n        mapped_entity_ids = [t.entity_uid for t in repo.selected_tables]\n        unmapped_entities: List[Entity] = [e for e in entities if e.uid not in mapped_entity_ids]\n\n        options = [uid for uid in entities_map.keys() if uid not in mapped_entity_ids]\n\n        if len(entities) == 0:\n            st.warning("You don\'t have Dimensions or Activities. Please go to the Entities page and configure it")\n            return\n\n        st.markdown(\n            """\n            Please select tables corresponding to DOMA entities:\n            1. Choose a Database and Schema.\n            2. Choose a table to map to a DOMA entity.\n            3. Select the appropriate DOMA entity from the \'Map with DOMA Entity\' dropdown.\n            4. Click the \'Add to selected tables \xe2\x9e\xa1\xef\xb8\x8f\' button.\n            5. Verify the table is now listed under \'Selected tables\' on the right side.\n            """\n        )\n\n        left, right = st.columns([2, 2])\n\n        with left:\n            self.source_tables(options=options, labels=labels, entities_map=entities_map)\n        with right:\n            self.selected_tables(repo=repo, unmapped_entities=unmapped_entities)\n\n    def source_tables(self, options: List[str], labels: Dict[str, str], entities_map: Dict[Id, Entity]) -> None:\n        st.markdown("#### Source tables")\n        cols = st.columns([1, 1])\n        selected_database = cols[0].selectbox("Database", key="selected_database", options=fetch_databases())\n        selected_schema = cols[1].selectbox("Schema", key="selected_schema", options=fetch_schemas(selected_database))\n        tables = fetch_tables(selected_database, selected_schema)\n        if len(tables) == 0:\n            st.write(f"There is no tables at `{selected_database}.{selected_schema}`")\n            return\n        selected_table = cols[0].selectbox("Table", key="selected_table", options=tables)\n\n        entity_uid = cols[1].selectbox(\n            label="Map with DOMA Entity",\n            options=options,\n            format_func=lambda uid: labels.get(uid),\n        )\n        full_selected_table = f"{selected_database}.{selected_schema}.{selected_table}"\n\n        columns = db.fetch_columns(full_selected_table)\n\n        st.button(\n            "Add to selected tables \xe2\x9e\xa1\xef\xb8\x8f",\n            disabled=len(options) == 0,\n            on_click=lambda: DbTable.create(\n                table_database=selected_database,\n                table_schema=selected_schema,\n                table_name=selected_table,\n                columns=columns,\n                entity=entities_map[entity_uid],\n            ),\n        )\n\n        st.write(f"Full table name: `{full_selected_table}`")\n        if collapse(f"Preview table"):\n            st.dataframe(db.preview_table(full_selected_table))\n\n    def selected_tables(self, repo: Repository, unmapped_entities: List[Entity]) -> None:\n        if unmapped_entities:\n            st.write("#### Unmapped entities")\n            st.write("\\n".join([f" - {e.name}" for e in unmapped_entities]))\n\n        st.markdown("#### Selected tables")\n        if not repo.selected_tables:\n            st.write("No tables selected yet.")\n            return\n\n        for table in repo.selected_tables:\n            entity_name = repo.get_entity(table.entity_type, table.entity_uid).name\n            title = f"Table {table.table_name} - {table.entity_type.capitalize()} {entity_name} "\n            with st.expander(title):\n                st.button(f"Delete", key=f"remove_btn_{table.uid}", on_click=table.delete)\n')
    __stickytape_write_module('data/db.py', b'from typing import List\n\nimport streamlit as st\n\nfrom range_analysis_app.db import snowflake_session\n\n\n@st.cache_data\ndef fetch_databases() -> List[str]:\n    return [row.name for row in snowflake_session().sql(f"show databases").collect()]\n\n\n@st.cache_data\ndef fetch_schemas(database: str = "") -> List[str]:\n    return [row.name for row in snowflake_session().sql(f"show schemas in database {database}").collect()]\n\n\n@st.cache_data\ndef fetch_tables(database: str, schema: str) -> List[str]:\n    return [row.name for row in snowflake_session().sql(f"show tables in schema {database}.{schema}").collect()]\n')
    __stickytape_write_module('range_analysis_app/components/collapse.py', b'from typing import Optional\n\nimport streamlit as st\n\n\ndef collapse(label: str = "", open_by_default: str = False, key: Optional[str] = None) -> bool:\n    if not key:\n        key = f"collapse-{label}"\n\n    is_open = st.session_state.get(key, open_by_default)\n\n    def switch() -> None:\n        st.session_state[key] = not is_open\n\n    caret = "\xe2\xac\x86\xef\xb8\x8f" if is_open else "\xe2\xac\x87\xef\xb8\x8f"\n    st.button(f"{caret} {label}", on_click=switch, key=f"btn-{key}")\n\n    return is_open\n')
    __stickytape_write_module('app_pages/doma_entities_page.py', b'from typing import Callable, List\n\nimport streamlit as st\nfrom streamlit.delta_generator import DeltaGenerator\nfrom streamlit.runtime.state import WidgetCallback\n\nfrom app_pages.page import Page\nfrom data.helpers import repository\nfrom data.metric import Metric\nfrom data.models import Activity, Dimension, Entity, EntityType, Id\nfrom diagrams.entity_diagram import graphviz_entities\n\n\ndef render_dimensions(i: int, item: Dimension, item_type: EntityType) -> DeltaGenerator:\n    col1, col2, col3 = st.columns([10, 1, 1])\n    with col1:\n        name_value = item.name\n        name_key = f"[{item_type}][{i}][name]"\n\n        def get_handler(dim: Dimension, key: str) -> WidgetCallback:\n            def h() -> None:\n                dim.name = st.session_state[key]\n\n            return h\n\n        st.text_input(\n            name_key,\n            label_visibility="collapsed",\n            value=name_value,\n            key=name_key,\n            on_change=get_handler(item, name_key),\n        )\n    with col2:\n        st.write("")\n    return col3\n\n\ndef render_activities(i: int, item: Activity, item_type: EntityType) -> DeltaGenerator:\n    col1, col2, col3, col4 = st.columns([5, 5, 1, 1])\n    with col1:\n        name_value = item.name\n        name_key = f"[{item_type}][{i}][name]"\n\n        def get_handler(act: Activity, key: str) -> WidgetCallback:\n            def h() -> None:\n                act.name = st.session_state[key]\n\n            return h\n\n        st.text_input(\n            name_key,\n            label_visibility="collapsed",\n            value=name_value,\n            key=name_key,\n            on_change=get_handler(item, name_key),\n        )\n\n    with col2:\n        selected_dimensions_key = f"[{item_type}][{i}][dimensions]"\n        selected_dimensions: List[Dimension] = item.dimensions\n\n        dim_map = {dim.uid: dim.name for dim in repository().dimensions}\n\n        options = [dim.uid for dim in repository().dimensions]\n\n        def format_dim(dim_uid: Id) -> str:\n            return dim_map[dim_uid]\n\n        selected_dim_ids = [dim.uid for dim in selected_dimensions]\n\n        def update_activity(dim_ids: List[Id]) -> None:\n            item_dim_ids = [dim.uid for dim in item.dimensions]\n            for dim_id in dim_ids:\n                if dim_id not in item_dim_ids:\n                    item.assign_dimension(dim_id)\n                    print("add")\n            for dim in [dim for dim in repository().dimensions]:\n                if dim.uid not in dim_ids:\n                    item.unassign_dimension(dim)\n                    print("remove", dim.uid)\n\n        st.multiselect(\n            selected_dimensions_key,\n            label_visibility="collapsed",\n            options=options,\n            default=selected_dim_ids,\n            key=selected_dimensions_key,\n            on_change=lambda: update_activity(st.session_state[selected_dimensions_key]),\n            format_func=format_dim,\n        )\n    with col3:\n        st.write("")\n    return col4\n\n\ndef render_metrics(i: int, item: Metric, item_type: EntityType) -> DeltaGenerator:\n    col1, col2, col3, col4 = st.columns([5, 5, 1, 1])\n    with col1:\n        name_value = item.name\n        name_key = f"[{item_type}][{i}][name]"\n\n        def get_handler(m: Metric, key: str) -> WidgetCallback:\n            def h() -> None:\n                m.name = st.session_state[key]\n\n            return h\n\n        st.text_input(\n            name_key,\n            label_visibility="collapsed",\n            value=name_value,\n            key=name_key,\n            on_change=get_handler(item, name_key),\n        )\n    with col2:\n        selected_activity_key = f"[{item_type}][{i}][activity]"\n\n        act_map = {str(act.uid): act.name for act in repository().activities}\n\n        options = list(act_map.keys())\n\n        def format_act(act_uid: Id) -> str:\n            return act_map[act_uid]\n\n        def update_metric(act_id: Id) -> None:\n            item.act_id = act_id\n\n        st.selectbox(\n            label=selected_activity_key,\n            label_visibility="collapsed",\n            options=options,\n            format_func=format_act,\n            index=options.index(item.act_id) if item.act_id in options else 0,\n            key=selected_activity_key,\n            on_change=lambda: update_metric(st.session_state[selected_activity_key]),\n        )\n\n    with col3:\n        st.write("")\n    return col4\n\n\nRenderer = Callable[[int, Entity, EntityType], DeltaGenerator]\n\n\ndef render_crud(item_type: EntityType, items: List[Entity], render: Renderer, on_create: WidgetCallback) -> None:\n    for i, item in enumerate(items):\n        last_col = render(i, item, item_type)\n\n        def delete_item(entity: Entity) -> WidgetCallback:\n            return lambda: entity.delete()\n\n        last_col.button(f"Delete", key=f"{item_type}{i}_delete_button", on_click=delete_item(item))\n\n    st.button(f"Add {item_type.capitalize()}", on_click=on_create)\n\n\nclass DomaEntitiesPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "This page provides a brief introduction to the DOMA framework and the purpose of the app."\n\n    def run(self) -> None:\n        st.markdown("### Summary")\n        graphviz_entities()\n\n        st.markdown(\n            """\n            Dimensions are attributes that describe characteristics of an object. \n            They are used to segment and filter data to gain insights into specific aspects of your business.\n\n            Some examples of dimensions might include: User, Location, Product, Category\n            """\n        )\n        render_crud(\n            item_type="dimension",\n            items=repository().dimensions,\n            render=render_dimensions,\n            on_create=lambda: Dimension.create(name=""),\n        )\n\n        st.markdown(\n            """\n            Activities are actions that occur within your business.\n            They are used to measure performance and identify areas for improvement.\n            \n            Some examples of activities might include:\n            Sales, Website Visits, Customer, Service Calls\n            """\n        )\n\n        activities = repository().activities\n        render_crud(\n            item_type="activity",\n            items=activities,\n            render=render_activities,\n            on_create=lambda: Activity.create(name=""),\n        )\n\n        st.markdown(\n            """\n            Metrics are numerical values that represent a specific aspect of your business performance.\n            They are calculated from a combination of dimensions and activities.\n            \n            Some examples of metrics might include: Revenue, Average Order Value, Conversion Rate\n            """\n        )\n        render_crud(\n            item_type="metric",\n            items=repository().metrics,\n            render=render_metrics,\n            on_create=lambda: Metric.create(name="", act_id=activities[0].uid if len(activities) else None),\n        )\n')
    __stickytape_write_module('diagrams/__init__.py', b'')
    __stickytape_write_module('diagrams/entity_diagram.py', b'from typing import Dict, List, Union\n\nimport streamlit as st\n\nfrom data.helpers import repository\nfrom data.metric import Metric\nfrom data.models import Activity, DbTable, Dimension, DimensionActivityMapping\n\nENTITY_COLORS = {"activity": "#ffdd6677", "dimension": "#66dd6677", "metric": "#ff335577"}\n\n\ndef entity_color(entity: Union[Activity, Dimension, Metric]) -> str:\n    return ENTITY_COLORS[type(entity).__name__.lower()]\n\n\ndef graphviz_entities() -> None:\n    """Diagram showing relationships between Dimensions Activities and Metrics."""\n    repo = repository()\n\n    nodes = []\n    nodes.extend(f\'"{dimension.name}" [fillcolor="{entity_color(dimension)}"]\' for dimension in repo.dimensions)\n    nodes.extend(f\'"{activity.name}" [fillcolor="{entity_color(activity)}"]\' for activity in repo.activities)\n    nodes.extend(f\'"{metric.name}" [fillcolor="{entity_color(metric)}"]\' for metric in repo.metrics)\n    nodes = "\\n".join(nodes)\n\n    edges = []\n    for activity in repo.activities:\n        edges.extend(f\'"{dim.name}" -> "{activity.name}"\' for dim in activity.dimensions)\n        edges.extend(f\'"{activity.name}" -> "{metric.name}"\' for metric in activity.metrics)\n    edges = "\\n".join(edges)\n\n    graphviz_dot = f"""\n        digraph G {{\n            graph [fontname="Helvetica,Arial,sans-serif", fontsize=20, layout=dot, newrank=true]\n            node [style="rounded,filled", shape=box]\n            edge [arrowsize=0.75]\n\n            {nodes}\n            {edges}\n        }}\n    """\n\n    st.graphviz_chart(graphviz_dot)\n')
    __stickytape_write_module('app_pages/mapping.py', b'from typing import List\n\nimport streamlit as st\nfrom streamlit.runtime.state import WidgetCallback\n\nfrom app_pages.page import Page\nfrom data.helpers import render_validation_component, repository, validate_state\nfrom data.models import Activity, DbTable, DimensionActivityMapping\nfrom diagrams.mapping_diagram import graphviz_table_relations\nfrom range_analysis_app.db import ID_TYPES\n\n\nclass MappingPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "This page provides an allows to configure relations between Activities and Dimensions"\n\n    def run(self) -> None:\n        self.validations()\n\n        st.markdown("Configure relations between Activities and Dimensions.")\n\n        for act_idx, activity in enumerate(repository().activities):\n            if act_idx != 0:\n                st.markdown("---")\n\n            self.render_activity(activity)\n\n    @staticmethod\n    def validations():\n        repo = repository()\n\n        if len(repo.activities) == 0 or len(repo.dimensions) == 0:\n            st.warning("No Activities or Dimensions have been defined. Please go to Entities to define some.")\n            st.stop()\n\n        # TODO: Share state validation between pages\n        dimensions_valid, dimensions_message = validate_state(repo.dimensions, "dimension", validate_each=True)\n        activities_valid, activities_message = validate_state(repo.activities, "activity", validate_each=True)\n\n        if not dimensions_valid or not activities_valid:\n            st.warning("Missing tables for Activities AND Dimensions. Please go to the RAW data page to select tables.")\n\n            render_validation_component(dimensions_valid, dimensions_message)\n            render_validation_component(activities_valid, activities_message)\n            st.stop()\n\n    def render_activity(self, activity: Activity) -> None:\n        activity_table = activity.db_table\n        assert activity_table, "Activity is validated to have a table"\n\n        dimension_tables: List[DbTable] = [dim.db_table for dim in activity.dimensions]\n        assert all(dimension_tables), "Dimensions are validated to have tables"\n\n        st.markdown(f"### Activity {activity.name} (`{activity_table.table_name}`):")\n        if len(activity.mappings) == 0:\n            st.markdown("No dimension to associate.")\n            return\n\n        for mapping in activity.mappings:\n            self.render_mapping(mapping, dimension_tables)\n\n        graphviz_table_relations(activity)\n\n    def render_mapping(self, mapping: DimensionActivityMapping, dimension_tables: List[DbTable]) -> None:\n        activity = mapping.activity\n        dimension = mapping.dimension\n\n        activity_table = activity.db_table\n        mapping_key = f"{activity.uid}_{dimension.uid}"\n\n        current_dim_table = dimension.db_table\n\n        cols = st.columns([3, 3, 1, 3, 3])\n\n        # Default mapping\n        if mapping.main_table is None or mapping.joined_table is None:\n            mapping.main_table_id = activity_table.uid\n            mapping.joined_table_id = current_dim_table.uid\n            mapping.main_column = activity_table.columns_of_type(ID_TYPES)[0]\n            mapping.joined_column = current_dim_table.columns_of_type(ID_TYPES)[0]\n\n        def set_mapping_field(field: str, state_key: str) -> WidgetCallback:\n            return lambda: setattr(mapping, field, st.session_state[state_key])\n\n        with cols[0]:\n            st.selectbox(\n                "Dimension table",\n                options=[mapping.joined_table.table_name],\n                key=f"dimension_{mapping_key}",\n                disabled=True,\n            )\n\n        with cols[1]:\n            joined_table = current_dim_table\n            joined_columns = joined_table.columns_of_type(ID_TYPES)\n\n            st.selectbox(\n                f"Column in `{joined_table.table_name}`",\n                options=joined_columns,\n                index=joined_columns.index(mapping.joined_column),\n                # format_func=lambda col: f"{joined_table.table_name}.{col}",\n                key=f"joined_column_{mapping_key}",\n                on_change=set_mapping_field("joined_column", f"joined_column_{mapping_key}"),\n            )\n\n        with cols[2]:\n            st.markdown("###")\n            st.markdown("#### -----\xe2\x86\x92")\n\n        with cols[3]:\n            other_dim_tables = [dim_table for dim_table in dimension_tables if dim_table != current_dim_table]\n\n            def update_main_table() -> None:\n                selected_table: DbTable = st.session_state[f"main_table_{mapping_key}"]\n                mapping.main_table_id = selected_table.uid\n                mapping.main_column = selected_table.columns_of_type(ID_TYPES)[0]\n\n            tables = [activity_table] + other_dim_tables\n\n            st.selectbox(\n                "Associated with table",\n                options=tables,\n                index=tables.index(mapping.main_table),\n                format_func=lambda table: table.table_name,\n                key=f"main_table_{mapping_key}",\n                on_change=update_main_table,\n            )\n\n        with cols[4]:\n            main_table = mapping.main_table\n            main_columns = main_table.columns_of_type(ID_TYPES)\n\n            st.selectbox(\n                f"Column in `{main_table.table_name}`",\n                options=main_columns,\n                index=main_columns.index(mapping.main_column),\n                # format_func=lambda col: f"{main_table.table_name}.{col}",\n                key=f"main_column_{mapping_key}",\n                on_change=set_mapping_field("main_column", f"main_column_{mapping_key}"),\n            )\n')
    __stickytape_write_module('diagrams/mapping_diagram.py', b'import streamlit as st\n\nfrom data.models import Activity, DbTable, DimensionActivityMapping\nfrom diagrams.entity_diagram import ENTITY_COLORS\n\n\ndef graphviz_table_relations(activity: Activity) -> None:\n    """Entity-Relationship-Diagram between Activity and Dimension tables."""\n    act_table = activity.db_table\n    tables = [act_table] + [dim.db_table for dim in activity.dimensions]\n\n    table_nodes = "\\n".join(_graphviz_table_node(table) for table in tables)\n    table_edges = "\\n".join(_graphviz_edge(mapping) for mapping in activity.mappings)\n\n    graphviz_dot = f"""\n        digraph G {{\n            graph [fontname="Helvetica,Arial,sans-serif", fontsize=20, layout=dot, rankdir=LR, newrank=true]\n            node [style=filled, shape=rect, shape=plaintext]\n            edge [arrowsize=0.75]\n\n            {table_nodes}\n            {table_edges}\n        }}\n    """\n\n    st.graphviz_chart(graphviz_dot, use_container_width=True)\n\n\ndef _graphviz_table_node(table: DbTable) -> str:\n    """Return a html table representing the table and its columns."""\n    col_table_rows = "\\n".join(f"""<tr><td port="{col}">{col}</td></tr>""" for col, _ in table.columns.items())\n    bgcolor = ENTITY_COLORS[table.entity_type]\n\n    label_table = f"""\n        <table border="0" cellborder="1" cellspacing="0">\n            <tr><td bgcolor="{bgcolor}" align="CENTER"><b>{table.table_name}</b></td></tr>\n            {col_table_rows}\n        </table>\n    """\n    return f"""{table.table_name} [fillcolor="#ffffff00" label=<{label_table}>];"""\n\n\ndef _graphviz_edge(mapping: DimensionActivityMapping) -> str:\n    """Return the edge between tables of the mapping."""\n    main_table = mapping.main_table.table_name\n    joined_table = mapping.joined_table.table_name\n\n    return f"{joined_table}:{mapping.joined_column} -> {main_table}:{mapping.main_column}"\n')
    __stickytape_write_module('app_pages/metrics_creation.py', b'from collections import ChainMap\nfrom typing import Dict, List\n\nimport pandas as pd\nimport streamlit as st\nfrom streamlit.runtime.state import WidgetCallback\n\nimport range_analysis_app.db as db\nfrom app_pages.page import Page\nfrom data.helpers import repository\nfrom data.metric import AGG_METHODS, Metric, MetricColumns\nfrom range_analysis_app.components.collapse import collapse\nfrom utils import chunks\n\n\nclass MetricsCreationPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "This page provides a form where users can define metrics as answers to business questions."\n\n    def run(self) -> None:\n        metrics = repository().metrics\n\n        for metric in metrics:\n            if metric != metrics[0]:\n                st.markdown("---")\n\n            form_metric_definition(metric)\n\n\ndef form_metric_definition(metric: Metric) -> None:\n    st.markdown(f"### Metric {metric.name}")\n\n    activity = metric.activity\n    errors = activity.validate_tables_and_mapping()\n    if errors:\n        for error in errors:\n            st.error(error)\n        return\n\n    columns: Dict[str, db.DataType] = {\n        **activity.db_table.columns,\n        **ChainMap(*[dim.db_table.columns for dim in activity.dimensions]),\n    }\n\n    def columns_of_types(data_types: List[db.DataType]) -> List[str]:\n        """Return the columns having one of the given types."""\n        return [col for col, data_type in columns.items() if data_type in data_types]\n\n    metric_columns = columns_of_types(["FIXED", "REAL", "INTEGER"]) or ["<no number column>"]\n    timestamp_columns = columns_of_types(db.DATETIME_TYPES) or ["<no timestamp column>"]\n\n    left, right = st.columns([6, 6])\n\n    with right:\n        if metric.columns is None:\n            metric.columns = MetricColumns(\n                dimensions=[],\n                metric_column=metric_columns[0],\n                timestamp=timestamp_columns[0],\n                agg_method="sum",\n            )\n\n        def set_columns_field(field: str, state_key: str) -> WidgetCallback:\n            return lambda: setattr(metric.columns, field, st.session_state[state_key])\n\n        def set_columns_field_and_update_dimensions(field: str, state_key: str) -> WidgetCallback:\n            def set_column():\n                column = st.session_state[state_key]\n                metric.columns.remove_dimension_columns({column})\n                setattr(metric.columns, field, column)\n\n            return set_column\n\n        st.markdown("##### Columns")\n\n        cols = st.columns([1, 1, 1])\n        cols[0].selectbox(\n            "Metric",\n            metric_columns,\n            index=metric_columns.index(metric.columns.metric_column),\n            key=f"metric-column-{metric.uid}",\n            on_change=set_columns_field_and_update_dimensions("metric_column", f"metric-column-{metric.uid}"),\n            help="Column containing a quantitative value to aggregate",\n        )\n        cols[1].selectbox(\n            "Timestamp",\n            timestamp_columns,\n            index=timestamp_columns.index(metric.columns.timestamp),\n            key=f"timestamp-{metric.uid}",\n            on_change=set_columns_field_and_update_dimensions("timestamp", f"timestamp-{metric.uid}"),\n            help="Column containing the date of the event",\n        )\n        cols[2].selectbox(\n            "Aggregate Method",\n            AGG_METHODS,\n            index=AGG_METHODS.index(metric.columns.agg_method),\n            key=f"agg-method-{metric.uid}",\n            on_change=set_columns_field("agg_method", f"agg-method-{metric.uid}"),\n            help="Method in which the metric column will be aggregated",\n        )\n        dimensions = st.multiselect(\n            "Dimensions",\n            [col for col in columns if col not in [metric.columns.timestamp, metric.columns.metric_column]],\n            default=metric.columns.dimensions,\n            key=f"dimensions-{metric.uid}",\n            on_change=set_columns_field("dimensions", f"dimensions-{metric.uid}"),\n            help="Columns that will be used as filters",\n        )\n\n        if dimensions:\n            form_column_labels(metric, dimensions)\n\n    with left:\n        st.markdown("##### Preview (first 10)")\n        st.dataframe(metric.preview_dataframe(), use_container_width=True)\n\n\ndef form_column_labels(metric: Metric, dimensions: List[str]) -> None:\n    """Text inputs for editing column labels."""\n    st.markdown("##### Labels")\n\n    def set_label(dimension: str, key: str) -> WidgetCallback:\n        def update_labels() -> None:\n            if value := st.session_state[key].strip():\n                metric.columns.labels.update(**{dimension: value})\n            else:\n                metric.columns.labels.pop(dimension, None)\n\n        return update_labels\n\n    NB_COLS = 3\n    for dims in chunks(dimensions, chunk_size=NB_COLS):\n        for dim, col in zip(dims, st.columns([1] * NB_COLS)):\n            col.text_input(\n                f"Label for `{dim}`",\n                value=metric.columns.labels.get(dim, ""),\n                placeholder=dim,\n                key=f"label-{dim}-{metric.uid}",\n                on_change=set_label(dim, f"label-{dim}-{metric.uid}"),\n            )\n')
    __stickytape_write_module('app_pages/summary_settings_page.py', b'import streamlit as st\n\nfrom app_pages.page import Page\nfrom data.helpers import render_validation_component, repository, validate_state\n\n\nclass SummarySettingsPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "Summary of settings"\n\n    def run(self) -> None:\n        _repo = repository()\n\n        _repo_dimensions = validate_state(_repo.dimensions, "dimension", validate_each=False)\n        _repo_activities = validate_state(_repo.activities, "activity", validate_each=False)\n        _repo_metrics = validate_state(_repo.metrics, "metric", validate_each=False)\n        _repo_each_dimensions = validate_state(_repo.dimensions, "dimension", validate_each=True)\n        _repo_each_activities = validate_state(_repo.activities, "activity", validate_each=True)\n        _repo_dim_act_mappings = validate_state(_repo.dim_act_mappings, "mapping column", permit_empty=True)\n        _repo_each_metrics = validate_state(_repo.metrics, "metric", validate_each=True)\n\n        col_entities, col_raw_data, col_mapping, col_metrics = st.columns(4)\n\n        with col_entities:\n            st.subheader("1. Entities")\n\n            render_validation_component(*_repo_dimensions)\n            render_validation_component(*_repo_activities)\n            render_validation_component(*_repo_metrics)\n\n        with col_raw_data:\n            st.subheader("2. RAW ERP Data")\n\n            render_validation_component(*_repo_each_dimensions)\n            render_validation_component(*_repo_each_activities)\n\n        with col_mapping:\n            st.subheader("3. Mapping")\n\n            render_validation_component(*_repo_dim_act_mappings)\n\n        with col_metrics:\n            st.subheader("4. Metrics")\n\n            render_validation_component(*_repo_each_metrics)\n')
    __stickytape_write_module('app_pages/settings_page.py', b'from typing import Dict, Type\n\nimport streamlit as st\n\nfrom app_pages.data_loading import DataLoadingPage\nfrom app_pages.doma_entities_page import DomaEntitiesPage\nfrom app_pages.mapping import MappingPage\nfrom app_pages.metrics_creation import MetricsCreationPage\nfrom app_pages.page import Page\nfrom app_pages.summary_settings_page import SummarySettingsPage\n\n\nclass SettingsPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "Settings"\n\n    def run(self) -> None:\n        settings_pages: Dict[str, Type[Page]] = {\n            "Summary": SummarySettingsPage,\n            "Entities": DomaEntitiesPage,\n            "RAW Data": DataLoadingPage,\n            "Mapping": MappingPage,\n            "Metrics": MetricsCreationPage,\n        }\n\n        for tab, page in zip(st.tabs(settings_pages.keys()), settings_pages.values()):\n            with tab:\n                page().run()\n')
    __stickytape_write_module('components/__init__.py', b'')
    __stickytape_write_module('components/segmented_control.py', b'from typing import List\n\nimport streamlit as st\nfrom streamlit.runtime.state import WidgetCallback\n\n\ndef segmented_control(labels: List[str], key: str) -> str:\n    """Group of buttons with the given labels. Return the selected label."""\n    if key not in st.session_state:\n        st.session_state[key] = labels[0]\n\n    def set_state(label: str) -> WidgetCallback:\n        return lambda: st.session_state.update(**{key: label})\n\n    cols = st.columns([2] * len(labels) + [12 - len(labels) * 2])\n\n    for col, label in zip(cols, labels):\n        btn_type = "primary" if st.session_state[key] == label else "secondary"\n        col.button(label, on_click=set_state(label), use_container_width=True, type=btn_type)\n\n    return st.session_state[key]\n')
    __stickytape_write_module('streamlit_in_snowflake.py', b'import base64\nfrom pathlib import Path\n\nimport streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.exceptions import SnowparkSessionException\n\n\n@st.cache_resource\ndef is_inside_snowflake() -> bool:\n    """Return true if Streamlit is executed within Snowflake."""\n    try:\n        _ = get_active_session()\n        return True\n    except SnowparkSessionException:\n        return False\n\n\ndef st_image_base64(image_file: str, folder: Path = Path("images"), **kwargs):\n    """Streamlit image using base64 that can be displayed within Snowflake."""\n    if is_inside_snowflake():\n        folder = Path(".")\n\n    content_bytes = (folder / image_file).read_bytes()\n\n    mime_type = image_file.split(".")[-1:][0].lower()\n    st.image(f"data:image/{mime_type};base64,{base64.b64encode(content_bytes).decode()}", **kwargs)\n\n\ndef snowflake_ui_styles():\n    """Replicate the CSS styles when Streamlit is executed within Snowflake."""\n    if is_inside_snowflake():\n        return\n\n    st.markdown(\n        """\n        <style>\n        \n        /* Hide "Made by Streamlit" */\n        .appview-container .main footer {\n            display: none;\n        }\n        \n        /* Hide background of the top header bar */\n        .stApp > header, .stApp > header > div[data-testid="stDecoration"] {\n            background: #ffffff00;\n        }\n\n        /* Reduce top padding */\n        .main > .block-container {\n            padding-top: 1rem;  /* default = 6rem */\n        }\n        \n        </style>\n        """,\n        unsafe_allow_html=True,\n    )\n')
    __stickytape_write_module('sample_configuration.py', b'sample_config = {\n    "dimensions": [\n        {"uid": "52e75e69-d9c3-49e4-9510-106aa91ca5ad", "name": "Products"},\n        {"uid": "11729b5b-8403-431f-b38e-f1d0087e31e6", "name": "Customers"},\n        {"uid": "c8024350-41ed-4725-84f6-c7360e6a15b3", "name": "Nations"},\n        {"uid": "a9283708-8751-410b-ae33-2af4e8e68601", "name": "Order details"},\n    ],\n    "activities": [{"uid": "df3ae0d6-f85b-4e32-bebd-b3e843040cfa", "name": "Sales Order"}],\n    "metrics": [\n        {\n            "uid": "ab5d4cb4-ed1e-4d59-9e47-5260d58ea80f",\n            "name": "Revenue (USD)",\n            "act_id": "df3ae0d6-f85b-4e32-bebd-b3e843040cfa",\n            "columns": {\n                "dimensions": ["P_NAME", "C_NAME", "N_NAME"],\n                "metric_column": "O_TOTALPRICE",\n                "timestamp": "O_ORDERDATE",\n                "agg_method": "sum",\n                "labels": {"P_NAME": "Product", "C_NAME": "Customer", "N_NAME": "Nation"},\n            },\n        },\n        {\n            "uid": "f9511976-0d96-49d1-b162-a2b1cc8b162b",\n            "name": "Sold quantities",\n            "act_id": "df3ae0d6-f85b-4e32-bebd-b3e843040cfa",\n            "columns": {\n                "dimensions": ["P_NAME", "N_NAME"],\n                "metric_column": "L_QUANTITY",\n                "timestamp": "O_ORDERDATE",\n                "agg_method": "sum",\n                "labels": {"P_NAME": "Product", "N_NAME": "Nation"},\n            },\n        },\n    ],\n    "selected_tables": [\n        {\n            "uid": "41361219-d2e7-4dbc-9360-bf0249373ac3",\n            "table_name": "CUSTOMER",\n            "table_schema": "TPCH_SF1",\n            "table_database": "SNOWFLAKE_SAMPLE_DATA",\n            "entity_uid": "11729b5b-8403-431f-b38e-f1d0087e31e6",\n            "entity_type": "dimension",\n            "columns": {\n                "C_CUSTKEY": "INTEGER",\n                "C_NAME": "TEXT",\n                "C_ADDRESS": "TEXT",\n                "C_NATIONKEY": "INTEGER",\n                "C_PHONE": "TEXT",\n                "C_ACCTBAL": "FIXED",\n                "C_MKTSEGMENT": "TEXT",\n                "C_COMMENT": "TEXT",\n            },\n        },\n        {\n            "uid": "c888b750-2c57-428d-91f8-e0b0238fbbe4",\n            "table_name": "PART",\n            "table_schema": "TPCH_SF1",\n            "table_database": "SNOWFLAKE_SAMPLE_DATA",\n            "entity_uid": "52e75e69-d9c3-49e4-9510-106aa91ca5ad",\n            "entity_type": "dimension",\n            "columns": {\n                "P_PARTKEY": "INTEGER",\n                "P_NAME": "TEXT",\n                "P_MFGR": "TEXT",\n                "P_BRAND": "TEXT",\n                "P_TYPE": "TEXT",\n                "P_SIZE": "INTEGER",\n                "P_CONTAINER": "TEXT",\n                "P_RETAILPRICE": "FIXED",\n                "P_COMMENT": "TEXT",\n            },\n        },\n        {\n            "uid": "a5e27063-bf81-4474-b1f3-1c88c766f3e2",\n            "table_name": "NATION",\n            "table_schema": "TPCH_SF1",\n            "table_database": "SNOWFLAKE_SAMPLE_DATA",\n            "entity_uid": "c8024350-41ed-4725-84f6-c7360e6a15b3",\n            "entity_type": "dimension",\n            "columns": {"N_NATIONKEY": "INTEGER", "N_NAME": "TEXT", "N_REGIONKEY": "INTEGER", "N_COMMENT": "TEXT"},\n        },\n        {\n            "uid": "0b65dcce-47ea-4cb4-a39d-6d89ab8cfdd8",\n            "table_name": "ORDERS",\n            "table_schema": "TPCH_SF1",\n            "table_database": "SNOWFLAKE_SAMPLE_DATA",\n            "entity_uid": "a9283708-8751-410b-ae33-2af4e8e68601",\n            "entity_type": "dimension",\n            "columns": {\n                "O_ORDERKEY": "INTEGER",\n                "O_CUSTKEY": "INTEGER",\n                "O_ORDERSTATUS": "TEXT",\n                "O_TOTALPRICE": "FIXED",\n                "O_ORDERDATE": "DATE",\n                "O_ORDERPRIORITY": "TEXT",\n                "O_CLERK": "TEXT",\n                "O_SHIPPRIORITY": "INTEGER",\n                "O_COMMENT": "TEXT",\n            },\n        },\n        {\n            "uid": "51ba4e44-5d02-4093-921f-b5b1c30ac330",\n            "table_name": "LINEITEM",\n            "table_schema": "TPCH_SF1",\n            "table_database": "SNOWFLAKE_SAMPLE_DATA",\n            "entity_uid": "df3ae0d6-f85b-4e32-bebd-b3e843040cfa",\n            "entity_type": "activity",\n            "columns": {\n                "L_ORDERKEY": "INTEGER",\n                "L_PARTKEY": "INTEGER",\n                "L_SUPPKEY": "INTEGER",\n                "L_LINENUMBER": "INTEGER",\n                "L_QUANTITY": "FIXED",\n                "L_EXTENDEDPRICE": "FIXED",\n                "L_DISCOUNT": "FIXED",\n                "L_TAX": "FIXED",\n                "L_RETURNFLAG": "TEXT",\n                "L_LINESTATUS": "TEXT",\n                "L_SHIPDATE": "DATE",\n                "L_COMMITDATE": "DATE",\n                "L_RECEIPTDATE": "DATE",\n                "L_SHIPINSTRUCT": "TEXT",\n                "L_SHIPMODE": "TEXT",\n                "L_COMMENT": "TEXT",\n            },\n        },\n    ],\n    "dim_act_mappings": [\n        {\n            "dim_id": "52e75e69-d9c3-49e4-9510-106aa91ca5ad",\n            "act_id": "df3ae0d6-f85b-4e32-bebd-b3e843040cfa",\n            "main_table_id": "51ba4e44-5d02-4093-921f-b5b1c30ac330",\n            "joined_table_id": "c888b750-2c57-428d-91f8-e0b0238fbbe4",\n            "main_column": "L_PARTKEY",\n            "joined_column": "P_PARTKEY",\n        },\n        {\n            "dim_id": "11729b5b-8403-431f-b38e-f1d0087e31e6",\n            "act_id": "df3ae0d6-f85b-4e32-bebd-b3e843040cfa",\n            "main_table_id": "0b65dcce-47ea-4cb4-a39d-6d89ab8cfdd8",\n            "joined_table_id": "41361219-d2e7-4dbc-9360-bf0249373ac3",\n            "main_column": "O_CUSTKEY",\n            "joined_column": "C_CUSTKEY",\n        },\n        {\n            "dim_id": "c8024350-41ed-4725-84f6-c7360e6a15b3",\n            "act_id": "df3ae0d6-f85b-4e32-bebd-b3e843040cfa",\n            "main_table_id": "41361219-d2e7-4dbc-9360-bf0249373ac3",\n            "joined_table_id": "a5e27063-bf81-4474-b1f3-1c88c766f3e2",\n            "main_column": "C_NATIONKEY",\n            "joined_column": "N_NATIONKEY",\n        },\n        {\n            "dim_id": "a9283708-8751-410b-ae33-2af4e8e68601",\n            "act_id": "df3ae0d6-f85b-4e32-bebd-b3e843040cfa",\n            "main_table_id": "51ba4e44-5d02-4093-921f-b5b1c30ac330",\n            "joined_table_id": "0b65dcce-47ea-4cb4-a39d-6d89ab8cfdd8",\n            "main_column": "L_ORDERKEY",\n            "joined_column": "O_ORDERKEY",\n        },\n    ],\n    "visualizations": {\n        "1036591095770": {\n            "viz_type": "range_comparison",\n            "comparison": {\n                "metric": {\n                    "uid": "f9511976-0d96-49d1-b162-a2b1cc8b162b",\n                    "name": "Sold quantities",\n                    "act_id": "df3ae0d6-f85b-4e32-bebd-b3e843040cfa",\n                    "columns": {\n                        "dimensions": ["P_NAME", "N_NAME"],\n                        "metric_column": "L_QUANTITY",\n                        "timestamp": "O_ORDERDATE",\n                        "agg_method": "sum",\n                        "labels": {"P_NAME": "Product", "N_NAME": "Nation"},\n                    },\n                },\n                "range_periodicity": "month",\n                "historical_dates": {"start": "1997-01-01", "end": "1997-12-31"},\n                "current_period_date": "1998-08-02",\n                "filters": {"mapping": {"N_NAME": ["CANADA"]}},\n            },\n        },\n        "703418588003": {\n            "viz_type": "time_series",\n            "time_series": {\n                "metric": {\n                    "uid": "ab5d4cb4-ed1e-4d59-9e47-5260d58ea80f",\n                    "name": "Revenue (USD)",\n                    "act_id": "df3ae0d6-f85b-4e32-bebd-b3e843040cfa",\n                    "columns": {\n                        "dimensions": ["P_NAME", "C_NAME", "N_NAME"],\n                        "metric_column": "O_TOTALPRICE",\n                        "timestamp": "O_ORDERDATE",\n                        "agg_method": "sum",\n                        "labels": {"P_NAME": "Product", "C_NAME": "Customer", "N_NAME": "Nation"},\n                    },\n                },\n                "grain": "month",\n            },\n            "filters": {"mapping": {"N_NAME": ["CANADA"]}},\n        },\n    },\n}\n')
    import json
    from pathlib import Path
    
    import streamlit as st
    
    # noinspection PyUnresolvedReferences
    import range_analysis_app.components.streamlit_nested_layout  # import for nested columns support
    from app_pages.about_page import AboutPage
    from app_pages.dashboard import DashboardPage
    from app_pages.data_analysis import DataAnalysisPage
    from app_pages.help_page import HelpPage
    from app_pages.settings_page import SettingsPage
    from components.segmented_control import segmented_control
    from data.helpers import save_repo_button
    from data.repository import Repository
    from streamlit_in_snowflake import snowflake_ui_styles, st_image_base64
    
    st.set_page_config(layout="wide", initial_sidebar_state="collapsed")
    
    snowflake_ui_styles()
    
    title_col, save_button_col = st.columns([8, 4])
    with title_col:
        st_image_base64("banner.png", width=400)
    with save_button_col:
        save_repo_button()
    
    
    # Define the app's pages
    pages = {
        "Dashboard": DashboardPage,
        "<= Widget creation": DataAnalysisPage,
        "Configuration": SettingsPage,
        "Help": HelpPage,
        "About Maxa": AboutPage,
    }
    
    
    selected_page = segmented_control(list(pages.keys()), key="current_page")
    selected_page_obj = pages[selected_page]()
    st.sidebar.write(selected_page_obj.page_description)
    
    if st.sidebar.button("Reset to sample Data"):
        from sample_configuration import sample_config
    
        st.session_state.repository = Repository.parse_obj(sample_config)
    
    # Call the selected page's function
    selected_page_obj.run()
    