#!/usr/bin/env python
import contextlib as __stickytape_contextlib

@__stickytape_contextlib.contextmanager
def __stickytape_temporary_dir():
    import tempfile
    import shutil
    dir_path = tempfile.mkdtemp()
    try:
        yield dir_path
    finally:
        shutil.rmtree(dir_path)

with __stickytape_temporary_dir() as __stickytape_working_dir:
    def __stickytape_write_module(path, contents):
        import os, os.path

        def make_package(path):
            parts = path.split("/")
            partial_path = __stickytape_working_dir
            for part in parts:
                partial_path = os.path.join(partial_path, part)
                if not os.path.exists(partial_path):
                    os.mkdir(partial_path)
                    with open(os.path.join(partial_path, "__init__.py"), "wb") as f:
                        f.write(b"\n")

        make_package(os.path.dirname(path))

        full_path = os.path.join(__stickytape_working_dir, path)
        with open(full_path, "wb") as module_file:
            module_file.write(contents)

    import sys as __stickytape_sys
    __stickytape_sys.path.insert(0, __stickytape_working_dir)

    __stickytape_write_module('erp_sales_assessment/external/__init__.py', b'"""\nThis `external` package contains libraries that are not available in the [Snowflake Snowpark for Python](https://repo.anaconda.com/pkgs/snowflake/).\n"""\n')
    __stickytape_write_module('erp_sales_assessment/external/streamlit_nested_layout.py', b'# Implementation from streamlit-nested-layout => https://github.com/joy13975/streamlit-nested-layout\n# This package is not available in the Snowflake Anaconda Channel => https://repo.anaconda.com/pkgs/snowflake/\n\nfrom streamlit.delta_generator import *\nfrom streamlit.delta_generator import _enqueue_message\n\n\ndef _nestable_block(\n    self: "DeltaGenerator",\n    block_proto: Block_pb2.Block = Block_pb2.Block(),\n) -> "DeltaGenerator":\n    # Operate on the active DeltaGenerator, in case we\'re in a `with` block.\n    dg = self._active_dg\n\n    # Prevent nested columns & expanders by checking all parents.\n    block_type = block_proto.WhichOneof("type")\n    # Convert the generator to a list, so we can use it multiple times.\n    # parent_block_types = frozenset(dg._parent_block_types)\n    # if block_type == "column" and block_type in parent_block_types:\n    #     raise StreamlitAPIException(\n    #         "Columns may not be nested inside other columns."\n    #     )\n    # if block_type == "expandable" and block_type in parent_block_types:\n    #     raise StreamlitAPIException(\n    #         "Expanders may not be nested inside other expanders."\n    #     )\n\n    if dg._root_container is None or dg._cursor is None:\n        return dg\n\n    msg = ForwardMsg_pb2.ForwardMsg()\n    msg.metadata.delta_path[:] = dg._cursor.delta_path\n    msg.delta.add_block.CopyFrom(block_proto)\n\n    # Normally we\'d return a new DeltaGenerator that uses the locked cursor\n    # below. But in this case we want to return a DeltaGenerator that uses\n    # a brand new cursor for this new block we\'re creating.\n    block_cursor = cursor.RunningCursor(\n        root_container=dg._root_container,\n        parent_path=dg._cursor.parent_path + (dg._cursor.index,),\n    )\n    block_dg = DeltaGenerator(\n        root_container=dg._root_container,\n        cursor=block_cursor,\n        parent=dg,\n        block_type=block_type,\n    )\n    # Blocks inherit their parent form ids.\n    # NOTE: Container form ids aren\'t set in proto.\n    block_dg._form_data = FormData(current_form_id(dg))\n\n    # Must be called to increment this cursor\'s index.\n    dg._cursor.get_locked_cursor(last_index=None)\n    _enqueue_message(msg)\n\n    caching.save_block_message(\n        block_proto,\n        invoked_dg_id=self.id,\n        used_dg_id=dg.id,\n        returned_dg_id=block_dg.id,\n    )\n\n    return block_dg\n\n\nDeltaGenerator._block = _nestable_block\n')
    __stickytape_write_module('erp_sales_assessment/app_pages/__init__.py', b'')
    __stickytape_write_module('erp_sales_assessment/app_pages/about_page.py', b'import streamlit as st\n\nfrom erp_sales_assessment.app_pages.page import Page\n\n\nclass AboutPage(Page):\n    def run(self) -> None:\n        st.markdown(\n            """\n            Maxa is a leading edge financial data platform powered by Snowflake that allows you to:\n            - Transform and blend raw data from multiple system of record\n            - Automate data extraction from ERP and core systems\n            - Navigate simply within easy to use data model\n            - Gain actionable insights and drive improvements\n            - Produce insights quickly, no special skills required!\n\n            If you are interested to see how Maxa can transform your business, contact us:\n            - Email: [snowflake-info@maxa.ai](mailto:snowflake-info@maxa.ai)\n            - Website: [maxa.ai/contact](https://www.maxa.ai/contact/)\n            """\n        )\n')
    __stickytape_write_module('erp_sales_assessment/app_pages/page.py', b'import abc\n\n\nclass Page(abc.ABC):\n    @abc.abstractmethod\n    def run(self) -> None:\n        pass\n')
    __stickytape_write_module('erp_sales_assessment/app_pages/configuration/__init__.py', b'')
    __stickytape_write_module('erp_sales_assessment/app_pages/configuration/configuration_page.py', b'import streamlit as st\n\nfrom erp_sales_assessment.app_pages.configuration.data_source_and_entities import form_select_source_and_define_entities\nfrom erp_sales_assessment.app_pages.configuration.entity_table_relationship import (\n    entities_are_valid,\n    form_entity_table_relationship,\n)\nfrom erp_sales_assessment.app_pages.configuration.metric_definition import form_metric_definition, mapping_is_valid\nfrom erp_sales_assessment.app_pages.page import Page\nfrom erp_sales_assessment.components.action_button import action_button\nfrom erp_sales_assessment.data import db\nfrom erp_sales_assessment.data.helpers import has_current_repository_changed, repository, save_custom_repository\n\n\nclass ConfigurationPage(Page):\n    def run(self) -> None:\n        MODES = ["Demo Mode", "Custom Mode"]\n\n        def update_repo():\n            st.session_state["selected_repository"] = st.session_state["second-repo"]\n\n        st.radio(\n            "**Data Source**",\n            MODES,\n            index=MODES.index(st.session_state["selected_repository"]),\n            horizontal=True,\n            key="second-repo",\n            on_change=update_repo,\n        )\n\n        metric = repository().metrics[0]\n        step1_valid = entities_are_valid()\n        step2_valid = mapping_is_valid(metric)\n        step3_valid = repository().is_valid()\n\n        with st.expander("**Step 1 - Select Data Source and Map Dimensions**", expanded=not step1_valid):\n            with db.catch_errors():\n                form_select_source_and_define_entities()\n\n        with st.expander("**Step 2 - Specify Relationships Between Tables**", expanded=step1_valid and not step2_valid):\n            if not step1_valid:\n                st.warning("Missing mapping in **:blue[Step 1]**.")\n            else:\n                with db.catch_errors():\n                    form_entity_table_relationship()\n\n        with st.expander("**Step 3 - Select Attributes**", expanded=step2_valid and not step3_valid):\n            if not step2_valid:\n                st.warning("Missing mapping in **:blue[Step 1]** and **:blue[Step 2]**.")\n            else:\n                with db.catch_errors():\n                    form_metric_definition(metric)\n\n        cols = st.columns([10, 2])\n        with cols[1]:\n            if st.session_state["selected_repository"] == "Custom Mode":\n\n                def apply_and_save():\n                    apply_config_to_dashboard()\n                    save_custom_repository()\n\n                action_button(\n                    "Save Configuration",\n                    message="Saved.",\n                    key="save-config",\n                    disabled=not has_current_repository_changed(exclude={"widgets"}),\n                    on_click=apply_and_save,\n                )\n\n                if last_updated_at := st.session_state.get("last_updated_at"):\n                    st.markdown(f"Last updated on {last_updated_at.strftime(\'%Y-%m-%d %H:%M\')}")\n\n            else:\n                action_button(\n                    "Apply configuration",\n                    message="Applied.",\n                    key="save-config",\n                    disabled=not has_current_repository_changed(exclude={"widgets"}),\n                    on_click=apply_config_to_dashboard,\n                )\n\n\ndef apply_config_to_dashboard():\n    repo = repository()\n    updated_metric = repo.metrics[0]\n    for widget in repo.widgets:\n        widget.metric = updated_metric.copy(deep=True)\n')
    __stickytape_write_module('erp_sales_assessment/app_pages/configuration/data_source_and_entities.py', b'from dataclasses import dataclass\nfrom typing import Dict, List, Optional, Union\n\nimport streamlit as st\n\nfrom erp_sales_assessment.components.selectbox_optional import selectbox_optional\nfrom erp_sales_assessment.data import db\nfrom erp_sales_assessment.data.helpers import repository\nfrom erp_sales_assessment.data.models import Activity, DbTable, Dimension\n\nPREDEFINED_DIMS = ["Purchase Order", "Product", "Category", "Customer", "Supplier", "Country", "Orders"]\n\nOBSERVED_FIELDS = {"dimensions", "activities", "selected_tables"}\n\n\n@dataclass\nclass DataSource:\n    database: str\n    schema: str\n    table_columns: Dict[str, Dict[str, db.ExtendedDataType]]\n\n    @property\n    def tables(self) -> List[str]:\n        return list(self.table_columns.keys())\n\n    def contains(self, table: Optional[DbTable]) -> bool:\n        if table is None:\n            return False\n\n        return (\n            table.table_database == self.database\n            and table.table_schema == self.schema\n            and table.table_name in self.tables\n        )\n\n\ndef form_select_source_and_define_entities() -> None:\n    """Form to select the source database, define Dimension entities and their related table."""\n    entities = repository().activities + repository().dimensions\n\n    left, right = st.columns([6, 6])\n    with left:\n        selected_data_source = _select_data_source()\n        with st.expander("How to add your database ?"):\n            current_app_name = db.fetch_current_database()\n\n            st.markdown(\n                f"""\n                To add your database, you must grant access to it to the application. For example:\n\n                ```sql\n                grant usage on database "SNOWFLAKE_SAMPLE_DATA" to application "{current_app_name}";\n                grant usage on schema "SNOWFLAKE_SAMPLE_DATA"."TPCH_SF1" to application "{current_app_name}";\n                grant select on all tables in schema "SNOWFLAKE_SAMPLE_DATA"."TPCH_SF1" to application "{current_app_name}";\n                grant select on all views in schema "SNOWFLAKE_SAMPLE_DATA"."TPCH_SF1" to application "{current_app_name}";\n                ```\n\n                Then refresh the databases:\n                """\n            )\n            st.button("Refresh databases", on_click=db.refresh_databases)\n    with right:\n        assign_entities_to_tables(entities, selected_data_source)\n\n\ndef _select_data_source() -> Optional[DataSource]:\n    """Form to select a data source ie. a database and a schema with its tables."""\n    if repository().selected_tables:\n        first_table = repository().selected_tables[0]\n        existing_database = first_table.table_database\n        existing_schema = first_table.table_schema\n    else:\n        existing_database = None\n        existing_schema = None\n\n    left, right = st.columns([6, 6])\n    with left:\n        st.markdown("**Database**")\n        database = selectbox_optional(\n            "Database",\n            none_label="Select database...",\n            key="selected_database",\n            options=db.fetch_databases(),\n            value=existing_database,\n            label_visibility="collapsed",\n        )\n        if not database:\n            return None\n\n    with right:\n        st.markdown("**Schema**")\n        schema = selectbox_optional(\n            "Schema",\n            none_label="Select schema...",\n            key="selected_schema",\n            options=db.fetch_schemas(database),\n            value=existing_schema,\n            label_visibility="collapsed",\n        )\n        if not schema:\n            return None\n\n    tables = db.fetch_all_table_columns(database, schema)\n    if not tables:\n        return None\n\n    return DataSource(database=database, schema=schema, table_columns=tables)\n\n\ndef assign_entities_to_tables(entities: List[Union[Dimension, Activity]], data_source: Optional[DataSource]):\n    remaining_dims = [dim for dim in PREDEFINED_DIMS if dim not in (entity.name for entity in entities)]\n\n    cols = st.columns([5, 5, 2])\n    cols[0].markdown("**Define dimensions**")\n    cols[1].markdown("**Associate table source**")\n\n    for entity in entities:\n        assign_entity_to_table(entity, data_source, remaining_dims)\n\n    cols = st.columns([5, 5, 2])\n\n    if remaining_dims:\n\n        def create_dimension():\n            dim = Dimension.create(name=remaining_dims[0])\n            repository().activities[0].assign_dimension(dim.uid)\n\n        cols[0].button("\\+ Add dimension", use_container_width=True, on_click=create_dimension)\n\n\ndef assign_entity_to_table(\n    entity: Union[Dimension, Activity],\n    data_source: Optional[DataSource],\n    remaining_dims: List[str],\n):\n    cols = st.columns([5, 5, 2])\n\n    with cols[0]:\n        if entity.predefined:\n            st.text_input(\n                "Entity name",\n                value=entity.name,\n                disabled=True,\n                label_visibility="collapsed",\n            )\n        else:\n\n            def update_entity_name():\n                entity.name = st.session_state[f"entity-name-{entity.uid}"]\n\n            st.selectbox(\n                "Entity name",\n                [entity.name] + remaining_dims,\n                label_visibility="collapsed",\n                key=f"entity-name-{entity.uid}",\n                on_change=update_entity_name,\n            )\n\n    table = entity.db_table\n\n    # Is in an other data source\n    if table and (data_source is None or not data_source.contains(table)):\n        full_name = remove_prefix(table.full_name, f"{data_source.database}.") if data_source else table.full_name\n\n        cols[1].text_input(\n            "Source table",\n            value=full_name,\n            key=f"source-table-{entity.uid}",\n            disabled=True,\n            label_visibility="collapsed",\n        )\n        if data_source:\n            cols[2].button("\xe2\x9c\x8e", key=f"edit_{entity.uid}", on_click=table.delete)\n        return\n\n    with cols[1]:\n\n        def update_selected_table():\n            if table:\n                table.delete()\n\n            selected_table_name = st.session_state[f"source-table-{entity.uid}"]\n            if selected_table_name is None or data_source is None:\n                return\n\n            DbTable.create(\n                table_database=data_source.database,\n                table_schema=data_source.schema,\n                table_name=selected_table_name,\n                columns=data_source.table_columns[selected_table_name],\n                entity=entity,\n            )\n\n        selectbox_optional(\n            "Source table",\n            key=f"source-table-{entity.uid}",\n            none_label="Select table...",\n            options=data_source.tables if data_source else [],\n            disabled=data_source is None,\n            value=table.table_name if table else None,\n            label_visibility="collapsed",\n            on_change=update_selected_table,\n        )\n\n    if not entity.predefined:\n        cols[2].button("\xe2\x9c\x95", key=f"remove_{entity.uid}", on_click=entity.delete)\n\n\ndef remove_prefix(text: str, prefix: str) -> str:\n    """Remove the given prefix from the text."""\n    if text.startswith(prefix):\n        return text[len(prefix) :]\n    return text\n')
    __stickytape_write_module('erp_sales_assessment/components/__init__.py', b'')
    __stickytape_write_module('erp_sales_assessment/components/selectbox_optional.py', b'from typing import Any, List, Optional, TypeVar\n\nimport streamlit as st\n\nT = TypeVar("T")\n\n\ndef selectbox_optional(\n    label: str,\n    options: List[T] = [],\n    value: Optional[T] = None,\n    none_label: str = "Select option...",\n    **kwargs: Any,\n) -> Optional[T]:\n    """Selectbox with `None` as the default option."""\n    if value in options:\n        index = options.index(value)\n    else:  # Only show the None option if there is no selection\n        options = [None] + options\n        index = 0\n\n    return st.selectbox(\n        label,\n        options=options,\n        index=index,\n        format_func=lambda item: none_label if item is None else item,\n        **kwargs,\n    )\n')
    __stickytape_write_module('erp_sales_assessment/data/__init__.py', b'')
    __stickytape_write_module('erp_sales_assessment/data/db.py', b'import json\nimport re\nfrom contextlib import contextmanager\nfrom typing import Dict, Iterable, List, Literal, Tuple, TypedDict, Union, get_args\n\nimport pandas as pd\nimport snowflake.snowpark.functions as f\nimport streamlit as st\nfrom pydantic import Field\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.exceptions import SnowparkSessionException, SnowparkSQLException\nfrom typing_extensions import NotRequired\n\nfrom erp_sales_assessment.utils import BaseModelCacheKey, groupby\n\n\n@st.cache_resource\ndef snowflake_session() -> Session:\n    try:\n        return get_active_session()\n    except SnowparkSessionException:\n        return Session.builder.configs(st.secrets["snowflake"]).create()\n\n\ndef refresh_databases() -> None:\n    """Clear the cache of all the queries fetching database definitions."""\n    fetch_databases.clear()\n    fetch_schemas.clear()\n    fetch_columns.clear()\n    fetch_all_table_columns.clear()\n\n\ndef db_identifier(name: str) -> str:\n    """Return an identifier to reference the given object name in a SQL query (table, schema, database).\n\n    It double quotes the name when it is non standard (not SCREAM_CASE).\n    """\n    return name if re.match(r"^[A-Z_]+$", name) else f\'"{name}"\'\n\n\n@st.cache_data\ndef fetch_databases() -> List[str]:\n    return [db_identifier(row.name) for row in snowflake_session().sql(f"show databases").collect()]\n\n\n@st.cache_data\ndef fetch_current_database() -> str:\n    row = snowflake_session().sql(f"select current_database() as current_database").first()\n    return row.CURRENT_DATABASE\n\n\n@st.cache_data\ndef fetch_schemas(database: str = "") -> List[str]:\n    return [\n        db_identifier(row.name) for row in snowflake_session().sql(f"show schemas in database {database}").collect()\n    ]\n\n\n@st.cache_data\ndef preview_table(table: str, limit: int = 5) -> pd.DataFrame:\n    return pd.DataFrame(snowflake_session().table(table).limit(limit).collect())\n\n\nDatetimeType = Literal["DATE", "TIME", "TIMESTAMP", "TIMESTAMP_LTZ", "TIMESTAMP_NTZ"]\nDataType = Union[Literal["TEXT", "FIXED", "REAL", "BINARY", "BOOLEAN", "ARRAY", "OBJECT"], DatetimeType]\nExtendedDataType = Union[DataType, Literal["INTEGER"]]\n\nDATETIME_TYPES: List[DatetimeType] = list(get_args(DatetimeType))\nID_TYPES: List[ExtendedDataType] = ["TEXT", "INTEGER", "BINARY", "ARRAY"]\n\n\n@st.cache_data\ndef fetch_columns(table: str) -> Dict[str, ExtendedDataType]:\n    return {\n        db_identifier(row.column_name): _parse_data_type(json.loads(row.data_type))\n        for row in snowflake_session().sql(f"show columns in table {table}").collect()\n    }\n\n\n@st.cache_data\ndef fetch_all_table_columns(database: str, schema: str) -> Dict[str, Dict[str, ExtendedDataType]]:\n    """Return a mapping of { table_name => { column_name => data type } } for all tables and views in the schema."""\n    all_rows = snowflake_session().sql(f"show columns in schema {database}.{schema}").collect()\n\n    return {\n        table_name: {db_identifier(row.column_name): _parse_data_type(json.loads(row.data_type)) for row in rows}\n        for table_name, rows in groupby(all_rows, key=lambda row: db_identifier(row.table_name)).items()\n    }\n\n\nclass DataTypeDict(TypedDict):\n    """Represent the `data_type` value in Snowflake `SHOW COLUMNS IN TABLE`."""\n\n    type: DataType\n    nullable: bool\n\n    # for type FIXED and TIME/TIMESTAMP\n    precision: NotRequired[int]\n    scale: NotRequired[int]\n\n    # for type TEXT and BINARY\n    length: NotRequired[int]\n    bytesLength: NotRequired[int]\n    fixed: NotRequired[bool]\n\n\ndef _parse_data_type(data_type: DataTypeDict) -> ExtendedDataType:\n    if data_type["type"] == "FIXED" and data_type["scale"] == 0:\n        return "INTEGER"\n\n    return data_type["type"]\n\n\nclass Filters(BaseModelCacheKey):\n    mapping: Dict[str, List[str]] = Field(default_factory=dict)\n\n    def add(self, key: str, values: List[str]) -> None:\n        self.mapping[key] = values\n\n    def to_condition(self) -> f.Column:\n        """Return a Snowpark filter condition to be used in the `DataFrame.filter` or `DataFrame.where` methods."""\n        condition = f.lit(True)\n        for column, values in self.mapping.items():\n            condition &= f.col(column).in_(values)\n        return condition\n\n    def is_empty(self) -> bool:\n        return len(self.mapping) == 0\n\n\n@contextmanager\ndef catch_errors():\n    try:\n        yield\n    except SnowparkSQLException as _:\n        st.error("An error happened in the SQL query. Please verify the configuration or refresh the page.")\n    except:\n        st.error("An unexpected error happened. Please try to refresh the page or reinstall the app.")\n')
    __stickytape_write_module('erp_sales_assessment/utils.py', b'from collections import defaultdict\nfrom typing import Callable, DefaultDict, Dict, Iterable, List, Optional, Tuple, Type, TypeVar\n\nfrom inflection import pluralize as inflection_pluralize\nfrom pydantic import BaseModel\n\nT = TypeVar("T")\nK = TypeVar("K")\n\n\ndef pluralize(count: int, singular: str) -> str:\n    return f"{count} {inflection_pluralize(singular) if count > 1 else singular}"\n\n\ndef chunks(values: List[T], chunk_size: int) -> Iterable[List[T]]:\n    """Split the given values into multiple chunks of the given size."""\n    for index in range(0, len(values), chunk_size):\n        yield values[index : index + chunk_size]\n\n\nclass BaseModelCacheKey(BaseModel):\n    """Model that can be used as caching key in st.cache_data."""\n\n    def __reduce__(self) -> Tuple[Type, Tuple[str, ...]]:\n        """Function from pickle used by the hasher of st.cache_data."""\n        return (self.parse_raw, (self.json(),))\n\n\ndef groupby(values: List[T], key: Callable[[T], K]) -> Dict[K, List[T]]:\n    result: DefaultDict[K, List[T]] = defaultdict(list)\n\n    for value in values:\n        result[key(value)].append(value)\n\n    return result\n\n\ndef first(values: List[T]) -> Optional[T]:\n    return values[0] if values else None\n')
    __stickytape_write_module('erp_sales_assessment/data/helpers.py', b'from datetime import datetime\nfrom typing import TYPE_CHECKING, Any, Callable, List, Literal, Set, Tuple, get_args\n\nimport streamlit as st\nfrom inflection import pluralize as inflection_pluralize\nfrom snowflake.snowpark.exceptions import SnowparkSQLException\nfrom snowflake.snowpark.types import StringType, StructField, StructType, TimestampType\n\nfrom erp_sales_assessment.data import db\nfrom erp_sales_assessment.utils import pluralize\n\nif TYPE_CHECKING:\n    from data.models import DomaBaseModel\n    from data.repository import Repository\n\n\nDB_TABLE = "doma_entities"\nDB_TABLE_SCHEMA = StructType(\n    [\n        StructField("name", StringType()),\n        StructField("json_data", StringType()),\n        StructField("updated_at", TimestampType()),\n    ]\n)\n\n\ndef create_table_if_not_exists() -> None:\n    try:\n        db.snowflake_session().table(DB_TABLE).collect()\n    except SnowparkSQLException as e:\n        (\n            db.snowflake_session()\n            .create_dataframe([], schema=DB_TABLE_SCHEMA)\n            .write.save_as_table(DB_TABLE, mode="overwrite")\n        )\n\n\ndef load_and_select_repository() -> None:\n    default_mode = "Demo Mode"\n\n    if "repository" not in st.session_state:\n        from erp_sales_assessment.data.repository import Repository\n        from erp_sales_assessment.data.sample_configuration import sample_config\n\n        create_table_if_not_exists()\n        st.session_state["demo_repository"] = Repository.parse_obj(sample_config)\n        st.session_state["prev_demo_repository"] = st.session_state.demo_repository.copy(deep=True)\n\n        row = db.snowflake_session().table(DB_TABLE).first()\n\n        if row is None:\n            st.session_state["repository"] = Repository.default_for_purchase_orders()\n            st.session_state["prev_repository"] = st.session_state.repository.copy(deep=True)\n        else:\n            data = row.JSON_DATA\n            st.session_state["repository"] = Repository.parse_raw(data)\n            st.session_state["prev_repository"] = st.session_state.repository.copy(deep=True)\n            st.session_state["last_updated_at"] = row.UPDATED_AT\n\n            if st.session_state.repository.is_valid():\n                default_mode = "Custom Mode"\n\n    MODES = ["Demo Mode", "Custom Mode"]\n\n    st.sidebar.radio(\n        "**Data Source**",\n        MODES,\n        index=MODES.index(default_mode),\n        horizontal=True,\n        key="selected_repository",\n    )\n\n\ndef repository() -> "Repository":\n    """Return the currently selected repository."""\n    if st.session_state.get("selected_repository") == "Custom Mode":\n        return st.session_state.repository\n    else:\n        return st.session_state.demo_repository\n\n\ndef save_custom_repository(include: Set[str] = None, exclude: Set[str] = None) -> None:\n    """Save the custom repository in the database, by only updating the given fields."""\n    from erp_sales_assessment.data.repository import Repository\n\n    with st.spinner():\n        updated_fields = st.session_state.repository.dict(include=include, exclude=exclude)\n        kept_fields = st.session_state.prev_repository.dict(include=exclude, exclude=include)\n\n        repo_to_save = Repository.parse_obj({**kept_fields, **updated_fields})\n\n        (\n            db.snowflake_session()\n            .create_dataframe([["Custom config", repo_to_save.json(), datetime.now()]], schema=DB_TABLE_SCHEMA)\n            .write.save_as_table(DB_TABLE, mode="overwrite")\n        )\n        st.session_state.prev_repository = st.session_state.repository.copy(deep=True)\n        st.session_state.last_updated_at = datetime.now()\n\n\ndef has_current_repository_changed(include: Set[str] = None, exclude: Set[str] = None) -> bool:\n    """Return true if the given fields of the current repository have changed."""\n    if st.session_state.get("selected_repository") == "Custom Mode":\n        current = st.session_state.repository\n        previous = st.session_state.prev_repository\n    else:\n        current = st.session_state.demo_repository\n        previous = st.session_state.prev_demo_repository\n\n    a = previous.json(include=include, exclude=exclude)\n    b = current.json(include=include, exclude=exclude)\n    return a != b\n\n\ndef validate_state(\n    models: List["DomaBaseModel"],\n    entity_name: str,\n    permit_empty: bool = False,\n    validate_each: bool = True,\n) -> Tuple[bool, str]:\n    """Return a pair (is_valid, message) describing the validation state of the given models."""\n\n    if len(models) == 0 and not permit_empty:\n        return False, f"{entity_name} cannot be empty"\n\n    if validate_each:\n        try:\n            success_message = [f"{pluralize(len(models), entity_name)} has been set"]\n            for model in models:\n                model.validate_entity()\n                success_message.append(f"- {model.validation_item_name()}")\n\n            return True, "\\n".join(success_message)\n        except ValueError as e:\n            return False, str(e)\n\n    success_message = f"All set for the {inflection_pluralize(entity_name)}"\n    return True, "\\n".join([success_message] + [f"- {x.validation_item_name()}" for x in models])\n\n\ndef render_validation_component(is_valid: bool, message: str):\n    if is_valid:\n        st.success(message, icon="\xe2\x9c\x94\xef\xb8\x8f")\n    else:\n        st.error(message, icon="\xe2\x9c\x96\xef\xb8\x8f")\n')
    __stickytape_write_module('erp_sales_assessment/data/repository.py', b'from typing import TYPE_CHECKING, Dict, List, Optional, Union\n\nfrom pydantic import Field\nfrom pydantic.main import BaseModel\nfrom typing_extensions import Annotated\n\nfrom erp_sales_assessment.data.helpers import validate_state\nfrom erp_sales_assessment.data.metric import Metric\nfrom erp_sales_assessment.data.models import (\n    Activity,\n    DbTable,\n    Dimension,\n    DimensionActivityMapping,\n    Entity,\n    EntityType,\n    Id,\n)\nfrom erp_sales_assessment.widgets.kpis import TotalPurchaseValue\nfrom erp_sales_assessment.widgets.outlier_detection_widgets import PurchaseAnomaliesHeatmap\nfrom erp_sales_assessment.widgets.product_category_distribution_widgets import (\n    ProductCategoriesComparisonBarGraph,\n    ProductCategoryDistributionPieChart,\n)\nfrom erp_sales_assessment.widgets.purchasing_trend_over_time_widgets import PurchasingTrendsLineChart\n\nENTITY_TYPE_TO_COLLECTION: Dict[EntityType, str] = {\n    "dimension": "dimensions",\n    "activity": "activities",\n    "metric": "metrics",\n}\n\n\nWidgetType = Annotated[\n    Union[\n        ProductCategoryDistributionPieChart,\n        ProductCategoriesComparisonBarGraph,\n        PurchaseAnomaliesHeatmap,\n        PurchasingTrendsLineChart,\n        TotalPurchaseValue,\n    ],\n    Field(..., discriminator="widget_type"),\n]\n\n\nclass Repository(BaseModel):\n    dimensions: List[Dimension] = []\n    activities: List[Activity] = []\n    metrics: List[Metric] = []\n    selected_tables: List[DbTable] = []\n    dim_act_mappings: List[DimensionActivityMapping] = []\n    widgets: List[WidgetType] = []\n\n    def default_for_purchase_orders():\n        product = Dimension(name="Product", predefined=True)\n        purchase_order = Activity(name="Purchase Order", predefined=True)\n        mapping = DimensionActivityMapping(act_id=purchase_order.uid, dim_id=product.uid)\n        metric = Metric(name="Revenue", act_id=purchase_order.uid)\n\n        return Repository(\n            dimensions=[product],\n            activities=[purchase_order],\n            dim_act_mappings=[mapping],\n            metrics=[metric],\n            widgets=[\n                # KPIs\n                TotalPurchaseValue(metric=metric.copy(), grain="year"),\n                TotalPurchaseValue(metric=metric.copy(), grain="month"),\n                TotalPurchaseValue(metric=metric.copy(), grain="week"),\n                TotalPurchaseValue(metric=metric.copy(), grain="day"),\n                # Charts\n                PurchasingTrendsLineChart(metric=metric.copy()),\n                ProductCategoriesComparisonBarGraph(metric=metric.copy()),\n                ProductCategoryDistributionPieChart(metric=metric.copy()),\n                PurchaseAnomaliesHeatmap(metric=metric.copy()),\n            ],\n        )\n\n    def get_entity(self, entity_type: EntityType, uid: Id) -> Optional[Entity]:\n        collection_name = ENTITY_TYPE_TO_COLLECTION.get(entity_type)\n        if collection_name is None:\n            return None\n\n        for e in getattr(self, collection_name):\n            if e.uid == uid:\n                return e\n        return None\n\n    def is_valid(self) -> bool:\n        validations = [\n            validate_state(self.dimensions, "dimension", validate_each=False),\n            validate_state(self.activities, "activity", validate_each=False),\n            validate_state(self.metrics, "metric", validate_each=False),\n            validate_state(self.dimensions, "dimension", validate_each=True),\n            validate_state(self.activities, "activity", validate_each=True),\n            validate_state(self.dim_act_mappings, "mapping column", permit_empty=True),\n            validate_state(self.metrics, "metric", validate_each=True),\n            validate_state([widget.metric for widget in self.widgets], "metric", validate_each=True),\n        ]\n        return all(is_valid for is_valid, _ in validations)\n')
    __stickytape_write_module('erp_sales_assessment/data/metric.py', b'import datetime\nfrom functools import partial\nfrom typing import Dict, List, Literal, Optional, Set, Tuple, get_args\n\nimport pandas as pd\nimport snowflake.snowpark.functions as f\nimport streamlit as st\nfrom pydantic import Field\nfrom snowflake import snowpark\n\nfrom erp_sales_assessment.data import db\nfrom erp_sales_assessment.data.helpers import repository\nfrom erp_sales_assessment.data.models import Activity, Dimension, DimensionActivityMapping, Entity, EntityType, Id\nfrom erp_sales_assessment.external.std_graphlib import TopologicalSorter\nfrom erp_sales_assessment.utils import BaseModelCacheKey, groupby\n\nAggMethod = Literal["sum", "count", "avg"]\nAGG_METHODS = get_args(AggMethod)\n\n\nclass MetricColumns(BaseModelCacheKey):\n    dimensions: List[str] = []\n    metric_column: str = ""\n    timestamp: str = ""\n    agg_method: Literal["sum", "count", "avg"] = "sum"\n\n    category: str = ""  # Specific to Consumer Behavior Analysis\n\n    labels: Dict[str, str] = Field(default_factory=dict)\n\n    def remove_dimension_columns(self, dim_columns: Set[str]) -> None:\n        for dim_col in set(self.dimensions) & dim_columns:\n            self.dimensions.remove(dim_col)\n            self.labels.pop(dim_col, None)\n\n    def is_valid(self) -> bool:\n        return bool(self.metric_column) and bool(self.timestamp) and bool(self.category) and all(self.dimensions)\n\n    def unquoted(self) -> "MetricColumns":\n        """Return the same columns unquoted eg. `"My column"` => `My column`.\n\n        This is used for Snowpark results, in which columns are returned unquoted.\n        """\n\n        def unquote(col: str) -> str:\n            return col.strip(\'"\')\n\n        return MetricColumns(\n            dimensions=[unquote(col) for col in self.dimensions],\n            metric_column=unquote(self.metric_column),\n            timestamp=unquote(self.timestamp),\n            agg_method=self.agg_method,\n            category=unquote(self.category),\n            labels={unquote(col): label for col, label in self.labels.items()},\n        )\n\n\nclass Metric(Entity):\n    act_id: Optional[Id] = None\n    columns: Optional[MetricColumns] = None\n\n    @classmethod\n    def get_type(cls) -> EntityType:\n        return "metric"\n\n    def validate_entity(self) -> None:\n        if not bool(self.columns) or not self.columns.is_valid():\n            raise ValueError(f"Metric columns must be set.")\n\n    @staticmethod\n    def create(name: str, act_id: Optional[Id] = None) -> "Metric":\n        met = Metric(name=name, act_id=act_id)\n        repository().metrics.append(met)\n        return met\n\n    def delete(self) -> None:\n        repository().metrics.remove(self)\n\n    @property\n    def activity(self) -> Optional[Activity]:\n        return repository().get_entity("activity", self.act_id)\n\n    def unassign_dimension(self, dimension: Dimension):\n        dim_table = dimension.db_table\n        if not self.columns or not dim_table:\n            return\n\n        dim_columns = set(dim_table.columns.keys())\n        self.columns.remove_dimension_columns(dim_columns)\n\n    def dataframe(self) -> snowpark.DataFrame:\n        """Return a dataframe based on the metric activity joined with its dimensions."""\n        if not self.columns:\n            raise ValueError(f"No columns in metric {self.name}.")\n\n        if self.activity.validate_tables_and_mapping():\n            raise ValueError(f"Missing tables and mappings in activity {self.activity.name}.")\n\n        mappings = sorted_mappings(self.activity.mappings)\n\n        snowpark_tables: Dict[str, snowpark.Table] = {}\n        for mapping in mappings:\n            for table in [mapping.main_table, mapping.joined_table]:\n                if table.uid not in snowpark_tables:\n                    snowpark_tables[table.uid] = db.snowflake_session().table(table.full_name)\n\n        act_table = snowpark_tables[self.activity.db_table.uid]\n\n        result_dataframe = act_table\n\n        for mapping in sorted_mappings(self.activity.mappings):\n            main_table = snowpark_tables[mapping.main_table_id]\n            joined_table = snowpark_tables[mapping.joined_table_id]\n\n            result_dataframe = result_dataframe.join(\n                joined_table,\n                how="left",\n                on=main_table.col(mapping.main_column) == joined_table.col(mapping.joined_column),\n            )\n\n        def col_ref(column_name: str) -> snowpark.Column:\n            """Return a reference for the column in one of the given tables eg. my_table.my_column"""\n            for table in snowpark_tables.values():\n                if column_name in table.columns:\n                    return table.col(column_name)\n\n        cols = self.columns\n\n        return result_dataframe.select(\n            [\n                *(col_ref(col).alias(col) for col in cols.dimensions),\n                col_ref(cols.category).alias(cols.category),\n                f.to_date(col_ref(cols.timestamp)).alias(cols.timestamp),\n                col_ref(cols.metric_column).alias(cols.metric_column),\n            ]\n        )\n\n    @st.cache_data\n    def dimension_values(self, dimension: str, limit: int = 1000) -> List[str]:\n        rows = (\n            self.dataframe()\n            .group_by(dimension)\n            .agg(f.sum(self.columns.metric_column).alias("total"))\n            .sort(f.col("total").desc_nulls_last())\n            .limit(limit)\n            .collect()\n        )\n        return [row[dimension] for row in rows if row[dimension] is not None]\n\n    @st.cache_data\n    def preview_dataframe(self, limit: int = 10) -> pd.DataFrame:\n        data = self.dataframe().limit(limit).to_pandas()\n\n        columns = [self.columns.metric_column, self.columns.category] + self.columns.dimensions\n        columns = [col.strip(\'"\') for col in columns]\n        index_col = self.columns.timestamp.strip(\'"\')\n\n        renames = {col: f"{col} ({label})" for col, label in self.columns.labels.items()}\n        return data.set_index(index_col)[columns].rename(columns=renames).rename_axis(renames.get(index_col, index_col))\n\n    @st.cache_data\n    def date_range(self) -> Tuple[datetime.date, datetime.date]:\n        """Return the (min_date, max_date) of the metric."""\n        timestamp_col = f.col(self.columns.timestamp)\n        row = (\n            self.dataframe()\n            .select(f.min(timestamp_col).alias("min_date"), f.max(timestamp_col).alias("max_date"))\n            .first()\n        )\n        return row.MIN_DATE, row.MAX_DATE\n\n\ndef sorted_mappings(mappings: List[DimensionActivityMapping]) -> List[DimensionActivityMapping]:\n    """Return the mappings sorted by the order in which tables must be joined in SQL."""\n    # A graph is a dict of { node => preceding nodes }, so here we have { table to join => tables to load beforehand }\n    tables_graph = {\n        joined_table_id: [mapping.main_table_id for mapping in mappings]\n        for joined_table_id, mappings in groupby(mappings, key=lambda mapping: mapping.joined_table_id).items()\n    }\n    sorted_table_ids = list(TopologicalSorter(tables_graph).static_order())\n\n    def sort_by_main_table(mapping: DimensionActivityMapping) -> int:\n        return sorted_table_ids.index(mapping.main_table_id)\n\n    return sorted(mappings, key=sort_by_main_table)\n')
    __stickytape_write_module('erp_sales_assessment/data/models.py', b'from abc import ABC, abstractmethod\nfrom typing import TYPE_CHECKING, Dict, List, Literal, NewType, Optional, Union\nfrom uuid import uuid4\n\nfrom pydantic import Field\n\nfrom erp_sales_assessment.data import db\nfrom erp_sales_assessment.data.helpers import repository\nfrom erp_sales_assessment.utils import BaseModelCacheKey\n\nif TYPE_CHECKING:\n    from erp_sales_assessment.data.metric import Metric\n\n\nId = NewType("Id", str)\n\nEntityType = Literal["dimension", "activity", "metric"]\n\n\ndef get_uuid4() -> Id:\n    return str(uuid4())\n\n\nclass DomaBaseModel(BaseModelCacheKey, ABC):\n    @abstractmethod\n    def validate_entity(self) -> None:\n        """Raise ValueError if the current entity is invalid."""\n\n    @abstractmethod\n    def validation_item_name(self) -> str:\n        """Name of the item to be validated"""\n\n\nclass Entity(DomaBaseModel, ABC):\n    uid: Id = Field(default_factory=get_uuid4)\n    name: str\n\n    @classmethod\n    @abstractmethod\n    def get_type(cls) -> EntityType:\n        ...\n\n    @abstractmethod\n    def delete(self) -> None:\n        ...\n\n    def validate_entity(self) -> None:\n        if not bool(self.name):\n            raise ValueError(f"An entity name must be set.")\n\n    def validation_item_name(self) -> str:\n        return self.name\n\n\nclass DimensionActivityMapping(DomaBaseModel):\n    dim_id: Id\n    act_id: Id\n\n    main_table_id: Optional[Id]\n    joined_table_id: Optional[Id]\n\n    main_column: Optional[str]\n    joined_column: Optional[str]\n\n    @property\n    def dimension(self) -> Optional["Dimension"]:\n        return repository().get_entity("dimension", self.dim_id)\n\n    @property\n    def activity(self) -> Optional["Activity"]:\n        return repository().get_entity("activity", self.act_id)\n\n    @property\n    def main_table(self) -> Optional["DbTable"]:\n        return self._find_table(self.main_table_id)\n\n    @property\n    def joined_table(self) -> Optional["DbTable"]:\n        return self._find_table(self.joined_table_id)\n\n    @staticmethod\n    def _find_table(uid: Id) -> Optional["DbTable"]:\n        for table in repository().selected_tables:\n            if uid == table.uid:\n                return table\n\n    def validate_entity(self) -> None:\n        if not bool(self.main_table_id) or not bool(self.main_column):\n            raise ValueError(f"A main table must be set.")\n\n        if not bool(self.joined_table_id) or not bool(self.joined_column):\n            raise ValueError(f"A joined table must be set.")\n\n    def validation_item_name(self) -> str:\n        if self.dimension and self.activity:\n            return f"{self.dimension.name} <=> {self.activity.name}"\n        return "none"\n\n\nclass Dimension(Entity):\n    predefined: bool = False\n\n    @classmethod\n    def get_type(cls) -> EntityType:\n        return "dimension"\n\n    @staticmethod\n    def create(name: str) -> "Dimension":\n        dim = Dimension(name=name)\n        repository().dimensions.append(dim)\n        return dim\n\n    def delete(self) -> None:\n        repo = repository()\n        for activity in self.activities:\n            activity.unassign_dimension(self)\n\n        if table := self.db_table:\n            table.delete()\n\n        repo.dimensions.remove(self)\n\n    @property\n    def activities(self) -> List["Activity"]:\n        act_ids = [mapping.act_id for mapping in repository().dim_act_mappings if mapping.dim_id == self.uid]\n        return [act for act in repository().activities if act.uid in act_ids]\n\n    @property\n    def db_table(self) -> Optional["DbTable"]:\n        for table in repository().selected_tables:\n            if table.entity_uid == self.uid and table.entity_type == "dimension":\n                return table\n\n    def validate_entity(self) -> None:\n        super().validate_entity()\n\n        if self.db_table is None:\n            raise ValueError(f"Missing table for dimension {self.name}")\n\n\nclass Activity(Entity):\n    predefined: bool = False\n\n    @classmethod\n    def get_type(cls) -> EntityType:\n        return "activity"\n\n    @staticmethod\n    def create(name: str) -> "Activity":\n        act = Activity(name=name)\n        repository().activities.append(act)\n        return act\n\n    def delete(self) -> None:\n        for dimension in self.dimensions:\n            self.unassign_dimension(dimension)\n\n        if table := self.db_table:\n            table.delete()\n\n        for metric in self.metrics:\n            metric.delete()\n\n        repository().activities.remove(self)\n\n    @property\n    def dimensions(self) -> List[Dimension]:\n        dim_ids = [mapping.dim_id for mapping in repository().dim_act_mappings if mapping.act_id == self.uid]\n        return [dim for dim in repository().dimensions if dim.uid in dim_ids]\n\n    def assign_dimension(self, dim_id: Id) -> None:\n        mapping = DimensionActivityMapping(act_id=self.uid, dim_id=dim_id)\n        repository().dim_act_mappings.append(mapping)\n\n    def unassign_dimension(self, dim: Dimension) -> None:\n        repository().dim_act_mappings = [\n            mapping\n            for mapping in repository().dim_act_mappings\n            if mapping.act_id != self.uid or mapping.dim_id != dim.uid\n        ]\n        for metric in self.metrics:\n            metric.unassign_dimension(dim)\n\n    @property\n    def metrics(self) -> List["Metric"]:\n        return [metric for metric in repository().metrics if metric.act_id == self.uid]\n\n    @property\n    def mappings(self) -> List[DimensionActivityMapping]:\n        return [mapping for mapping in repository().dim_act_mappings if mapping.act_id == self.uid]\n\n    @property\n    def db_table(self) -> Optional["DbTable"]:\n        for table in repository().selected_tables:\n            if table.entity_uid == self.uid and table.entity_type == "activity":\n                return table\n\n    def validate_entity(self) -> None:\n        super().validate_entity()\n\n        if self.db_table is None:\n            raise ValueError(f"Missing table for activity {self.name}")\n\n    def validate_tables_and_mapping(self) -> List[str]:\n        """Return a list of error messages for each missing tables or mappings in the activity."""\n        errors = []\n        if self.db_table is None:\n            errors.append(f"Missing table for activity {self.name}")\n\n        for dim in self.dimensions:\n            if dim.db_table is None:\n                errors.append(f"Missing table for dimension {dim.name}")\n\n        for mapping in self.mappings:\n            try:\n                mapping.validate_entity()\n            except ValueError:\n                errors.append(f"Missing mapping: {mapping.validation_item_name()}")\n\n        return errors\n\n\nclass DbTable(DomaBaseModel):\n    uid: Id = Field(default_factory=get_uuid4)\n\n    table_name: str\n    table_schema: str\n    table_database: str\n\n    entity_uid: Id\n    entity_type: EntityType\n\n    columns: Dict[str, db.ExtendedDataType] = {}\n\n    @staticmethod\n    def create(\n        table_name: str,\n        table_schema: str,\n        table_database: str,\n        columns: Dict[str, str],\n        entity: Entity,\n    ) -> "DbTable":\n        entity_uid = entity.uid\n        entity_type = entity.get_type()\n        tbl = DbTable(\n            table_name=table_name,\n            table_schema=table_schema,\n            table_database=table_database,\n            columns=columns,\n            entity_uid=entity_uid,\n            entity_type=entity_type,\n        )\n        repository().selected_tables.append(tbl)\n        return tbl\n\n    @property\n    def full_name(self) -> str:\n        return f"{self.table_database}.{self.table_schema}.{self.table_name}"\n\n    @property\n    def entity(self) -> Union[Activity, Dimension]:\n        return repository().get_entity(self.entity_type, self.entity_uid)\n\n    def delete(self) -> None:\n        repository().selected_tables.remove(self)\n\n        # Reset mapping with the table to delete\n        for mapping in repository().dim_act_mappings:\n            if self.uid in [mapping.main_table_id, mapping.joined_table_id]:\n                mapping.main_table_id = None\n                mapping.joined_table_id = None\n                mapping.main_column = None\n                mapping.joined_column = None\n\n        # Remove table columns from metrics\n        entity = self.entity\n\n        if isinstance(entity, Activity):\n            for metric in entity.metrics:\n                metric.columns = None\n\n        if isinstance(entity, Dimension):\n            for activity in entity.activities:\n                for metric in activity.metrics:\n                    metric.unassign_dimension(entity)\n\n    def validate_entity(self) -> None:\n        if db.snowflake_session().table(self.full_name).first() is None:\n            raise ValueError(f"Table \'{self.full_name}\' is empty or does not exist.")\n\n    def validation_item_name(self) -> str:\n        return self.full_name\n\n    def columns_of_type(self, data_types: List[db.DataType]) -> List[str]:\n        """Return the columns having one of the given types."""\n        return [col for col, data_type in self.columns.items() if data_type in data_types]\n')
    __stickytape_write_module('erp_sales_assessment/external/std_graphlib.py', b'# Backport of the `graphlib` library that is available in python 3.9:\n# => https://docs.python.org/3/library/graphlib.html\n#\n# TODO: replace by `graphlib` from std once in python 3.9\n\n\n__all__ = ["TopologicalSorter", "CycleError"]\n\n_NODE_OUT = -1\n_NODE_DONE = -2\n\n\nclass _NodeInfo:\n    __slots__ = "node", "npredecessors", "successors"\n\n    def __init__(self, node):\n        # The node this class is augmenting.\n        self.node = node\n\n        # Number of predecessors, generally >= 0. When this value falls to 0,\n        # and is returned by get_ready(), this is set to _NODE_OUT and when the\n        # node is marked done by a call to done(), set to _NODE_DONE.\n        self.npredecessors = 0\n\n        # List of successor nodes. The list can contain duplicated elements as\n        # long as they\'re all reflected in the successor\'s npredecessors attribute.\n        self.successors = []\n\n\nclass CycleError(ValueError):\n    """Subclass of ValueError raised by TopologicalSorter.prepare if cycles\n    exist in the working graph.\n\n    If multiple cycles exist, only one undefined choice among them will be reported\n    and included in the exception. The detected cycle can be accessed via the second\n    element in the *args* attribute of the exception instance and consists in a list\n    of nodes, such that each node is, in the graph, an immediate predecessor of the\n    next node in the list. In the reported list, the first and the last node will be\n    the same, to make it clear that it is cyclic.\n    """\n\n    pass\n\n\nclass TopologicalSorter:\n    """Provides functionality to topologically sort a graph of hashable nodes"""\n\n    def __init__(self, graph=None):\n        self._node2info = {}\n        self._ready_nodes = None\n        self._npassedout = 0\n        self._nfinished = 0\n\n        if graph is not None:\n            for node, predecessors in graph.items():\n                self.add(node, *predecessors)\n\n    def _get_nodeinfo(self, node):\n        if (result := self._node2info.get(node)) is None:\n            self._node2info[node] = result = _NodeInfo(node)\n        return result\n\n    def add(self, node, *predecessors):\n        """Add a new node and its predecessors to the graph.\n\n        Both the *node* and all elements in *predecessors* must be hashable.\n\n        If called multiple times with the same node argument, the set of dependencies\n        will be the union of all dependencies passed in.\n\n        It is possible to add a node with no dependencies (*predecessors* is not provided)\n        as well as provide a dependency twice. If a node that has not been provided before\n        is included among *predecessors* it will be automatically added to the graph with\n        no predecessors of its own.\n\n        Raises ValueError if called after "prepare".\n        """\n        if self._ready_nodes is not None:\n            raise ValueError("Nodes cannot be added after a call to prepare()")\n\n        # Create the node -> predecessor edges\n        nodeinfo = self._get_nodeinfo(node)\n        nodeinfo.npredecessors += len(predecessors)\n\n        # Create the predecessor -> node edges\n        for pred in predecessors:\n            pred_info = self._get_nodeinfo(pred)\n            pred_info.successors.append(node)\n\n    def prepare(self):\n        """Mark the graph as finished and check for cycles in the graph.\n\n        If any cycle is detected, "CycleError" will be raised, but "get_ready" can\n        still be used to obtain as many nodes as possible until cycles block more\n        progress. After a call to this function, the graph cannot be modified and\n        therefore no more nodes can be added using "add".\n        """\n        if self._ready_nodes is not None:\n            raise ValueError("cannot prepare() more than once")\n\n        self._ready_nodes = [i.node for i in self._node2info.values() if i.npredecessors == 0]\n        # ready_nodes is set before we look for cycles on purpose:\n        # if the user wants to catch the CycleError, that\'s fine,\n        # they can continue using the instance to grab as many\n        # nodes as possible before cycles block more progress\n        cycle = self._find_cycle()\n        if cycle:\n            raise CycleError(f"nodes are in a cycle", cycle)\n\n    def get_ready(self):\n        """Return a tuple of all the nodes that are ready.\n\n        Initially it returns all nodes with no predecessors; once those are marked\n        as processed by calling "done", further calls will return all new nodes that\n        have all their predecessors already processed. Once no more progress can be made,\n        empty tuples are returned.\n\n        Raises ValueError if called without calling "prepare" previously.\n        """\n        if self._ready_nodes is None:\n            raise ValueError("prepare() must be called first")\n\n        # Get the nodes that are ready and mark them\n        result = tuple(self._ready_nodes)\n        n2i = self._node2info\n        for node in result:\n            n2i[node].npredecessors = _NODE_OUT\n\n        # Clean the list of nodes that are ready and update\n        # the counter of nodes that we have returned.\n        self._ready_nodes.clear()\n        self._npassedout += len(result)\n\n        return result\n\n    def is_active(self):\n        """Return ``True`` if more progress can be made and ``False`` otherwise.\n\n        Progress can be made if cycles do not block the resolution and either there\n        are still nodes ready that haven\'t yet been returned by "get_ready" or the\n        number of nodes marked "done" is less than the number that have been returned\n        by "get_ready".\n\n        Raises ValueError if called without calling "prepare" previously.\n        """\n        if self._ready_nodes is None:\n            raise ValueError("prepare() must be called first")\n        return self._nfinished < self._npassedout or bool(self._ready_nodes)\n\n    def __bool__(self):\n        return self.is_active()\n\n    def done(self, *nodes):\n        """Marks a set of nodes returned by "get_ready" as processed.\n\n        This method unblocks any successor of each node in *nodes* for being returned\n        in the future by a call to "get_ready".\n\n        Raises :exec:`ValueError` if any node in *nodes* has already been marked as\n        processed by a previous call to this method, if a node was not added to the\n        graph by using "add" or if called without calling "prepare" previously or if\n        node has not yet been returned by "get_ready".\n        """\n\n        if self._ready_nodes is None:\n            raise ValueError("prepare() must be called first")\n\n        n2i = self._node2info\n\n        for node in nodes:\n            # Check if we know about this node (it was added previously using add()\n            if (nodeinfo := n2i.get(node)) is None:\n                raise ValueError(f"node {node!r} was not added using add()")\n\n            # If the node has not being returned (marked as ready) previously, inform the user.\n            stat = nodeinfo.npredecessors\n            if stat != _NODE_OUT:\n                if stat >= 0:\n                    raise ValueError(f"node {node!r} was not passed out (still not ready)")\n                elif stat == _NODE_DONE:\n                    raise ValueError(f"node {node!r} was already marked done")\n                else:\n                    assert False, f"node {node!r}: unknown status {stat}"\n\n            # Mark the node as processed\n            nodeinfo.npredecessors = _NODE_DONE\n\n            # Go to all the successors and reduce the number of predecessors, collecting all the ones\n            # that are ready to be returned in the next get_ready() call.\n            for successor in nodeinfo.successors:\n                successor_info = n2i[successor]\n                successor_info.npredecessors -= 1\n                if successor_info.npredecessors == 0:\n                    self._ready_nodes.append(successor)\n            self._nfinished += 1\n\n    def _find_cycle(self):\n        n2i = self._node2info\n        stack = []\n        itstack = []\n        seen = set()\n        node2stacki = {}\n\n        for node in n2i:\n            if node in seen:\n                continue\n\n            while True:\n                if node in seen:\n                    # If we have seen already the node and is in the\n                    # current stack we have found a cycle.\n                    if node in node2stacki:\n                        return stack[node2stacki[node] :] + [node]\n                    # else go on to get next successor\n                else:\n                    seen.add(node)\n                    itstack.append(iter(n2i[node].successors).__next__)\n                    node2stacki[node] = len(stack)\n                    stack.append(node)\n\n                # Backtrack to the topmost stack entry with\n                # at least another successor.\n                while stack:\n                    try:\n                        node = itstack[-1]()\n                        break\n                    except StopIteration:\n                        del node2stacki[stack.pop()]\n                        itstack.pop()\n                else:\n                    break\n        return None\n\n    def static_order(self):\n        """Returns an iterable of nodes in a topological order.\n\n        The particular order that is returned may depend on the specific\n        order in which the items were inserted in the graph.\n\n        Using this method does not require to call "prepare" or "done". If any\n        cycle is detected, :exc:`CycleError` will be raised.\n        """\n        self.prepare()\n        while self.is_active():\n            node_group = self.get_ready()\n            yield from node_group\n            self.done(*node_group)\n')
    __stickytape_write_module('erp_sales_assessment/widgets/__init__.py', b'\n')
    __stickytape_write_module('erp_sales_assessment/widgets/kpis.py', b'import datetime\nfrom datetime import date\nfrom typing import Callable, Dict, Literal, Optional, Tuple\n\nimport plotly.graph_objs as go\nimport snowflake.snowpark.functions as f\nimport streamlit as st\nfrom snowflake import snowpark\nfrom typing_extensions import Literal\n\nfrom erp_sales_assessment.components.plotly_styles import COLORS, KPI_ANNOTATION_STYLE, KPI_LAYOUT\nfrom erp_sales_assessment.data import db\nfrom erp_sales_assessment.data.metric import Metric\nfrom erp_sales_assessment.time_series.period import Period\nfrom erp_sales_assessment.time_series.time_series import GRAINS, Grain, TimeSeries\nfrom erp_sales_assessment.widgets.example_charts import example_kpi_metric\nfrom erp_sales_assessment.widgets.parameters import multiselect_dimension_filters, year_range_slider\nfrom erp_sales_assessment.widgets.widget import Widget, WidgetSize\n\n\nclass TotalPurchaseValue(Widget):\n    widget_kind: Literal["chart", "kpi", "text"] = "kpi"\n    widget_type: Literal["TotalPurchaseValue"] = "TotalPurchaseValue"\n    size: WidgetSize = "small"\n\n    # Parameters\n    metric: Metric\n    grain: Grain = "month"\n\n    # Filters\n    period: Optional[Period]\n    filters: db.Filters = db.Filters()\n\n    @staticmethod\n    def description() -> str:\n        return "Total Purchase Value"\n\n    @staticmethod\n    def example_figure() -> go.Figure:\n        return example_kpi_metric()\n\n    def default_title(self) -> str:\n        def period_over_period(grain: Grain) -> str:\n            letter = grain[0].upper()\n            return f"{letter}o{letter}"\n\n        return f"Revenue {period_over_period(self.grain)}"\n\n    def edit_parameters(self) -> None:\n        title = st.text_input("Title", value=self.title, placeholder=self.default_title())\n        grain = st.selectbox("Periodicity", GRAINS, index=GRAINS.index(self.grain))\n\n        st.markdown("#### Filters")\n        start_year, end_year = year_range_slider(self.metric, self.period)\n        filters = multiselect_dimension_filters(self.metric, self.filters)\n\n        def apply_changes() -> None:\n            self.title = title\n            self.grain = grain\n            self.period = Period(start=date(start_year, 1, 1), end=date(end_year, 12, 31))\n            self.filters = filters\n\n        st.button("Apply", use_container_width=True, on_click=apply_changes)\n\n    def figure(self) -> go.Figure:\n        min_date, max_date = self.metric.date_range()\n        period = self.period or Period(start=min_date, end=max_date)\n\n        current, previous = fetch_total_values(self.metric, self.grain, self.filters, period)\n\n        title = self.displayed_title()\n        diff_percent = (current["VALUE"] / previous["VALUE"] - 1) * 100\n\n        delta_arrow = "\xe2\x96\xb2" if diff_percent > 0 else "\xe2\x96\xbc"\n        delta_color = COLORS["indicator-increasing"] if diff_percent > 0 else COLORS["indicator-decreasing"]\n\n        return (\n            go.Figure(layout=KPI_LAYOUT)\n            .add_annotation(\n                text=title,\n                yanchor="top",\n                xshift=12,\n                yshift=-4,\n                font=dict(size=16, color=COLORS["indicator-text"]),\n                **KPI_ANNOTATION_STYLE,\n            )\n            .add_annotation(\n                text=f\'${current["VALUE"]:,.0f}\',\n                xshift=12,\n                yshift=-24,\n                font=dict(size=36, color=COLORS["indicator-text"]),\n                **KPI_ANNOTATION_STYLE,\n            )\n            .add_annotation(\n                text=f"{delta_arrow} {diff_percent:.0f}% ",\n                xshift=12,\n                yshift=-72,\n                font=dict(size=14, color=delta_color),\n                **KPI_ANNOTATION_STYLE,\n            )\n            .update_xaxes(showticklabels=False, showgrid=False, zeroline=False)\n            .update_yaxes(showticklabels=False, showgrid=False, zeroline=False)\n        )\n\n\n@st.cache_data\ndef fetch_total_values(\n    metric: Metric,\n    grain: Grain,\n    filters: db.Filters,\n    period: Period,\n) -> Tuple[snowpark.Row, snowpark.Row]:\n    cols = metric.columns\n    period_column = f.date_trunc(grain, cols.timestamp).alias("period")\n\n    is_grain_to_date = GRAIN_TO_DATE_FUNCTION[grain]\n\n    current, previous = (\n        metric.dataframe()\n        .filter(f.col(cols.timestamp).between(period.start, period.end))\n        .filter(is_grain_to_date(cols.timestamp, period.end))\n        .filter(filters.to_condition())\n        .select([cols.metric_column, period_column])\n        .group_by(period_column.get_name())\n        .agg(f.sum(cols.metric_column).alias("value"))\n        .sort(period_column.get_name(), ascending=False)\n        .first(2)\n    )\n    return current, previous\n\n\ndef isodayofweek(col: str) -> snowpark.Column:\n    return f.iff(f.dayofweek(col) == 0, 7, f.dayofweek(col))\n\n\nGRAIN_TO_DATE_FUNCTION: Dict[Grain, Callable[[str, date], snowpark.Column]] = {\n    "day": lambda col, end_date: ~f.is_null(col),\n    "week": lambda col, end_date: isodayofweek(col) <= end_date.isoweekday(),\n    "month": lambda col, end_date: f.dayofmonth(col) <= end_date.day,\n    "quarter": lambda col, end_date: (f.month(col) <= end_date.month) & (f.dayofmonth(col) <= end_date.day),\n    "year": lambda col, end_date: (f.month(col) <= end_date.month) & (f.dayofmonth(col) <= end_date.day),\n}\n\n\ndef period_text(grain: Grain, period: datetime.date):\n    GRAIN_FORMATS: Dict[Grain, str] = {\n        "day": "%Y-%m-%d",\n        "week": "Week %W",\n        "month": "%b %Y",\n        "quarter": "Q%q",\n        "year": "%Y",\n    }\n    return period.strftime(GRAIN_FORMATS[grain])\n')
    __stickytape_write_module('erp_sales_assessment/components/plotly_styles.py', b'import plotly.express as px\nimport plotly.graph_objects as go\n\nGRADIENT_COLOR = [\n    "#1a72c0",\n    "#168bf4",\n    "#5cb2ff",\n    "#7cbbf5",\n    "#8a96ff",\n    "#4c5ded",\n    "#3e49a7",\n    "#352a92",\n    "#110670",\n]\n\n\nCOLORS = {\n    "text": "#3e49a7",\n    "ticks": "#aaa8b8",\n    "background": "#f0f2f6",\n    "plot-background": "#ffffff",\n    "selected-background": "#fff0f0",\n    "line-color": "#4c5ded",\n    "line-background": "#9cdcf7",\n    "highlight": "#BD2593",\n    "indicator-text": "#ffffff",\n    "indicator-background": "#2d3579",\n    "indicator-increasing": "#2fed6a",\n    "indicator-decreasing": "#fd3131",\n    "bar-chart-a": "#7cbbf5",\n    "bar-chart-b": "#110670",\n}\n\n# Doc: https://plotly.com/python/configuration-options\nPLOTLY_CONFIG = {\n    "displayModeBar": False,\n}\n\n# Doc: https://plotly.com/python-api-reference/generated/plotly.graph_objects.layout.html#plotly.graph_objects.layout.Shape\nPLOT_BORDER = go.layout.Shape(\n    type="rect",\n    xref="paper",\n    yref="paper",\n    x0=0,\n    y0=0,\n    x1=1,\n    y1=1,\n    line={"width": 1, "color": COLORS["text"]},\n)\n\n# Doc: https://plotly.com/python-api-reference/generated/plotly.graph_objects.Layout.html\nPLOT_LAYOUT = go.Layout(\n    height=420,\n    margin={"t": 46, "b": 32, "l": 32, "r": 32, "pad": 0, "autoexpand": True},\n    font={\n        "family": "Source Sans Pro",\n        "color": COLORS["text"],\n    },\n    showlegend=True,\n    paper_bgcolor=COLORS["background"],\n    plot_bgcolor=COLORS["plot-background"],\n    title={\n        "pad": dict(t=16, b=0, l=16, r=0),\n        "font": {"color": COLORS["text"]},\n        "x": 0,\n        "y": 1,\n        "xanchor": "left",\n        "yanchor": "top",\n    },\n    xaxis={\n        "title": {"font": {"color": COLORS["ticks"], "size": 14}, "standoff": 16},\n        "showgrid": True,\n        "color": COLORS["ticks"],\n        "ticks": "outside",\n        "tickcolor": COLORS["ticks"],\n        "tickfont": {"size": 12, "color": COLORS["ticks"], "family": "Arial"},\n        "dtick": "M12",\n    },\n    yaxis={\n        "title": {"font": {"color": COLORS["ticks"]}, "standoff": 16},\n        "showgrid": True,\n        "color": COLORS["ticks"],\n        "ticks": "outside",\n        "tickcolor": COLORS["ticks"],\n        "tickfont": {"size": 14, "color": COLORS["ticks"]},\n    },\n    legend={\n        "font": {"color": COLORS["text"]},\n        "x": 1,\n        "y": 1.125,\n        "xanchor": "right",\n        "yanchor": "top",\n        "orientation": "h",\n    },\n)\n\n\n# Doc: https://plotly.com/python-api-reference/generated/plotly.graph_objects.Box.html\nBOXPLOT_STYLE = go.Box(\n    fillcolor=COLORS["line-background"],\n    line={"color": COLORS["line-color"]},\n)\n\n\nSCATTER_STYLE = go.Scatter(\n    hovertemplate="<b>%{y}</b><br>%{x}<extra></extra>",\n    # margin={"line": {"width": 0}, "size": 6},\n    line={"width": 1, "color": COLORS["line-color"]},\n)\n\nBAR_STYLE = go.Bar(hovertemplate="<b>%{y}</b><br>%{x}<extra></extra>")\n\n# Doc: https://plotly.com/python/reference/pie\nPIE_STYLE = go.Pie(\n    marker_colors=list(reversed(GRADIENT_COLOR)),\n    hoverinfo="label",\n)\nPIE_LAYOUT = go.Layout(\n    # margin={"t": 46, "b": 32, "l": 32, "r": 32},\n    legend={\n        "x": 0.5,\n        "xanchor": "center",\n        "y": -0.1,\n        "yanchor": "middle",\n        "orientation": "h",\n    },\n)\n\nHEATMAP_STYLE = go.Heatmap(\n    # colorscale=["#cbe2fa", "#5cb2ff", "#110670"],\n    autocolorscale=True,\n    hovertemplate="<b>%{z}</b><br>%{y} - %{x}<extra></extra>",\n    texttemplate="%{z}",\n)\n\n\n# Style of an example chart\nEXAMPLE_LAYOUT = go.Layout(\n    height=150,\n    margin={"t": 15, "b": 15, "l": 15, "r": 15},\n    showlegend=False,\n    xaxis={"title": None},\n    yaxis={"title": None},\n)\nEXAMPLE_COLORS = px.colors.sequential.Blues_r\n\n\n# Styles of a selected chart\nSELECTED_STYLE = go.Layout(\n    # title={"font": {"color": COLORS["highlight"]}},\n    # paper_bgcolor=COLORS["selected-background"],\n    shapes=[PLOT_BORDER.update(go.layout.Shape(line={"width": 3, "color": COLORS["highlight"]}))],\n)\n\nINDICATOR_LAYOUT = go.Layout(\n    height=100,\n    font_family="Source Sans Pro",\n    font={"color": COLORS["indicator-text"]},\n    paper_bgcolor=COLORS["indicator-background"],\n    margin={"t": 150, "l": 12},\n)\nINDICATOR_STYLE = go.Indicator(\n    mode="number+delta",\n    title={\n        "align": "left",\n        "font": {"size": 16},\n    },\n    align="left",\n    number={\n        "font": {"size": 36},\n        "valueformat": "$,.0f",\n    },\n    delta={\n        "font": {"size": 14},\n        "valueformat": "$,.0f",\n        "position": "bottom",\n        "increasing": {\n            "color": COLORS["indicator-increasing"],\n            "symbol": "\xe2\x86\x91 ",\n        },\n        "decreasing": {\n            "color": COLORS["indicator-decreasing"],\n            "symbol": "\xe2\x86\x93 ",\n        },\n    },\n)\nINDICATOR_SELECTED_STYLE = {\n    "paper_bgcolor": COLORS["highlight"],\n}\n\n\nKPI_LAYOUT = go.Layout(\n    height=96,\n    font_family="Source Sans Pro",\n    plot_bgcolor=COLORS["indicator-background"],\n    margin=dict(l=0, r=0, t=0, b=0),\n)\nKPI_ANNOTATION_STYLE = dict(borderpad=0, showarrow=False, xref="paper", yref="paper", x=0, y=1)\n')
    __stickytape_write_module('erp_sales_assessment/time_series/__init__.py', b'')
    __stickytape_write_module('erp_sales_assessment/time_series/period.py', b'import datetime\nfrom typing import Literal\n\nimport pandas as pd\n\nfrom erp_sales_assessment.utils import BaseModelCacheKey\n\nPeriodFrequency = Literal["W", "M", "Y", "year-weeks"]\n\n\nclass Period(BaseModelCacheKey):\n    """Period of time between two dates."""\n\n    start: datetime.date\n    end: datetime.date\n\n    @staticmethod\n    def from_pandas(period: pd.Period) -> "Period":\n        return Period(start=period.start_time, end=period.end_time)\n\n    def from_year(year: int) -> "Period":\n        return Period(start=datetime.date(year, 1, 1), end=datetime.date(year, 12, 31))\n\n    @staticmethod\n    def from_date(date: datetime.date, freq: PeriodFrequency) -> "Period":\n        if freq == "year-weeks":\n            return period_of_year_weeks(date.year)\n\n        return Period.from_pandas(pd.Timestamp(date).to_period(freq))\n\n\ndef period_of_year_weeks(year: int) -> Period:\n    """Return the start and end date of the first and last weeks of the given year."""\n    year_period = pd.Period(year, freq="Y")\n\n    first_week = year_period.start_time.to_period("W")\n    last_week = year_period.end_time.to_period("W")\n\n    if first_week.week != 1:  # the last week of previous year\n        first_week += 1\n\n    if last_week.week == 1:  # the first week of next year\n        last_week -= 1\n\n    return Period(start=first_week.start_time.floor("D"), end=last_week.end_time.floor("D"))\n')
    __stickytape_write_module('erp_sales_assessment/time_series/time_series.py', b'from typing import Literal, get_args\n\nimport snowflake.snowpark as snowpark\nimport snowflake.snowpark.functions as f\n\nfrom erp_sales_assessment.data.metric import Metric\nfrom erp_sales_assessment.utils import BaseModelCacheKey\n\nGrain = Literal["day", "week", "month", "quarter", "year"]\nGRAINS = get_args(Grain)\n\n\nclass TimeSeries(BaseModelCacheKey):\n    metric: Metric\n    grain: Grain\n\n    def dataframe(self) -> snowpark.DataFrame:\n        cols = self.metric.columns\n        period_column = f.date_trunc(self.grain, cols.timestamp).alias("period")\n\n        return (\n            self.metric.dataframe()\n            .select(cols.dimensions + [cols.metric_column, period_column])\n            .group_by(cols.dimensions + [period_column.get_name()])\n            .agg(getattr(f, cols.agg_method)(cols.metric_column).alias(cols.metric_column))\n        )\n')
    __stickytape_write_module('erp_sales_assessment/widgets/example_charts.py', b'import itertools\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.graph_objs as go\n\nfrom erp_sales_assessment.components.plotly_styles import (\n    EXAMPLE_COLORS,\n    EXAMPLE_LAYOUT,\n    INDICATOR_LAYOUT,\n    INDICATOR_STYLE,\n    PLOT_LAYOUT,\n)\n\n\ndef example_pie_chart() -> go.Figure:\n    df = pd.DataFrame(\n        {\n            "Product Category": list("ABCDE"),\n            "Percentage": np.random.randint(100, size=5),\n        }\n    )\n    return (\n        px.pie(df, names="Product Category", values="Percentage", title=" ", color_discrete_sequence=EXAMPLE_COLORS)\n        .update_layout(PLOT_LAYOUT)\n        .update_layout(EXAMPLE_LAYOUT)\n    )\n\n\ndef example_bar_comparison_chart() -> go.Figure:\n    return (\n        go.Figure(\n            layout=go.Layout(title=""),\n            data=[\n                go.Bar(name="2022", x=list("ABCDE"), y=np.random.randint(75, size=5) + 25),\n                go.Bar(name="2023", x=list("ABCDE"), y=np.random.randint(75, size=5) + 25),\n            ],\n        )\n        .update_layout(PLOT_LAYOUT)\n        .update_layout(EXAMPLE_LAYOUT)\n    )\n\n\ndef example_bar_chart() -> go.Figure:\n    df = pd.DataFrame(\n        {\n            "Product Category": list("ABCDEFGHIJ"),\n            "Purchasing Volume": np.random.randint(100, size=10),\n        }\n    )\n    return (\n        px.bar(df, x="Product Category", y="Purchasing Volume", title=" ", color_discrete_sequence=EXAMPLE_COLORS)\n        .update_layout(PLOT_LAYOUT)\n        .update_layout(EXAMPLE_LAYOUT)\n    )\n\n\ndef example_box_chart() -> go.Figure:\n    df = pd.DataFrame(\n        {\n            "Product Category": itertools.chain(*([category] * 10 for category in list("ABCDEFGHIJ"))),\n            "Purchase Amount": np.random.randint(100, size=100),\n        }\n    )\n    return (\n        px.box(df, x="Product Category", y="Purchase Amount", title=" ", color_discrete_sequence=EXAMPLE_COLORS)\n        .update_layout(PLOT_LAYOUT)\n        .update_layout(EXAMPLE_LAYOUT)\n    )\n\n\ndef example_line_chart() -> go.Figure:\n    df = pd.DataFrame(\n        {\n            "month": pd.date_range(end="2023-01-01", periods=30, freq="MS"),\n            "trend": np.random.randint(100, size=30) + 50,\n        }\n    )\n    return (\n        px.line(df, x="month", y="trend", title=" ", range_y=[0, 200], color_discrete_sequence=EXAMPLE_COLORS)\n        .update_layout(PLOT_LAYOUT)\n        .update_layout(EXAMPLE_LAYOUT)\n    )\n\n\ndef example_area_chart() -> go.Figure:\n    df = pd.DataFrame(\n        {\n            "quarter": pd.date_range(end="2023-01-01", periods=24, freq="Q"),\n            "trend": np.random.randint(100, size=24) + 50,\n        }\n    )\n    return (\n        px.area(df, x="quarter", y="trend", title=" ", color_discrete_sequence=EXAMPLE_COLORS)\n        .update_layout(PLOT_LAYOUT)\n        .update_layout(EXAMPLE_LAYOUT)\n    )\n\n\ndef example_heatmap_chart() -> go.Figure:\n    scores = np.concatenate((np.random.randint(50, size=45), np.random.randint(10, size=5) + 90))\n    np.random.shuffle(scores)\n\n    df = pd.DataFrame(\n        {\n            "X": list("ABCDE") * 10,\n            "Y": itertools.chain(*([category] * 10 for category in list("ABCDE"))),\n            "score": scores,\n        }\n    )\n    return (\n        go.Figure(\n            layout={"title": " "},\n            data=go.Heatmap(\n                x=df["X"],\n                y=df["Y"],\n                z=df["score"],\n                # colorscale=list(reversed(EXPRESS_COLORS)),\n                autocolorscale=True,\n                hovertext=df["X"] + " - " + df["Y"] + ": " + df["score"].astype(str),\n                hoverinfo="text",\n                colorbar=dict(title="Score"),\n            ),\n        )\n        .update_layout(PLOT_LAYOUT)\n        .update_layout(EXAMPLE_LAYOUT)\n    )\n\n\ndef example_kpi_metric() -> go.Figure:\n    return go.Figure(\n        layout=INDICATOR_LAYOUT,\n        data=go.Indicator(\n            value=30000,\n            delta=go.indicator.Delta(reference=30000 * 0.90),\n            title={"text": "Average per Month"},\n        ).update(INDICATOR_STYLE),\n    )\n')
    __stickytape_write_module('erp_sales_assessment/widgets/parameters.py', b'from typing import Optional, Tuple\n\nimport streamlit as st\n\nfrom erp_sales_assessment.data import db\nfrom erp_sales_assessment.data.helpers import repository\nfrom erp_sales_assessment.data.metric import Metric\nfrom erp_sales_assessment.time_series.period import Period\n\n\ndef year_range_slider(metric: Metric, period: Optional[Period] = None) -> Tuple[int, int]:\n    """Slider to select the range of years."""\n    min_date, max_date = metric.date_range()\n    return st.slider(\n        "Time Period",\n        min_value=min_date.year,\n        max_value=max_date.year,\n        value=(period.start.year, period.end.year) if period else (min_date.year, max_date.year),\n    )\n\n\ndef metric_selection(metric: Metric) -> Metric:\n    """Selectbox among all metrics."""\n    return st.selectbox(\n        "Metric",\n        options=repository().metrics,\n        index=repository().metrics.index(metric),\n        format_func=lambda metric: metric.name,\n    )\n\n\ndef multiselect_dimension_filters(metric: Metric, current_filters: db.Filters) -> db.Filters:\n    """List of multiselect filters for each dimension in the metric."""\n    filters = db.Filters()\n\n    cols = metric.columns.unquoted()\n\n    for dimension in cols.dimensions:\n        label = cols.labels.get(dimension, dimension)\n\n        values = st.multiselect(\n            label,\n            metric.dimension_values(dimension),\n            default=current_filters.mapping.get(dimension),\n        )\n        if values:\n            filters.add(dimension, values)\n\n    return filters\n')
    __stickytape_write_module('erp_sales_assessment/widgets/widget.py', b'from abc import abstractmethod\nfrom typing import Literal\n\nimport plotly.graph_objects as go\n\nfrom erp_sales_assessment.utils import BaseModelCacheKey\n\nWidgetSize = Literal["small", "medium", "large"]\n\n\nclass Widget(BaseModelCacheKey):\n    widget_kind: Literal["chart", "kpi", "text"] = "chart"\n    size: WidgetSize = "medium"\n\n    title: str = ""\n\n    @staticmethod\n    @abstractmethod\n    def description() -> str:\n        """Description of the widget in the gallery."""\n\n    @abstractmethod\n    def default_title(self) -> str:\n        """Default title inferred from parameters."""\n\n    def displayed_title(self) -> str:\n        """Either the title defined by the user or the one inferred from parameters."""\n        return (self.title or self.default_title()).strip()\n\n    @staticmethod\n    @abstractmethod\n    def example_figure() -> go.Figure:\n        """Return an example plotly figure showing how the widget look like."""\n\n    @abstractmethod\n    def edit_parameters(self) -> None:\n        """Render a form for editing the widget parameters."""\n\n    @abstractmethod\n    def figure(self) -> go.Figure:\n        """Return the plotly figure with the defined parameters."""\n')
    __stickytape_write_module('erp_sales_assessment/widgets/outlier_detection_widgets.py', b'import calendar\nfrom datetime import date\nfrom typing import Any, Dict, List, Literal, Optional, get_args\n\nimport pandas as pd\nimport plotly.graph_objs as go\nimport snowflake.snowpark.functions as f\nimport streamlit as st\n\nfrom erp_sales_assessment.components.plotly_styles import HEATMAP_STYLE, PLOT_LAYOUT\nfrom erp_sales_assessment.data import db\nfrom erp_sales_assessment.data.metric import Metric\nfrom erp_sales_assessment.time_series.period import Period\nfrom erp_sales_assessment.widgets.example_charts import example_heatmap_chart\nfrom erp_sales_assessment.widgets.parameters import multiselect_dimension_filters, year_range_slider\nfrom erp_sales_assessment.widgets.widget import Widget, WidgetSize\n\n\nclass PurchaseAnomaliesHeatmap(Widget):\n    widget_kind: Literal["chart", "kpi", "text"] = "chart"\n    widget_type: Literal["PurchaseAnomaliesHeatmap"] = "PurchaseAnomaliesHeatmap"\n    size: WidgetSize = "large"\n\n    # Parameters\n    metric: Metric\n    x_col: str = "day_of_week"\n\n    # Filters\n    period: Optional[Period]\n    filters: db.Filters = db.Filters()\n\n    @staticmethod\n    def description() -> str:\n        return "Visualizes purchase anomalies"\n\n    @staticmethod\n    def example_figure() -> go.Figure:\n        return example_heatmap_chart()\n\n    def default_title(self) -> str:\n        return "Purchase Velocity - Product"\n\n    def edit_parameters(self) -> None:\n        title = st.text_input("Title", value=self.title, placeholder=self.default_title())\n\n        # dim_columns = self.metric.columns.dimensions + list(RANGE_PERIODICITIES)\n        # x_col = st.selectbox("X column", options=dim_columns, index=dim_columns.index(self.x_col))\n        # y_col = st.selectbox("Y column", options=dim_columns, index=dim_columns.index(self.y_col))\n\n        PERIODICITIES = ["day_of_week", "month", "quarter"]\n        x_col = st.selectbox(\n            "Period",\n            options=PERIODICITIES,\n            index=PERIODICITIES.index(self.x_col),\n            format_func=lambda text: text.capitalize().replace("_", " "),\n        )\n\n        st.markdown("#### Filters")\n        start_year, end_year = year_range_slider(self.metric, self.period)\n        filters = multiselect_dimension_filters(self.metric, self.filters)\n\n        def apply_changes() -> None:\n            self.title = title\n            self.x_col = x_col\n            # self.y_col = y_col\n            self.period = Period(start=date(start_year, 1, 1), end=date(end_year, 12, 31))\n            self.filters = filters\n\n        st.button("Apply", use_container_width=True, on_click=apply_changes)\n\n    def figure(self) -> go.Figure:\n        metric = self.metric\n        cols = metric.columns.unquoted()\n        y_col = cols.category\n\n        df = fetch_correlation(self.metric, self.x_col, y_col, self.filters, self.period)\n\n        x = df[self.x_col.upper()]\n        y = df[y_col]\n        z = df[cols.metric_column]\n\n        if self.x_col in PERIOD_NAMES:\n            x = x.map(dict(enumerate(PERIOD_NAMES[self.x_col])))\n\n        return go.Figure(\n            layout=go.Layout(title=self.displayed_title()),\n            data=go.Heatmap(x=x, y=y, z=z).update(HEATMAP_STYLE),\n        ).update_layout(PLOT_LAYOUT)\n\n\n@st.cache_data\ndef fetch_correlation(\n    metric: Metric,\n    x_column: str,\n    y_column: str,\n    filters: db.Filters,\n    period: Optional[Period] = None,\n) -> pd.DataFrame:\n    """Fetch the dataframe with [X, Y, Metric] columns."""\n    cols = metric.columns\n\n    def category_column(col: str) -> f.Column:\n        """Either use the given dimension column or the truncated date column"""\n        if col in RANGE_PERIODICITIES_CONFIGS:\n            config = RANGE_PERIODICITIES_CONFIGS[col]\n            return config["period_function"](cols.timestamp).alias(col)\n\n        return f.col(col)\n\n    x_col = category_column(x_column)\n    y_col = category_column(y_column)\n\n    return (\n        metric.dataframe()\n        .select([x_col, y_col, cols.metric_column])\n        .filter(filters.to_condition())\n        .filter(f.col(cols.timestamp).between(period.start, period.end) if period else f.lit(True))\n        .group_by([x_col.get_name(), y_col.get_name()])\n        .agg(getattr(f, cols.agg_method)(cols.metric_column).alias(cols.metric_column))\n        .sort([x_col.get_name(), y_col.get_name()])\n        .to_pandas()\n    )\n\n\nRangePeriodicity = Literal["day_of_week", "month", "quarter"]\n# ^ "day_of_month", "day_of_week", "day_of_year", "week", "month", "quarter"\nRANGE_PERIODICITIES = get_args(RangePeriodicity)\n\nPERIOD_NAMES: Dict[RangePeriodicity, List[str]] = {\n    "day_of_week": ["Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"],\n    "month": calendar.month_name,\n    "quarter": ["", "Q1", "Q2", "Q3", "Q4"],\n}\n\nRANGE_PERIODICITIES_CONFIGS: Dict[RangePeriodicity, Dict[str, Any]] = {\n    "day_of_week": {"period_function": f.dayofweek, "grain": "day"},\n    "day_of_month": {"period_function": f.dayofmonth, "grain": "day"},\n    "day_of_year": {"period_function": f.dayofyear, "grain": "day"},\n    "week": {"period_function": f.weekofyear, "grain": "week"},\n    "month": {"period_function": f.month, "grain": "month"},\n    "quarter": {"period_function": f.quarter, "grain": "quarter"},\n}\n')
    __stickytape_write_module('erp_sales_assessment/widgets/product_category_distribution_widgets.py', b'from datetime import date\nfrom typing import Literal, Optional, Tuple, get_args\n\nimport pandas as pd\nimport plotly.graph_objs as go\nimport snowflake.snowpark.functions as f\nimport streamlit as st\n\nfrom erp_sales_assessment.components.plotly_styles import BAR_STYLE, COLORS, PIE_LAYOUT, PIE_STYLE, PLOT_LAYOUT\nfrom erp_sales_assessment.data import db\nfrom erp_sales_assessment.data.metric import Metric\nfrom erp_sales_assessment.time_series.period import Period\nfrom erp_sales_assessment.widgets.example_charts import example_bar_comparison_chart, example_pie_chart\nfrom erp_sales_assessment.widgets.parameters import multiselect_dimension_filters, year_range_slider\nfrom erp_sales_assessment.widgets.widget import Widget, WidgetSize\n\nViewingMode = Literal["percent", "value"]\nVIEWING_MODES = get_args(ViewingMode)\n\nSortOrder = Literal["desc", "asc"]\nSORT_LABELS = {"desc": "Bestselling", "asc": "Underperforming"}\n\n\nclass ProductCategoryDistributionPieChart(Widget):\n    widget_kind: Literal["chart", "kpi", "text"] = "chart"\n    widget_type: Literal["ProductCategoryDistributionPieChart"] = "ProductCategoryDistributionPieChart"\n    size: WidgetSize = "medium"\n\n    # Parameters\n    metric: Metric\n    textinfo: Literal["percent", "value"] = "percent"\n    sort_order: Literal["asc", "desc"] = "desc"  # top or bottom products\n    limit: int = 5\n\n    # Filters\n    period: Optional[Period]\n    filters: db.Filters = db.Filters()\n\n    @staticmethod\n    def description() -> str:\n        return "Purchase Behaviour - Product"\n\n    @staticmethod\n    def example_figure() -> go.Figure:\n        return example_pie_chart()\n\n    def default_title(self) -> str:\n        return f"Purchase Behavior - {SORT_LABELS[self.sort_order]} Product"\n\n    def edit_parameters(self) -> None:\n        title = st.text_input("Title", value=self.title, placeholder=self.default_title())\n        textinfo = st.radio(\n            "Viewing mode",\n            VIEWING_MODES,\n            index=VIEWING_MODES.index(self.textinfo),\n            horizontal=True,\n            format_func=str.capitalize,\n        )\n\n        sort_order = st.radio(\n            "Bestselling or Underperforming",\n            ["desc", "asc"],\n            index=["desc", "asc"].index(self.sort_order),\n            horizontal=True,\n            format_func=lambda value: SORT_LABELS[value],\n        )\n\n        limit = st.number_input(f"Count", value=self.limit)\n\n        st.markdown("#### Filters")\n        start_year, end_year = year_range_slider(self.metric, self.period)\n        filters = multiselect_dimension_filters(self.metric, self.filters)\n\n        def apply_changes() -> None:\n            self.title = title\n            self.textinfo = textinfo\n            self.limit = limit\n            self.sort_order = sort_order\n            self.period = Period(start=date(start_year, 1, 1), end=date(end_year, 12, 31))\n            self.filters = filters\n\n        st.button("Apply", use_container_width=True, on_click=apply_changes)\n\n    def figure(self) -> go.Figure:\n        df = fetch_categories(self.metric, self.filters, self.period, limit=self.limit, sort_order=self.sort_order)\n\n        cols = self.metric.columns.unquoted()\n        return (\n            go.Figure(\n                layout=go.Layout(title=self.displayed_title()),\n                data=go.Pie(\n                    labels=df[cols.category],\n                    values=df[cols.metric_column],\n                    textinfo=self.textinfo,\n                ).update(PIE_STYLE),\n            )\n            .update_layout(PLOT_LAYOUT)\n            .update_layout(PIE_LAYOUT)\n        )\n\n\nclass ProductCategoriesComparisonBarGraph(Widget):\n    widget_kind: Literal["chart", "kpi", "text"] = "chart"\n    widget_type: Literal["ProductCategoriesComparisonBarGraph"] = "ProductCategoriesComparisonBarGraph"\n    size: WidgetSize = "medium"\n\n    # Parameters\n    metric: Metric\n    year_a: Optional[int]\n    year_b: Optional[int]\n\n    # Filters\n    filters: db.Filters = db.Filters()\n\n    @staticmethod\n    def description() -> str:\n        return "Bar Graph of Top Product Categories"\n\n    @staticmethod\n    def example_figure() -> go.Figure:\n        return example_bar_comparison_chart()\n\n    def default_title(self) -> str:\n        return "Product Category YoY"\n\n    def edit_parameters(self) -> None:\n        title = st.text_input("Title", value=self.title, placeholder=self.default_title())\n\n        min_date, max_date = self.metric.date_range()\n        years = list(range(min_date.year, max_date.year + 1))\n\n        cols = st.columns([1, 1])\n        year_a = cols[0].selectbox("Comparing year", years, index=years.index(self.year_a or max_date.year - 1))\n        year_b = cols[1].selectbox("with year", years, index=years.index(self.year_b or max_date.year))\n\n        st.markdown("#### Filters")\n        filters = multiselect_dimension_filters(self.metric, self.filters)\n\n        def apply_changes() -> None:\n            self.title = title\n            self.year_a = year_a\n            self.year_b = year_b\n            self.filters = filters\n\n        st.button("Apply", use_container_width=True, on_click=apply_changes)\n\n    def figure(self) -> go.Figure:\n        period_a, period_b = self.periods()\n        df_a = fetch_categories(self.metric, self.filters, period_a)\n        df_b = fetch_categories(self.metric, self.filters, period_b)\n\n        cols = self.metric.columns.unquoted()\n\n        year_a, year_b = self.years()\n\n        return go.Figure(\n            layout=go.Layout(title=self.displayed_title()),\n            data=[\n                go.Bar(\n                    name=year_a,\n                    x=df_a[cols.category],\n                    y=df_a[cols.metric_column],\n                    marker_color=COLORS["bar-chart-a"],\n                ).update(BAR_STYLE),\n                go.Bar(\n                    name=year_b,\n                    x=df_b[cols.category],\n                    y=df_b[cols.metric_column],\n                    marker_color=COLORS["bar-chart-b"],\n                ).update(BAR_STYLE),\n            ],\n        ).update_layout(PLOT_LAYOUT)\n\n    def periods(self) -> Tuple[Period, Period]:\n        """Return the pair of periods and if the periods are year-to-date (YTD)."""\n        year_a, year_b = self.years()\n        min_date, max_date = self.metric.date_range()\n\n        start_month_day = (1, 1)\n        end_month_day = (12, 31)\n\n        if min_date.year in [year_a, year_b]:\n            start_month_day = (min_date.month, min_date.day)\n\n        if max_date.year in [year_a, year_b]:\n            end_month_day = (max_date.month, max_date.day)\n\n        def year_period(year: int) -> Period:\n            return Period(start=date(year, *start_month_day), end=date(year, *end_month_day))\n\n        return year_period(year_a), year_period(year_b)\n\n    def years(self) -> Tuple[int, int]:\n        _, max_date = self.metric.date_range()\n\n        year_a = self.year_a or max_date.year - 1\n        year_b = self.year_b or max_date.year\n\n        return year_a, year_b\n\n\n@st.cache_data\ndef fetch_categories(\n    metric: Metric,\n    filters: db.Filters,\n    period: Optional[Period] = None,\n    limit: int = 100,\n    sort_order: SortOrder = "desc",\n) -> pd.DataFrame:\n    cols = metric.columns\n    return (\n        metric.dataframe()\n        .filter(filters.to_condition())\n        .filter(f.col(cols.timestamp).between(period.start, period.end) if period else f.lit(True))\n        .group_by(cols.category)\n        .agg(f.sum(cols.metric_column).alias(cols.metric_column))\n        .sort(cols.metric_column, ascending=sort_order == "asc")\n        .limit(limit)\n        .to_pandas()\n    )\n\n\ndef retrieve_category_column(metric: Metric) -> str:\n    assert len(metric.columns.dimensions) > 0, "Metric must have a category column."\n\n    CATEGORY_WORDS = ["CATEGORY", "KIND", "TYPE", "CAT"]\n\n    for word in CATEGORY_WORDS:\n        for col in metric.columns.dimensions:\n            if word in col.upper():\n                return col\n\n    return metric.columns.dimensions[0]\n')
    __stickytape_write_module('erp_sales_assessment/widgets/purchasing_trend_over_time_widgets.py', b'from datetime import date\nfrom typing import Literal, Optional\n\nimport pandas as pd\nimport plotly.graph_objs as go\nimport snowflake.snowpark.functions as f\nimport streamlit as st\nfrom typing_extensions import Literal\n\nfrom erp_sales_assessment.components.plotly_styles import PLOT_LAYOUT, SCATTER_STYLE\nfrom erp_sales_assessment.data import db\nfrom erp_sales_assessment.data.metric import Metric\nfrom erp_sales_assessment.time_series.period import Period\nfrom erp_sales_assessment.time_series.time_series import GRAINS, Grain, TimeSeries\nfrom erp_sales_assessment.widgets.example_charts import example_line_chart\nfrom erp_sales_assessment.widgets.parameters import multiselect_dimension_filters, year_range_slider\nfrom erp_sales_assessment.widgets.widget import Widget, WidgetSize\n\n\nclass PurchasingTrendsLineChart(Widget):\n    widget_kind: Literal["chart", "kpi", "text"] = "chart"\n    widget_type: Literal["MonthlyPurchasingTrendsLineChart"] = "MonthlyPurchasingTrendsLineChart"\n    size: WidgetSize = "medium"\n\n    # Parameters\n    metric: Metric\n    grain: Grain = "month"\n\n    # Filters\n    period: Optional[Period]\n    filters: db.Filters = db.Filters()\n\n    @staticmethod\n    def description() -> str:\n        return "Line Chart of Purchasing Trends"\n\n    @staticmethod\n    def example_figure() -> go.Figure:\n        return example_line_chart()\n\n    def default_title(self) -> str:\n        NAMES = {\n            "day": "Daily",\n            "week": "Weekly",\n            "month": "Monthly",\n            "quarter": "Quarterly",\n            "year": "Yearly",\n        }\n        return f"{NAMES[self.grain]} Purchasing Trend"\n\n    def edit_parameters(self) -> None:\n        title = st.text_input("Title", value=self.title, placeholder=self.default_title())\n        grain = st.selectbox("Periodicity", GRAINS, index=GRAINS.index(self.grain))\n\n        st.markdown("#### Filters")\n        start_year, end_year = year_range_slider(self.metric, self.period)\n        filters = multiselect_dimension_filters(self.metric, self.filters)\n\n        def apply_changes() -> None:\n            self.title = title\n            self.grain = grain\n            self.period = Period(start=date(start_year, 1, 1), end=date(end_year, 12, 31))\n            self.filters = filters\n\n        st.button("Apply", use_container_width=True, on_click=apply_changes)\n\n    def figure(self) -> go.Figure:\n        df = fetch_time_series(self.metric, self.grain, self.filters, self.period)\n\n        metric = self.metric\n        cols = metric.columns.unquoted()\n\n        return go.Figure(\n            layout=go.Layout(title=self.displayed_title()),\n            data=go.Scatter(\n                name=metric.name,\n                x=df["PERIOD"],\n                y=df[cols.metric_column],\n                mode="lines+markers",\n            ).update(SCATTER_STYLE),\n        ).update_layout(PLOT_LAYOUT)\n\n\n@st.cache_data\ndef fetch_time_series(\n    metric: Metric,\n    grain: Grain,\n    filters: db.Filters,\n    period: Optional[Period] = None,\n) -> pd.DataFrame:\n    metric_column = metric.columns.metric_column\n\n    return (\n        TimeSeries(metric=metric, grain=grain)\n        .dataframe()\n        .filter(filters.to_condition())\n        .filter(f.col("period").between(period.start, period.end) if period else f.lit(True))\n        .group_by("period")\n        .agg(f.sum(metric_column).alias(metric_column))\n        .sort("period")\n        .to_pandas()\n    )\n')
    __stickytape_write_module('erp_sales_assessment/data/sample_configuration.py', b'import datetime\n\nfrom erp_sales_assessment.data import db\n\nsample_data = {"database": db.fetch_current_database(), "schema": "MAXA_DEMO"}\n\nsample_config = {\n    "dimensions": [\n        {"uid": "e9913d67-07b9-43cd-9793-97c8f714928e", "name": "Product", "predefined": True},\n        {"uid": "41798dd3-0d3d-4d2b-ac45-0a9f8b4204c5", "name": "Category", "predefined": False},\n        {"uid": "47b57c81-ae2f-4e97-8fd0-50f592c8fc70", "name": "Customer", "predefined": False},\n    ],\n    "activities": [{"uid": "3c706611-2461-46df-aee4-9aca8687f796", "name": "Purchase Order", "predefined": True}],\n    "metrics": [\n        {\n            "uid": "949a127b-9414-4d36-9b80-4bed90d33e50",\n            "name": "Revenue",\n            "act_id": "3c706611-2461-46df-aee4-9aca8687f796",\n            "columns": {\n                "dimensions": ["REGION", "CUSTOMER_NAME", "PRODUCT_NAME"],\n                "metric_column": "PRICE",\n                "timestamp": "PURCHASE_DATE",\n                "agg_method": "sum",\n                "category": "CATEGORY",\n                "labels": {\n                    "PURCHASE_DATE": "Period",\n                    "PRICE": "Revenue",\n                    "CATEGORY": "Product Category",\n                    "REGION": "Region",\n                    "CUSTOMER_NAME": "Customer Name",\n                    "PRODUCT_NAME": "Product Name",\n                },\n            },\n        }\n    ],\n    "selected_tables": [\n        {\n            "uid": "bb9a436e-167a-4b05-9d5d-6b186495c547",\n            "table_name": "PURCHASE_ORDER",\n            "table_schema": sample_data["schema"],\n            "table_database": sample_data["database"],\n            "entity_uid": "3c706611-2461-46df-aee4-9aca8687f796",\n            "entity_type": "activity",\n            "columns": {\n                "ORDER_ID": "INTEGER",\n                "PURCHASE_DATE": "DATE",\n                "PRODUCT_ID": "INTEGER",\n                "QUANTITY": "INTEGER",\n                "PRICE": "INTEGER",\n                "CUSTOMER_ID": "INTEGER",\n                "SUPPLIER_ID": "INTEGER",\n                "STATUS": "TEXT",\n            },\n        },\n        {\n            "uid": "6b563b80-1a35-489f-8c46-2c84d088bfa1",\n            "table_name": "PRODUCT",\n            "table_schema": sample_data["schema"],\n            "table_database": sample_data["database"],\n            "entity_uid": "e9913d67-07b9-43cd-9793-97c8f714928e",\n            "entity_type": "dimension",\n            "columns": {"PRODUCT_ID": "INTEGER", "PRODUCT_NAME": "TEXT", "CATEGORY_ID": "INTEGER", "PRICE": "INTEGER"},\n        },\n        {\n            "uid": "39a13c71-d570-4f18-9164-22a2148bdb04",\n            "table_name": "CATEGORY",\n            "table_schema": sample_data["schema"],\n            "table_database": sample_data["database"],\n            "entity_uid": "41798dd3-0d3d-4d2b-ac45-0a9f8b4204c5",\n            "entity_type": "dimension",\n            "columns": {"CATEGORY_ID": "INTEGER", "CATEGORY": "TEXT"},\n        },\n        {\n            "uid": "810311a4-6894-4693-84bb-e213d6a46577",\n            "table_name": "CUSTOMER",\n            "table_schema": sample_data["schema"],\n            "table_database": sample_data["database"],\n            "entity_uid": "47b57c81-ae2f-4e97-8fd0-50f592c8fc70",\n            "entity_type": "dimension",\n            "columns": {"CUSTOMER_ID": "INTEGER", "CUSTOMER_NAME": "TEXT", "CUSTOMER_PHONE": "TEXT", "REGION": "TEXT"},\n        },\n    ],\n    "dim_act_mappings": [\n        {\n            "dim_id": "e9913d67-07b9-43cd-9793-97c8f714928e",\n            "act_id": "3c706611-2461-46df-aee4-9aca8687f796",\n            "main_table_id": "bb9a436e-167a-4b05-9d5d-6b186495c547",\n            "joined_table_id": "6b563b80-1a35-489f-8c46-2c84d088bfa1",\n            "main_column": "PRODUCT_ID",\n            "joined_column": "PRODUCT_ID",\n        },\n        {\n            "dim_id": "41798dd3-0d3d-4d2b-ac45-0a9f8b4204c5",\n            "act_id": "3c706611-2461-46df-aee4-9aca8687f796",\n            "main_table_id": "6b563b80-1a35-489f-8c46-2c84d088bfa1",\n            "joined_table_id": "39a13c71-d570-4f18-9164-22a2148bdb04",\n            "main_column": "CATEGORY_ID",\n            "joined_column": "CATEGORY_ID",\n        },\n        {\n            "dim_id": "47b57c81-ae2f-4e97-8fd0-50f592c8fc70",\n            "act_id": "3c706611-2461-46df-aee4-9aca8687f796",\n            "main_table_id": "bb9a436e-167a-4b05-9d5d-6b186495c547",\n            "joined_table_id": "810311a4-6894-4693-84bb-e213d6a46577",\n            "main_column": "CUSTOMER_ID",\n            "joined_column": "CUSTOMER_ID",\n        },\n    ],\n    "widgets": [\n        {\n            "widget_kind": "kpi",\n            "size": "small",\n            "title": "",\n            "widget_type": "TotalPurchaseValue",\n            "metric": {\n                "uid": "949a127b-9414-4d36-9b80-4bed90d33e50",\n                "name": "Revenue",\n                "act_id": "3c706611-2461-46df-aee4-9aca8687f796",\n                "columns": {\n                    "dimensions": ["REGION", "CUSTOMER_NAME", "PRODUCT_NAME"],\n                    "metric_column": "PRICE",\n                    "timestamp": "PURCHASE_DATE",\n                    "agg_method": "sum",\n                    "category": "CATEGORY",\n                    "labels": {\n                        "PURCHASE_DATE": "Period",\n                        "PRICE": "Revenue",\n                        "CATEGORY": "Product Category",\n                        "REGION": "Region",\n                        "CUSTOMER_NAME": "Customer Name",\n                        "PRODUCT_NAME": "Product Name",\n                    },\n                },\n            },\n            "grain": "year",\n            "period": None,\n            "filters": {"mapping": {}},\n        },\n        {\n            "widget_kind": "kpi",\n            "size": "small",\n            "title": "",\n            "widget_type": "TotalPurchaseValue",\n            "metric": {\n                "uid": "949a127b-9414-4d36-9b80-4bed90d33e50",\n                "name": "Revenue",\n                "act_id": "3c706611-2461-46df-aee4-9aca8687f796",\n                "columns": {\n                    "dimensions": ["REGION", "CUSTOMER_NAME", "PRODUCT_NAME"],\n                    "metric_column": "PRICE",\n                    "timestamp": "PURCHASE_DATE",\n                    "agg_method": "sum",\n                    "category": "CATEGORY",\n                    "labels": {\n                        "PURCHASE_DATE": "Period",\n                        "PRICE": "Revenue",\n                        "CATEGORY": "Product Category",\n                        "REGION": "Region",\n                        "CUSTOMER_NAME": "Customer Name",\n                        "PRODUCT_NAME": "Product Name",\n                    },\n                },\n            },\n            "grain": "month",\n            "period": None,\n            "filters": {"mapping": {}},\n        },\n        {\n            "widget_kind": "kpi",\n            "size": "small",\n            "title": "",\n            "widget_type": "TotalPurchaseValue",\n            "metric": {\n                "uid": "949a127b-9414-4d36-9b80-4bed90d33e50",\n                "name": "Revenue",\n                "act_id": "3c706611-2461-46df-aee4-9aca8687f796",\n                "columns": {\n                    "dimensions": ["REGION", "CUSTOMER_NAME", "PRODUCT_NAME"],\n                    "metric_column": "PRICE",\n                    "timestamp": "PURCHASE_DATE",\n                    "agg_method": "sum",\n                    "category": "CATEGORY",\n                    "labels": {\n                        "PURCHASE_DATE": "Period",\n                        "PRICE": "Revenue",\n                        "CATEGORY": "Product Category",\n                        "REGION": "Region",\n                        "CUSTOMER_NAME": "Customer Name",\n                        "PRODUCT_NAME": "Product Name",\n                    },\n                },\n            },\n            "grain": "week",\n            "period": None,\n            "filters": {"mapping": {}},\n        },\n        {\n            "widget_kind": "kpi",\n            "size": "small",\n            "title": "",\n            "widget_type": "TotalPurchaseValue",\n            "metric": {\n                "uid": "949a127b-9414-4d36-9b80-4bed90d33e50",\n                "name": "Revenue",\n                "act_id": "3c706611-2461-46df-aee4-9aca8687f796",\n                "columns": {\n                    "dimensions": ["REGION", "CUSTOMER_NAME", "PRODUCT_NAME"],\n                    "metric_column": "PRICE",\n                    "timestamp": "PURCHASE_DATE",\n                    "agg_method": "sum",\n                    "category": "CATEGORY",\n                    "labels": {\n                        "PURCHASE_DATE": "Period",\n                        "PRICE": "Revenue",\n                        "CATEGORY": "Product Category",\n                        "REGION": "Region",\n                        "CUSTOMER_NAME": "Customer Name",\n                        "PRODUCT_NAME": "Product Name",\n                    },\n                },\n            },\n            "grain": "day",\n            "period": None,\n            "filters": {"mapping": {}},\n        },\n        {\n            "widget_kind": "chart",\n            "size": "medium",\n            "title": "",\n            "widget_type": "MonthlyPurchasingTrendsLineChart",\n            "metric": {\n                "uid": "949a127b-9414-4d36-9b80-4bed90d33e50",\n                "name": "Revenue",\n                "act_id": "3c706611-2461-46df-aee4-9aca8687f796",\n                "columns": {\n                    "dimensions": ["REGION", "CUSTOMER_NAME", "PRODUCT_NAME"],\n                    "metric_column": "PRICE",\n                    "timestamp": "PURCHASE_DATE",\n                    "agg_method": "sum",\n                    "category": "CATEGORY",\n                    "labels": {\n                        "PURCHASE_DATE": "Period",\n                        "PRICE": "Revenue",\n                        "CATEGORY": "Product Category",\n                        "REGION": "Region",\n                        "CUSTOMER_NAME": "Customer Name",\n                        "PRODUCT_NAME": "Product Name",\n                    },\n                },\n            },\n            "grain": "month",\n            "period": None,\n            "filters": {"mapping": {}},\n        },\n        {\n            "widget_kind": "chart",\n            "size": "medium",\n            "title": "",\n            "widget_type": "ProductCategoriesComparisonBarGraph",\n            "metric": {\n                "uid": "949a127b-9414-4d36-9b80-4bed90d33e50",\n                "name": "Revenue",\n                "act_id": "3c706611-2461-46df-aee4-9aca8687f796",\n                "columns": {\n                    "dimensions": ["REGION", "CUSTOMER_NAME", "PRODUCT_NAME"],\n                    "metric_column": "PRICE",\n                    "timestamp": "PURCHASE_DATE",\n                    "agg_method": "sum",\n                    "category": "CATEGORY",\n                    "labels": {\n                        "PURCHASE_DATE": "Period",\n                        "PRICE": "Revenue",\n                        "CATEGORY": "Product Category",\n                        "REGION": "Region",\n                        "CUSTOMER_NAME": "Customer Name",\n                        "PRODUCT_NAME": "Product Name",\n                    },\n                },\n            },\n            "year_a": None,\n            "year_b": None,\n            "filters": {"mapping": {}},\n        },\n        {\n            "widget_kind": "chart",\n            "size": "medium",\n            "title": "",\n            "widget_type": "ProductCategoryDistributionPieChart",\n            "metric": {\n                "uid": "949a127b-9414-4d36-9b80-4bed90d33e50",\n                "name": "Revenue",\n                "act_id": "3c706611-2461-46df-aee4-9aca8687f796",\n                "columns": {\n                    "dimensions": ["REGION", "CUSTOMER_NAME", "PRODUCT_NAME"],\n                    "metric_column": "PRICE",\n                    "timestamp": "PURCHASE_DATE",\n                    "agg_method": "sum",\n                    "category": "CATEGORY",\n                    "labels": {\n                        "PURCHASE_DATE": "Period",\n                        "PRICE": "Revenue",\n                        "CATEGORY": "Product Category",\n                        "REGION": "Region",\n                        "CUSTOMER_NAME": "Customer Name",\n                        "PRODUCT_NAME": "Product Name",\n                    },\n                },\n            },\n            "textinfo": "percent",\n            "sort_order": "desc",\n            "limit": 5,\n            "period": None,\n            "filters": {"mapping": {}},\n        },\n        {\n            "widget_kind": "chart",\n            "size": "large",\n            "title": "",\n            "widget_type": "PurchaseAnomaliesHeatmap",\n            "metric": {\n                "uid": "949a127b-9414-4d36-9b80-4bed90d33e50",\n                "name": "Revenue",\n                "act_id": "3c706611-2461-46df-aee4-9aca8687f796",\n                "columns": {\n                    "dimensions": ["REGION", "CUSTOMER_NAME", "PRODUCT_NAME"],\n                    "metric_column": "PRICE",\n                    "timestamp": "PURCHASE_DATE",\n                    "agg_method": "sum",\n                    "category": "CATEGORY",\n                    "labels": {\n                        "PURCHASE_DATE": "Period",\n                        "PRICE": "Revenue",\n                        "CATEGORY": "Product Category",\n                        "REGION": "Region",\n                        "CUSTOMER_NAME": "Customer Name",\n                        "PRODUCT_NAME": "Product Name",\n                    },\n                },\n            },\n            "x_col": "day_of_week",\n            "period": {"start": datetime.date(2019, 1, 1), "end": datetime.date(2023, 12, 31)},\n            "filters": {"mapping": {}},\n        },\n    ],\n}\n')
    __stickytape_write_module('erp_sales_assessment/app_pages/configuration/entity_table_relationship.py', b'from typing import List\n\nimport streamlit as st\nfrom streamlit.runtime.state import WidgetCallback\n\nfrom erp_sales_assessment.components.selectbox_optional import selectbox_optional\nfrom erp_sales_assessment.components.streamlit_in_snowflake import st_image_base64\nfrom erp_sales_assessment.data.db import ID_TYPES\nfrom erp_sales_assessment.data.helpers import repository, validate_state\nfrom erp_sales_assessment.data.models import Activity, DbTable, DimensionActivityMapping\nfrom erp_sales_assessment.diagrams.mapping_diagram import graphviz_table_relations\n\n\ndef form_entity_table_relationship() -> None:\n    """Form to define the relationship between tables."""\n    if not entities_are_valid():\n        return\n\n    for act_idx, activity in enumerate(repository().activities):\n        if act_idx != 0:\n            st.markdown("---")\n\n        _activity_table_relationship(activity)\n\n\ndef entities_are_valid() -> bool:\n    """Return true if dimensions and activities have a defined associated table."""\n    dimensions_valid, _ = validate_state(repository().dimensions, "dimension", validate_each=True)\n    activities_valid, _ = validate_state(repository().activities, "activity", validate_each=True)\n\n    return dimensions_valid and activities_valid\n\n\ndef _activity_table_relationship(activity: Activity) -> None:\n    activity_table = activity.db_table\n    assert activity_table, "Activity is validated to have a table"\n\n    dimension_tables: List[DbTable] = [dim.db_table for dim in activity.dimensions]\n    assert all(dimension_tables), "Dimensions are validated to have tables"\n\n    if len(activity.mappings) == 0:\n        st.markdown("No dimension to associate.")\n        return\n\n    for index, mapping in enumerate(activity.mappings):\n        _edit_mapping(mapping, dimension_tables, is_first=index == 0)\n\n    st.markdown("###")\n    graphviz_table_relations(activity)\n\n\ndef _edit_mapping(mapping: DimensionActivityMapping, dimension_tables: List[DbTable], is_first: bool) -> None:\n    activity = mapping.activity\n    dimension = mapping.dimension\n\n    activity_table = activity.db_table\n    mapping_key = f"{activity.uid}_{dimension.uid}"\n\n    current_dim_table = dimension.db_table\n\n    cols = st.columns([3, 3, 1, 3, 3])\n\n    # Default mapping\n    if mapping.main_table is None or mapping.joined_table is None:\n        mapping.main_table_id = activity_table.uid\n        mapping.joined_table_id = current_dim_table.uid\n        mapping.main_column = None  # Column to actively set\n        mapping.joined_column = current_dim_table.columns_of_type(ID_TYPES)[0]\n\n    def set_mapping_field(field: str, state_key: str) -> WidgetCallback:\n        return lambda: setattr(mapping, field, st.session_state[state_key])\n\n    with cols[0]:\n        st.text_input(\n            "**Dimension table**",\n            value=mapping.joined_table.table_name,\n            key=f"dimension_{mapping_key}",\n            disabled=True,\n            label_visibility="visible" if is_first else "collapsed",\n        )\n\n    with cols[1]:\n        joined_table = current_dim_table\n        joined_columns = joined_table.columns_of_type(ID_TYPES)\n\n        st.selectbox(\n            "**Column**",\n            options=joined_columns,\n            index=joined_columns.index(mapping.joined_column),\n            key=f"joined_column_{mapping_key}",\n            on_change=set_mapping_field("joined_column", f"joined_column_{mapping_key}"),\n            label_visibility="visible" if is_first else "collapsed",\n        )\n\n    with cols[2]:\n        if is_first:\n            st.markdown("##")\n        st_image_base64("arrow.png")\n\n    with cols[3]:\n        other_dim_tables = [dim_table for dim_table in dimension_tables if dim_table != current_dim_table]\n\n        def update_main_table() -> None:\n            selected_table: DbTable = st.session_state[f"main_table_{mapping_key}"]\n            mapping.main_table_id = selected_table.uid\n            mapping.main_column = selected_table.columns_of_type(ID_TYPES)[0]\n\n        tables = [activity_table] + other_dim_tables\n\n        st.selectbox(\n            "**Associated with table**",\n            options=tables,\n            index=tables.index(mapping.main_table),\n            format_func=lambda table: table.table_name,\n            key=f"main_table_{mapping_key}",\n            on_change=update_main_table,\n            label_visibility="visible" if is_first else "collapsed",\n        )\n\n    with cols[4]:\n        main_table = mapping.main_table\n        main_columns = main_table.columns_of_type(ID_TYPES)\n\n        selectbox_optional(\n            "**Target Column**",\n            options=main_columns,\n            value=mapping.main_column,\n            none_label="Select column...",\n            key=f"main_column_{mapping_key}",\n            on_change=set_mapping_field("main_column", f"main_column_{mapping_key}"),\n            label_visibility="visible" if is_first else "collapsed",\n        )\n')
    __stickytape_write_module('erp_sales_assessment/components/streamlit_in_snowflake.py', b'import base64\nfrom pathlib import Path\n\nimport streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.exceptions import SnowparkSessionException\n\n\n@st.cache_resource\ndef is_inside_snowflake() -> bool:\n    """Return true if Streamlit is executed within Snowflake."""\n    try:\n        _ = get_active_session()\n        return True\n    except SnowparkSessionException:\n        return False\n\n\ndef st_image_base64(image_file: str, folder: Path = Path("images"), **kwargs):\n    """Streamlit image using base64 that can be displayed within Snowflake."""\n    if is_inside_snowflake():\n        folder = Path(".")\n\n    content_bytes = (folder / image_file).read_bytes()\n\n    mime_type = image_file.split(".")[-1:][0].lower()\n    st.image(f"data:image/{mime_type};base64,{base64.b64encode(content_bytes).decode()}", **kwargs)\n\n\ndef snowflake_ui_styles():\n    """Replicate the CSS styles when Streamlit is executed within Snowflake."""\n    if is_inside_snowflake():\n        return\n\n    st.markdown(\n        """\n        <style>\n        \n        /* Hide "Made by Streamlit" */\n        .appview-container .main footer {\n            display: none;\n        }\n        \n        /* Hide background of the top header bar */\n        .stApp > header, .stApp > header > div[data-testid="stDecoration"] {\n            background: #ffffff00;\n        }\n\n        /* Reduce top padding */\n        .main > .block-container {\n            padding-top: 1rem;  /* default = 6rem */\n        }\n        \n        </style>\n        """,\n        unsafe_allow_html=True,\n    )\n')
    __stickytape_write_module('erp_sales_assessment/diagrams/__init__.py', b'')
    __stickytape_write_module('erp_sales_assessment/diagrams/mapping_diagram.py', b'import streamlit as st\n\nfrom erp_sales_assessment.data.models import Activity, DbTable, DimensionActivityMapping\nfrom erp_sales_assessment.diagrams.entity_diagram import ENTITY_COLORS\n\n\ndef graphviz_table_relations(activity: Activity) -> None:\n    """Entity-Relationship-Diagram between Activity and Dimension tables."""\n    act_table = activity.db_table\n    tables = [act_table] + [dim.db_table for dim in activity.dimensions]\n\n    table_nodes = "\\n".join(_graphviz_table_node(table) for table in tables)\n    table_edges = "\\n".join(_graphviz_edge(mapping) for mapping in activity.mappings)\n\n    graphviz_dot = f"""\n        digraph G {{\n            graph [fontname="Source Sans Pro", fontsize=20, layout=dot, rankdir=LR, newrank=true]\n            node [style=filled, shape=rect, shape=plaintext]\n            edge [arrowsize=0.75]\n\n            {table_nodes}\n            {table_edges}\n        }}\n    """\n\n    st.graphviz_chart(graphviz_dot, use_container_width=True)\n\n\ndef _graphviz_table_node(table: DbTable) -> str:\n    """Return a html table representing the table and its columns."""\n    cols = [col.strip(\'"\') for col, _ in table.columns.items()]\n\n    col_table_rows = "\\n".join(f"""<tr><td port="{col}">{col}</td></tr>""" for col in cols)\n    bgcolor = ENTITY_COLORS[table.entity_type]\n\n    label_table = f"""\n        <table border="0" cellborder="1" cellspacing="0">\n            <tr><td bgcolor="{bgcolor}" align="CENTER"><b>{table.table_name}</b></td></tr>\n            {col_table_rows}\n        </table>\n    """\n    return f"""{table.table_name} [fillcolor="#ffffff00" label=<{label_table}>];"""\n\n\ndef _graphviz_edge(mapping: DimensionActivityMapping) -> str:\n    """Return the edge between tables of the mapping."""\n    main_table = mapping.main_table.table_name\n    joined_table = mapping.joined_table.table_name\n\n    return f"{joined_table}:{mapping.joined_column} -> {main_table}:{mapping.main_column}"\n')
    __stickytape_write_module('erp_sales_assessment/diagrams/entity_diagram.py', b'from typing import Dict, List, Union\n\nimport streamlit as st\n\nfrom erp_sales_assessment.data.helpers import repository\nfrom erp_sales_assessment.data.metric import Metric\nfrom erp_sales_assessment.data.models import Activity, Dimension\n\nENTITY_COLORS = {"activity": "#aab3fb", "dimension": "#7cbbf5", "metric": "#1a72c0"}\n\n\ndef entity_color(entity: Union[Activity, Dimension, Metric]) -> str:\n    return ENTITY_COLORS[type(entity).__name__.lower()]\n\n\ndef graphviz_entities() -> None:\n    """Diagram showing relationships between Dimensions Activities and Metrics."""\n    repo = repository()\n\n    nodes = []\n    nodes.extend(f\'"{dimension.name}" [fillcolor="{entity_color(dimension)}"]\' for dimension in repo.dimensions)\n    nodes.extend(f\'"{activity.name}" [fillcolor="{entity_color(activity)}"]\' for activity in repo.activities)\n    nodes.extend(f\'"{metric.name}" [fillcolor="{entity_color(metric)}"]\' for metric in repo.metrics)\n    nodes = "\\n".join(nodes)\n\n    edges = []\n    for activity in repo.activities:\n        edges.extend(f\'"{dim.name}" -> "{activity.name}"\' for dim in activity.dimensions)\n        edges.extend(f\'"{activity.name}" -> "{metric.name}"\' for metric in activity.metrics)\n    edges = "\\n".join(edges)\n\n    graphviz_dot = f"""\n        digraph G {{\n            graph [fontname="Source Sans Pro", fontsize=20, layout=dot, newrank=true]\n            node [style="rounded,filled", shape=box]\n            edge [arrowsize=0.75]\n\n            {nodes}\n            {edges}\n        }}\n    """\n\n    st.graphviz_chart(graphviz_dot)\n')
    __stickytape_write_module('erp_sales_assessment/app_pages/configuration/metric_definition.py', b'from collections import ChainMap\nfrom typing import Dict, List, Optional\n\nimport streamlit as st\nfrom streamlit.runtime.state import WidgetCallback\n\nfrom erp_sales_assessment.components.selectbox_optional import selectbox_optional\nfrom erp_sales_assessment.data import db\nfrom erp_sales_assessment.data.metric import Metric, MetricColumns\nfrom erp_sales_assessment.utils import first\n\n\ndef mapping_is_valid(metric: Metric) -> bool:\n    """Return true if the mapping of the tables used by the given metric are defined."""\n    activity = metric.activity\n    if not activity:\n        return False\n\n    errors = activity.validate_tables_and_mapping()\n    return len(errors) == 0\n\n\ndef form_metric_definition(metric: Metric) -> None:\n    """Form used to define columns of the metric."""\n    if not mapping_is_valid(metric):\n        return\n\n    activity = metric.activity\n\n    columns: Dict[str, db.ExtendedDataType] = {\n        **activity.db_table.columns,\n        **ChainMap(*[dim.db_table.columns for dim in activity.dimensions]),\n    }\n\n    def columns_of_types(data_types: List[db.ExtendedDataType]) -> List[str]:\n        """Return the columns having one of the given types."""\n        return [col for col, data_type in columns.items() if data_type in data_types]\n\n    metric_columns = columns_of_types(["FIXED", "REAL", "INTEGER"])\n    timestamp_columns = columns_of_types(db.DATETIME_TYPES)\n    category_columns = columns_of_types(["TEXT"])\n\n    if metric.columns is None:\n        metric.columns = MetricColumns(\n            metric_column=first(metric_columns) or "",\n            timestamp=first(timestamp_columns) or "",\n            category=retrieve_category_column(category_columns) or "",\n        )\n\n    def set_columns_field_and_update_dimensions(field: str, state_key: str) -> WidgetCallback:\n        def set_column():\n            column = st.session_state[state_key]\n            metric.columns.remove_dimension_columns({column})\n            setattr(metric.columns, field, column)\n\n        return set_column\n\n    def set_label(dimension: str, key: str) -> WidgetCallback:\n        def update_labels() -> None:\n            if value := st.session_state[key].strip():\n                metric.columns.labels.update(**{dimension: value})\n            else:\n                metric.columns.labels.pop(dimension, None)\n\n        return update_labels\n\n    left, right = st.columns([6, 6])\n\n    with left:\n        st.markdown("##### Attributes")\n        cols = st.columns([6, 6])\n        with cols[0]:\n            selectbox_optional(\n                "**Measure**",\n                metric_columns,\n                value=metric.columns.metric_column,\n                none_label="Select column..." if metric_columns else "No quantity columns...",\n                disabled=not metric_columns,\n                key=f"metric-column-{metric.uid}",\n                on_change=set_columns_field_and_update_dimensions("metric_column", f"metric-column-{metric.uid}"),\n                help="Column containing a quantitative value to aggregate",\n            )\n            selectbox_optional(\n                "**Timestamp**",\n                timestamp_columns,\n                value=metric.columns.timestamp,\n                none_label="Select column..." if timestamp_columns else "No date columns...",\n                disabled=not timestamp_columns,\n                key=f"timestamp-{metric.uid}",\n                on_change=set_columns_field_and_update_dimensions("timestamp", f"timestamp-{metric.uid}"),\n                help="Column containing the date of the event",\n            )\n            selectbox_optional(\n                "**Product Category**",\n                category_columns,\n                value=metric.columns.category,\n                none_label="Select column..." if category_columns else "No category columns...",\n                disabled=not category_columns,\n                key=f"category-{metric.uid}",\n                on_change=set_columns_field_and_update_dimensions("category", f"category-{metric.uid}"),\n                help="Column containing the category of the product",\n            )\n\n        with cols[1]:\n            st.text_input(\n                "**Label**",\n                value=metric.columns.labels.get(metric.columns.metric_column, ""),\n                placeholder="Type here...",\n                key=f"label-metric-{metric.uid}",\n                on_change=set_label(metric.columns.metric_column, f"label-metric-{metric.uid}"),\n            )\n            st.text_input(\n                "Label timestamp",\n                value=metric.columns.labels.get(metric.columns.timestamp, ""),\n                placeholder="Type here...",\n                key=f"label-timestamp-{metric.uid}",\n                on_change=set_label(metric.columns.timestamp, f"label-timestamp-{metric.uid}"),\n                label_visibility="hidden",\n            )\n            st.text_input(\n                "Label category",\n                value=metric.columns.labels.get(metric.columns.category, ""),\n                placeholder="Type here...",\n                key=f"label-category-{metric.uid}",\n                on_change=set_label(metric.columns.category, f"label-category-{metric.uid}"),\n                label_visibility="hidden",\n            )\n\n    with right:\n        st.markdown("##### Additional filters")\n\n        c = metric.columns\n        remaining_columns = [\n            col for col in columns if col not in [c.timestamp, c.metric_column, c.category] + c.dimensions\n        ]\n\n        for index, dim in enumerate(metric.columns.dimensions):\n            left, right, remove = st.columns([5, 5, 2])\n\n            def update_filter(dimension: str, state_key: str):\n                def update():\n                    metric.columns.dimensions[metric.columns.dimensions.index(dimension)] = st.session_state[state_key]\n\n                return update\n\n            left.selectbox(\n                "**Filter**",\n                [dim] + remaining_columns,\n                label_visibility="visible" if index == 0 else "collapsed",\n                key=f"dimension-{dim}",\n                on_change=update_filter(dim, f"dimension-{dim}"),\n            )\n            right.text_input(\n                "**Label**",\n                value=metric.columns.labels.get(dim, ""),\n                placeholder="Type here...",\n                key=f"label-{dim}-{metric.uid}",\n                on_change=set_label(dim, f"label-{dim}-{metric.uid}"),\n                label_visibility="visible" if index == 0 else "collapsed",\n            )\n\n            def remove_dim(dimension: str):\n                return lambda: metric.columns.dimensions.remove(dimension)\n\n            if index == 0:\n                remove.markdown("##")\n            remove.button("\xe2\x9c\x95", key=f"remove-{dim}", on_click=remove_dim(dim))\n\n        if remaining_columns:\n            left, _, _ = st.columns([5, 5, 2])\n\n            def add_filter():\n                metric.columns.dimensions.append(remaining_columns[0])\n\n            if not metric.columns.dimensions:\n                left.markdown("##")\n\n            left.button("\\+ Add filter", use_container_width=True, on_click=add_filter)\n\n    if metric.columns.is_valid():\n        st.markdown("")\n        st.markdown("##### Preview (first 10)")\n        st.dataframe(metric.preview_dataframe(), use_container_width=True)\n\n\ndef retrieve_category_column(columns: List[str]) -> Optional[str]:\n    """Retrieve the first column with the name looking like a category."""\n    if not columns:\n        return None\n\n    CATEGORY_WORDS = ["CATEGORY", "KIND", "TYPE", "CAT"]\n\n    for word in CATEGORY_WORDS:\n        for col in columns:\n            if word in col.upper():\n                return col\n\n    return columns[0]\n')
    __stickytape_write_module('erp_sales_assessment/components/action_button.py', b'from typing import Any, Callable\n\nimport streamlit as st\n\n\ndef action_button(label: str, message: str, key: str, disabled: bool, on_click: Callable[..., Any]) -> bool:\n    """Button that is replaced by the given message once done."""\n    placeholder = st.empty()\n\n    btn_container = placeholder.container()\n    btn_container.markdown("")\n\n    if btn_container.button(label, key=key, type="primary", use_container_width=True, disabled=disabled):\n        placeholder.empty()\n        on_click()\n        placeholder.info(message, icon="\xe2\x9c\x94\xef\xb8\x8f")\n')
    __stickytape_write_module('erp_sales_assessment/app_pages/dashboard_page.py', b'from typing import Literal, Optional\n\nimport streamlit as st\nfrom pydantic import BaseModel\n\nfrom erp_sales_assessment.app_pages.page import Page\nfrom erp_sales_assessment.components.plotly_styles import INDICATOR_SELECTED_STYLE, PLOTLY_CONFIG, SELECTED_STYLE\nfrom erp_sales_assessment.data import db\nfrom erp_sales_assessment.data.helpers import repository\nfrom erp_sales_assessment.utils import groupby\nfrom erp_sales_assessment.widgets.widget import Widget\n\n\nclass DashboardState(BaseModel):\n    """State of the Dashboard:\n    - list-widgets: Show the list of current dashboard widgets in the sidebar\n    - add-widget: Show the gallery of available widgets in the sidebar\n    - edit-widget: Highlight a selected widget and edit its parameters in the sidebar\n    """\n\n    sidebar: Literal["list-widgets", "add-widget", "edit-widget"] = "list-widgets"\n    selected_widget: Optional[Widget]\n\n\nclass DashboardPage(Page):\n    def __init__(self) -> None:\n        if "dashboard_state" not in st.session_state:\n            st.session_state["dashboard_state"] = DashboardState()\n\n        self.state: DashboardState = st.session_state["dashboard_state"]\n\n    def run(self) -> None:\n        if not repository().is_valid():\n            st.warning("Go to the **Configuration** tab to map your data.")\n            return\n\n        with st.sidebar:\n            self.widget_list()\n\n            # if self.state.sidebar == "add-widget":\n\n            #     def add_and_edit_widget(widget_class: Type[Widget]):\n            #         repository().widgets.append(widget_class.default())\n\n            #     widget_gallery(on_add=add_and_edit_widget)\n            #     return\n\n            if self.state.sidebar == "edit-widget":\n                with db.catch_errors():\n                    self.state.selected_widget.edit_parameters()\n                return\n\n        with db.catch_errors():\n            self.render_widgets()\n\n    def render_widgets(self) -> None:\n        """Charts of all the dashboard widgets aligned in two columns."""\n        widgets = repository().widgets\n        kpi_widgets = [widget for widget in widgets if widget.widget_kind == "kpi"]\n        chart_widgets = groupby([widget for widget in widgets if widget.widget_kind == "chart"], key=lambda w: w.size)\n\n        # Top row: Text + 4 KPIs\n        text_col, *kpi_cols = st.columns([4] + [2] * 4)\n        text_col.markdown(\n            """\n            ### :blue[ERP Sales Data Assessment]\n            \n            Identify trends, anomalies and top product categories to drive cost savings and improve financial performance.\n            """\n        )\n        for col, kpi in zip(kpi_cols, kpi_widgets):\n            with col:\n                widget_kpi(kpi, is_selected=kpi is self.state.selected_widget)\n\n        # Middle row: 3 medium charts\n        cols = st.columns([4, 4, 4])\n        for widget, col in zip(chart_widgets["medium"], cols):\n            with col:\n                widget_chart(widget, is_selected=widget is self.state.selected_widget)\n\n        # Bottom row: 1 large chart + 1 text\n        left, right = st.columns([6, 6])\n        if chart_widgets["large"] and (widget := chart_widgets["large"].pop()):\n            with left:\n                widget_chart(widget, is_selected=widget is self.state.selected_widget)\n\n        with right:\n            with st.expander("**Purchasing Behavior and Anomalies**", expanded=True):\n                st.markdown(\n                    """\n                    - Gain valuable insights into purchasing behavior\n                    - Track revenue trends and compare timelines\n                    - Identify anomalies and outliers in purchasing patterns\n                    """\n                )\n\n            with st.expander("**Streamline Procurement**", expanded=True):\n                st.markdown(\n                    """\n                    - Simplify procurement analysis and boost financial performance\n                    - Visualize product distribution via embedded charts and widgets\n                    - Equip and empower financial managers with intelligence\n                    """\n                )\n\n    def widget_list(self):\n        """List of widgets with action buttons: Edit, Remove."""\n\n        # def add_widget():\n        #     self.state.sidebar = "add-widget"\n\n        # st.button("\\+ Add widget", use_container_width=True, on_click=add_widget)\n\n        widgets = repository().widgets\n        kpi_widgets = [widget for widget in widgets if widget.widget_kind == "kpi"]\n        chart_widgets = [widget for widget in widgets if widget.widget_kind == "chart"]\n\n        with st.expander("**KPIs**"):\n            for index, widget in enumerate(kpi_widgets):\n                self.widget_list_item(index, widget)\n\n        with st.expander("**Charts**"):\n            for index, widget in enumerate(chart_widgets):\n                self.widget_list_item(index + len(kpi_widgets), widget)\n\n    def widget_list_item(self, index: int, widget: Widget) -> None:\n        is_selected = self.state.selected_widget is widget\n\n        def edit_or_unselect_widget():\n            if is_selected:\n                self.state.sidebar = "list-widgets"\n                self.state.selected_widget = None\n            else:\n                self.state.sidebar = "edit-widget"\n                self.state.selected_widget = widget\n\n        title = widget.displayed_title()\n\n        cols = st.columns([4, 1])\n        cols[0].markdown(f"**:blue[{title}]**" if is_selected else title)\n        cols[1].button(\n            "\xe2\x9c\x8e",\n            key=f"edit-{index}",\n            use_container_width=True,\n            on_click=edit_or_unselect_widget,\n            type="primary" if is_selected else "secondary",\n        )\n\n\ndef widget_chart(widget: Widget, is_selected: bool) -> None:\n    """Render the chart of the given widget."""\n    st.plotly_chart(\n        widget.figure().update_layout(SELECTED_STYLE if is_selected else {}),\n        config=PLOTLY_CONFIG,\n        use_container_width=True,\n    )\n\n\ndef widget_kpi(kpi: Widget, is_selected: bool) -> None:\n    """Render the indicator of the given KPI widget."""\n    st.plotly_chart(\n        kpi.figure().update_layout(INDICATOR_SELECTED_STYLE if is_selected else {}),\n        config=PLOTLY_CONFIG,\n        use_container_width=True,\n    )\n')
    __stickytape_write_module('erp_sales_assessment/app_pages/learn_page.py', b'import streamlit as st\n\nfrom erp_sales_assessment.app_pages.page import Page\n\n\nclass LearnPage(Page):\n    def run(self) -> None:\n        st.markdown(\n            """\n            ##### Maxa\'s ERP Sales Data Assessment\n\n            - Introduction to Maxa\'s automated ERP data analytics solutions.\n            - First step in a company\'s journey to unlocking the full potential of its ERP data.\n            - Rapid assessment of ERP sales and customer insights directly from the Snowflake Data Cloud.\n\n            ##### Modes\n\n            1. :blue[**Demo Mode**] (default): Maxa comes preloaded with a dataset and pre-generated widgets for immediate \n               data exploration and analysis.  \n               Dive into the :blue[Dashboard] tab to visualize the dataset and experience Maxa\'s capabilities.\n               Ideal for quick analysis and familiarizing yourself with the platform.\n            2. :blue[**Custom Mode**]: Map your own tables for a personalized analysis environment and gain deeper \n               insights on your data. \n\n            Switching between modes:\n\n            - Go to the :blue[Configuration] tab to map your own tables or reset to the :blue[Demo Mode]\n\n            ##### Workflow\n\n            1. Map your ERP data to create a personalized analysis environment or use the preloaded Maxa dataset.\n            2. Navigate to the :blue[Dashboard] tab to start visualizing and analyzing your data.\n            3. Customize and configure widgets to display relevant information for your analysis.\n                - *By default, the timeframe selected will be the last period seen on the dataset.*\n            4. Gain insights into ERP sales and customer data: identify main trends and detect outliers.\n            5. Repeat the process to update or switch datasets as needed for ongoing analysis.\n\n            To uninstall the app, follow these steps:\n\n            1. Go to Snowsight on the :blue[Apps] section. \n            2. Locate :blue[Maxa\'s ERP Sales Data Assessment] app.\n            3. Click on the three dots on the right (:blue[\xe2\x8b\xaf]), and select :blue[Uninstall].\n            4. Follow the prompts to complete the uninstallation.\n            """\n        )\n')
    __stickytape_write_module('erp_sales_assessment/app_pages/support_page.py', b'import streamlit as st\n\nfrom erp_sales_assessment.app_pages.page import Page\n\n\nclass SupportPage(Page):\n    def run(self) -> None:\n        st.markdown(\n            """\n            - Explore the :blue[Learn] section for best practices on how to use the Maxa app.\n            - Need assistance or have questions? Our dedicated support team is here to help.\n                - Visit [support.maxa.ai](https://support.maxa.ai) and fill out the support form.\n                - We appreciate your patience as we work to assist you!\n            """\n        )\n')
    import numpy as np
    import streamlit as st
    
    # noinspection PyUnresolvedReferences
    import erp_sales_assessment.external.streamlit_nested_layout  # import for nested columns support
    from erp_sales_assessment.app_pages.about_page import AboutPage
    from erp_sales_assessment.app_pages.configuration.configuration_page import ConfigurationPage
    from erp_sales_assessment.app_pages.dashboard_page import DashboardPage
    from erp_sales_assessment.app_pages.learn_page import LearnPage
    from erp_sales_assessment.app_pages.support_page import SupportPage
    from erp_sales_assessment.components.action_button import action_button
    from erp_sales_assessment.components.streamlit_in_snowflake import snowflake_ui_styles, st_image_base64
    from erp_sales_assessment.data.helpers import (
        has_current_repository_changed,
        load_and_select_repository,
        save_custom_repository,
    )
    
    np.random.seed(seed=42)
    st.set_page_config(layout="wide", initial_sidebar_state="expanded")
    
    snowflake_ui_styles()
    
    load_and_select_repository()
    
    st_image_base64("bar.png", use_column_width=True)
    
    title_col, save_button_col = st.columns([10, 2])
    with title_col:
        st_image_base64("banner.png", width=400)
    with save_button_col:
        if st.session_state["selected_repository"] == "Custom Mode" and st.session_state.repository.is_valid():
            action_button(
                "Save Dashboard",
                message="Saved.",
                key="save-dashboard",
                disabled=not has_current_repository_changed(include={"widgets"}),
                on_click=lambda: save_custom_repository(include={"widgets"}),
            )
    
    
    dashboard, config, learn, support, about = st.tabs(["Dashboard", "Configuration", "Learn", "Support", "About Maxa"])
    
    # Execute the slowest tabs at the end
    with learn:
        LearnPage().run()
    with support:
        SupportPage().run()
    with about:
        AboutPage().run()
    with config:
        ConfigurationPage().run()
    with dashboard:
        DashboardPage().run()
    