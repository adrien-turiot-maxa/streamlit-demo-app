#!/usr/bin/env python
import contextlib as __stickytape_contextlib

@__stickytape_contextlib.contextmanager
def __stickytape_temporary_dir():
    import tempfile
    import shutil
    dir_path = tempfile.mkdtemp()
    try:
        yield dir_path
    finally:
        shutil.rmtree(dir_path)

with __stickytape_temporary_dir() as __stickytape_working_dir:
    def __stickytape_write_module(path, contents):
        import os, os.path

        def make_package(path):
            parts = path.split("/")
            partial_path = __stickytape_working_dir
            for part in parts:
                partial_path = os.path.join(partial_path, part)
                if not os.path.exists(partial_path):
                    os.mkdir(partial_path)
                    with open(os.path.join(partial_path, "__init__.py"), "wb") as f:
                        f.write(b"\n")

        make_package(os.path.dirname(path))

        full_path = os.path.join(__stickytape_working_dir, path)
        with open(full_path, "wb") as module_file:
            module_file.write(contents)

    import sys as __stickytape_sys
    __stickytape_sys.path.insert(0, __stickytape_working_dir)

    __stickytape_write_module('range_analysis_app/__init__.py', b'')
    __stickytape_write_module('range_analysis_app/components/__init__.py', b'')
    __stickytape_write_module('range_analysis_app/components/streamlit_nested_layout.py', b'# Implementation from streamlit-nested-layout => https://github.com/joy13975/streamlit-nested-layout\n# This package is not available in the Snowflake Anaconda Channel => https://repo.anaconda.com/pkgs/snowflake/\n\nfrom streamlit.delta_generator import *\nfrom streamlit.delta_generator import _enqueue_message\n\n\ndef _nestable_block(\n    self: "DeltaGenerator",\n    block_proto: Block_pb2.Block = Block_pb2.Block(),\n) -> "DeltaGenerator":\n    # Operate on the active DeltaGenerator, in case we\'re in a `with` block.\n    dg = self._active_dg\n\n    # Prevent nested columns & expanders by checking all parents.\n    block_type = block_proto.WhichOneof("type")\n    # Convert the generator to a list, so we can use it multiple times.\n    # parent_block_types = frozenset(dg._parent_block_types)\n    # if block_type == "column" and block_type in parent_block_types:\n    #     raise StreamlitAPIException(\n    #         "Columns may not be nested inside other columns."\n    #     )\n    # if block_type == "expandable" and block_type in parent_block_types:\n    #     raise StreamlitAPIException(\n    #         "Expanders may not be nested inside other expanders."\n    #     )\n\n    if dg._root_container is None or dg._cursor is None:\n        return dg\n\n    msg = ForwardMsg_pb2.ForwardMsg()\n    msg.metadata.delta_path[:] = dg._cursor.delta_path\n    msg.delta.add_block.CopyFrom(block_proto)\n\n    # Normally we\'d return a new DeltaGenerator that uses the locked cursor\n    # below. But in this case we want to return a DeltaGenerator that uses\n    # a brand new cursor for this new block we\'re creating.\n    block_cursor = cursor.RunningCursor(\n        root_container=dg._root_container,\n        parent_path=dg._cursor.parent_path + (dg._cursor.index,),\n    )\n    block_dg = DeltaGenerator(\n        root_container=dg._root_container,\n        cursor=block_cursor,\n        parent=dg,\n        block_type=block_type,\n    )\n    # Blocks inherit their parent form ids.\n    # NOTE: Container form ids aren\'t set in proto.\n    block_dg._form_data = FormData(current_form_id(dg))\n\n    # Must be called to increment this cursor\'s index.\n    dg._cursor.get_locked_cursor(last_index=None)\n    _enqueue_message(msg)\n\n    caching.save_block_message(\n        block_proto,\n        invoked_dg_id=self.id,\n        used_dg_id=dg.id,\n        returned_dg_id=block_dg.id,\n    )\n\n    return block_dg\n\n\nDeltaGenerator._block = _nestable_block\n')
    __stickytape_write_module('app_pages/__init__.py', b'')
    __stickytape_write_module('app_pages/about_page.py', b'import streamlit as st\n\nfrom app_pages.page import Page\n\n\nclass AboutPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "About Maxa"\n\n    def run(self) -> None:\n        st.markdown(\n            """\n            # Maxa\n            \n            https://www.maxa.ai\n            """\n        )\n')
    __stickytape_write_module('app_pages/page.py', b'import abc\n\n\nclass Page(abc.ABC):\n    @property\n    @abc.abstractmethod\n    def page_description(self) -> str:\n        pass\n\n    @abc.abstractmethod\n    def run(self) -> None:\n        pass\n')
    __stickytape_write_module('app_pages/dashboard.py', b'from typing import Literal, Optional, Type\n\nimport streamlit as st\nfrom pydantic import BaseModel\n\nfrom app_pages.page import Page\nfrom data.helpers import repository\nfrom range_analysis_app.exploration.plotly_styles import INDICATOR_SELECTED_STYLE, PLOTLY_CONFIG, SELECTED_STYLE\nfrom utils import groupby\nfrom widgets.widget import Widget\nfrom widgets.widget_gallery import widget_gallery\n\n\nclass DashboardState(BaseModel):\n    """State of the Dashboard:\n    - list-widgets: Show the list of current dashboard widgets in the sidebar\n    - add-widget: Show the gallery of available widgets in the sidebar\n    - edit-widget: Highlight a selected widget and edit its parameters in the sidebar\n    """\n\n    sidebar: Literal["list-widgets", "add-widget", "edit-widget"] = "list-widgets"\n    selected_widget: Optional[Widget]\n\n\nclass DashboardPage(Page):\n    def __init__(self) -> None:\n        if "dashboard_state" not in st.session_state:\n            st.session_state["dashboard_state"] = DashboardState()\n\n        self.state: DashboardState = st.session_state["dashboard_state"]\n\n    @property\n    def page_description(self) -> str:\n        return "Dashboard of saved visualizations."\n\n    def run(self) -> None:\n        self.render_widgets()\n\n        with st.sidebar:\n            if self.state.sidebar == "list-widgets":\n                self.widget_list()\n                return\n\n            if self.state.sidebar == "add-widget":\n                self.sidebar_title("Add widget")\n\n                def add_and_edit_widget(widget_class: Type[Widget]):\n                    repository().widgets.append(widget_class.default())\n\n                widget_gallery(on_add=add_and_edit_widget)\n                return\n\n            if self.state.sidebar == "edit-widget":\n                self.sidebar_title("Edit widget")\n\n                self.state.selected_widget.edit_parameters()\n                return\n\n    def sidebar_title(self, title: str):\n        def go_back_to_widget_list():\n            self.state.sidebar = "list-widgets"\n            self.state.selected_widget = None\n\n        cols = st.columns([4, 6])\n        cols[0].button("Back", on_click=go_back_to_widget_list)\n        cols[1].markdown(f"### {title}")\n\n    def render_widgets(self) -> None:\n        """Charts of all the dashboard widgets aligned in two columns."""\n        widgets = repository().widgets\n\n        if not widgets:\n            st.warning("<= Add visualizations to the dashboard in the sidebar.")\n            return\n\n        kpi_widgets = [widget for widget in widgets if widget.widget_kind == "kpi"]\n        chart_widgets = groupby([widget for widget in widgets if widget.widget_kind == "chart"], key=lambda w: w.size)\n\n        # Top row: Text + 4 KPIs\n        text_col, *kpi_cols = st.columns([4] + [2] * 4)\n        text_col.markdown(\n            """\n            Optimize purchasing decisions with real-time insights. Identify trends, anomalies \n            and top product categories to drive cost savings and improve financial performance.\n            """\n        )\n        for col, kpi in zip(kpi_cols, kpi_widgets):\n            with col:\n                widget_kpi(kpi, is_selected=kpi is self.state.selected_widget)\n\n        # Middle row: 2 small charts + 2 medium charts\n        cols = st.columns([2, 2, 4, 4])\n        for widget, col in zip(chart_widgets["small"], cols[0:2]):\n            with col:\n                widget_chart(widget, is_selected=widget is self.state.selected_widget)\n        for widget, col in zip(chart_widgets["medium"], cols[2:4]):\n            with col:\n                widget_chart(widget, is_selected=widget is self.state.selected_widget)\n\n        # Bottom row: 1 large chart + 1 text\n        left, right = st.columns([6, 6])\n        if chart_widgets["large"] and (widget := chart_widgets["large"].pop()):\n            with left:\n                widget_chart(widget, is_selected=widget is self.state.selected_widget)\n\n        with right:\n            with st.expander("See explanation"):\n                st.markdown(\n                    """\n                    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore \n                    et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut \n                    aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse\n                    cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in \n                    culpa qui officia deserunt mollit anim id est.\n                    """\n                )\n\n            with st.expander("See explanation"):\n                st.markdown(\n                    """\n                    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore \n                    et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut \n                    aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse\n                    cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in \n                    culpa qui officia deserunt mollit anim id est.\n                    """\n                )\n\n    def widget_list(self):\n        """List of widgets with action buttons: Edit, Remove."""\n\n        def add_widget():\n            self.state.sidebar = "add-widget"\n\n        st.button("\\+ Add widget", use_container_width=True, on_click=add_widget)\n\n        for index, widget in enumerate(repository().widgets):\n            self.widget_list_item(index, widget)\n\n    def widget_list_item(self, index: int, widget: Widget) -> None:\n        def edit_widget():\n            self.state.sidebar = "edit-widget"\n            self.state.selected_widget = widget\n\n        def remove_widget():\n            repository().widgets.pop(index)\n\n        cols = st.columns([4, 1, 1])\n        cols[0].markdown(f"**- {widget.displayed_title()}**")\n        cols[1].button("\xe2\x9c\x8e", key=f"edit-{index}", use_container_width=True, on_click=edit_widget)\n        cols[2].button("\xf0\x9f\x97\x91\xef\xb8\x8f", key=f"remove-{index}", use_container_width=True, on_click=remove_widget)\n\n\ndef widget_chart(widget: Widget, is_selected: bool) -> None:\n    """Render the chart of the given widget."""\n    st.plotly_chart(\n        widget.figure().update_layout(SELECTED_STYLE if is_selected else {}),\n        config=PLOTLY_CONFIG,\n        use_container_width=True,\n    )\n\n\ndef widget_kpi(kpi: Widget, is_selected: bool) -> None:\n    """Render the indicator of the given KPI widget."""\n    st.plotly_chart(\n        kpi.figure().update_layout(INDICATOR_SELECTED_STYLE if is_selected else {}),\n        config=PLOTLY_CONFIG,\n        use_container_width=True,\n    )\n')
    __stickytape_write_module('data/__init__.py', b'')
    __stickytape_write_module('data/helpers.py', b'from typing import TYPE_CHECKING, List, Optional, Tuple\n\nimport streamlit as st\nfrom inflection import pluralize as inflection_pluralize\nfrom snowflake.snowpark.exceptions import SnowparkSQLException\nfrom snowflake.snowpark.types import StringType, StructField, StructType\n\nfrom range_analysis_app.db import snowflake_session\nfrom utils import pluralize\n\nif TYPE_CHECKING:\n    from data.models import DomaBaseModel\n    from data.repository import Repository\n\n\ndef create_table_if_not_exists() -> None:\n    session = snowflake_session()\n    try:\n        session.table("doma_entities").collect()\n    except SnowparkSQLException as e:\n        schema = StructType([StructField("json_data", StringType())])\n        df = session.create_dataframe([], schema=schema)\n        df.write.save_as_table("doma_entities", mode="overwrite")\n\n\ndef repository() -> "Repository":\n    if "repository" not in st.session_state:\n        from data.repository import Repository\n\n        create_table_if_not_exists()\n        row = snowflake_session().table("doma_entities").first()\n        if row is None:\n            st.session_state.repository = Repository()\n        else:\n            data = row.JSON_DATA\n            st.session_state["prev_repository_json"] = data\n            st.session_state.repository = Repository.parse_raw(data)\n\n    return st.session_state.repository\n\n\ndef save_repo_button() -> None:\n    """Component for saving the repository and going to the next page."""\n    raw_json = repository().json()\n    prev_raw_json = st.session_state.get("prev_repository_json")\n\n    if prev_raw_json == raw_json:\n        return\n\n    placeholder = st.empty()\n\n    btn_container = placeholder.container()\n    btn_container.markdown("")\n\n    if btn_container.button("Save Dashboard", type="primary", use_container_width=True):\n        placeholder.empty()\n        with st.spinner():\n            schema = StructType([StructField("json_data", StringType())])\n            (\n                snowflake_session()\n                .create_dataframe([raw_json], schema=schema)\n                .write.save_as_table("doma_entities", mode="overwrite")\n            )\n\n        st.session_state["prev_repository_json"] = raw_json\n        placeholder.info("Saved.", icon="\xe2\x9c\x94\xef\xb8\x8f")\n\n\ndef validate_state(\n    models: List["DomaBaseModel"],\n    entity_name: str,\n    permit_empty: bool = False,\n    validate_each: bool = True,\n) -> Tuple[bool, str]:\n    """Return a pair (is_valid, message) describing the validation state of the given models."""\n\n    if len(models) == 0 and not permit_empty:\n        return False, f"{entity_name} cannot be empty"\n\n    if validate_each:\n        try:\n            success_message = [f"{pluralize(len(models), entity_name)} has been set"]\n            for model in models:\n                model.validate_entity()\n                success_message.append(f"- {model.validation_item_name()}")\n\n            return True, "\\n".join(success_message)\n        except ValueError as e:\n            return False, str(e)\n\n    success_message = f"All set for the {inflection_pluralize(entity_name)}"\n    return True, "\\n".join([success_message] + [f"- {x.validation_item_name()}" for x in models])\n\n\ndef render_validation_component(is_valid: bool, message: str):\n    if is_valid:\n        st.success(message, icon="\xe2\x9c\x94\xef\xb8\x8f")\n    else:\n        st.error(message, icon="\xe2\x9c\x96\xef\xb8\x8f")\n')
    __stickytape_write_module('range_analysis_app/db.py', b'import json\nfrom typing import Dict, Iterable, List, Literal, TypedDict, Union, get_args\n\nimport pandas as pd\nimport snowflake.snowpark.functions as f\nimport streamlit as st\nfrom pydantic import Field\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.exceptions import SnowparkSessionException\nfrom typing_extensions import NotRequired\n\nfrom utils import BaseModelCacheKey\n\n\n@st.cache_resource\ndef snowflake_session() -> Session:\n    try:\n        return get_active_session()\n    except SnowparkSessionException:\n        return Session.builder.configs(st.secrets["snowflake"]).create()\n\n\n@st.cache_data\ndef fetch_tables(schema: str = "") -> List[str]:\n    return [row.name for row in snowflake_session().sql(f"show tables in schema {schema}").collect()]\n\n\nDatetimeType = Literal["DATE", "TIME", "TIMESTAMP", "TIMESTAMP_LTZ", "TIMESTAMP_NTZ"]\nDataType = Union[Literal["TEXT", "FIXED", "REAL", "BINARY", "BOOLEAN", "ARRAY", "OBJECT"], DatetimeType]\nExtendedDataType = Union[DataType, Literal["INTEGER"]]\n\nDATETIME_TYPES: Iterable[DatetimeType] = get_args(DatetimeType)\nID_TYPES: Iterable[ExtendedDataType] = ["TEXT", "INTEGER", "BINARY", "ARRAY"]\n\n\n@st.cache_data\ndef fetch_columns(table: str) -> Dict[str, DataType]:\n    return {\n        row.column_name: parse_data_type(json.loads(row.data_type))\n        for row in snowflake_session().sql(f"show columns in table {table}").collect()\n    }\n\n\nclass DataTypeDict(TypedDict):\n    """Represent the `data_type` value in Snowflake `SHOW COLUMNS IN TABLE`."""\n\n    type: DataType\n    nullable: bool\n\n    # for type FIXED and TIME/TIMESTAMP\n    precision: NotRequired[int]\n    scale: NotRequired[int]\n\n    # for type TEXT and BINARY\n    length: NotRequired[int]\n    bytesLength: NotRequired[int]\n    fixed: NotRequired[bool]\n\n\ndef parse_data_type(data_type: DataTypeDict) -> ExtendedDataType:\n    if data_type["type"] == "FIXED" and data_type["scale"] == 0:\n        return "INTEGER"\n\n    return data_type["type"]\n\n\n@st.cache_data\ndef preview_table(table: str, limit: int = 5) -> pd.DataFrame:\n    return pd.DataFrame(snowflake_session().table(table).limit(limit).collect())\n\n\nclass Filters(BaseModelCacheKey):\n    mapping: Dict[str, List[str]] = Field(default_factory=dict)\n\n    def add(self, key: str, values: List[str]) -> None:\n        self.mapping[key] = values\n\n    def to_condition(self) -> f.Column:\n        """Return a Snowpark filter condition to be used in the `DataFrame.filter` or `DataFrame.where` methods."""\n        condition = f.lit(True)\n        for column, values in self.mapping.items():\n            condition &= f.col(column).in_(values)\n        return condition\n\n    def is_empty(self) -> bool:\n        return len(self.mapping) == 0\n\n\ndef drop_temporary_tables() -> None:\n    for table in fetch_tables.execute_without_cache():\n        if table.startswith("TMP_"):\n            snowflake_session().table(table).drop_table()\n\n    fetch_tables.clear_cache()\n')
    __stickytape_write_module('utils.py', b'from collections import defaultdict\nfrom typing import Callable, Dict, Iterable, List, Optional, Tuple, Type, TypeVar\n\nimport streamlit as st\nfrom inflection import pluralize as inflection_pluralize\nfrom pydantic import BaseModel\n\nT = TypeVar("T")\nK = TypeVar("K")\n\n\ndef pluralize(count: int, singular: str) -> str:\n    return f"{count} {inflection_pluralize(singular) if count > 1 else singular}"\n\n\ndef chunks(values: List[T], chunk_size: int) -> Iterable[List[T]]:\n    """Split the given values into multiple chunks of the given size."""\n    for index in range(0, len(values), chunk_size):\n        yield values[index : index + chunk_size]\n\n\nclass BaseModelCacheKey(BaseModel):\n    """Model that can be used as caching key in st.cache_data."""\n\n    def __reduce__(self) -> Tuple[Type, Tuple[str, ...]]:\n        """Function from pickle used by the hasher of st.cache_data."""\n        return (self.parse_raw, (self.json(),))\n\n\ndef groupby(values: List[T], key: Callable[[T], K]) -> Dict[K, List[T]]:\n    result = defaultdict(list)\n\n    for value in values:\n        result[key(value)].append(value)\n\n    return result\n')
    __stickytape_write_module('data/models.py', b'from abc import ABC, abstractmethod\nfrom typing import TYPE_CHECKING, Dict, List, Literal, NewType, Optional, Union\nfrom uuid import uuid4\n\nfrom pydantic import Field\n\nfrom data.helpers import repository\nfrom range_analysis_app import db\nfrom range_analysis_app.db import snowflake_session\nfrom utils import BaseModelCacheKey\n\nif TYPE_CHECKING:\n    from data.metric import Metric\n\n\nId = NewType("Id", str)\n\nEntityType = Literal["dimension", "activity", "metric"]\n\n\ndef get_uuid4() -> Id:\n    return str(uuid4())\n\n\nclass DomaBaseModel(BaseModelCacheKey, ABC):\n    @abstractmethod\n    def validate_entity(self) -> None:\n        """Raise ValueError if the current entity is invalid."""\n\n    @abstractmethod\n    def validation_item_name(self) -> str:\n        """Name of the item to be validated"""\n\n\nclass Entity(DomaBaseModel, ABC):\n    uid: Id = Field(default_factory=get_uuid4)\n    name: str\n\n    @classmethod\n    @abstractmethod\n    def get_type(cls) -> EntityType:\n        ...\n\n    @abstractmethod\n    def delete(self) -> None:\n        ...\n\n    def validate_entity(self) -> None:\n        if not bool(self.name):\n            raise ValueError(f"An entity name must be set.")\n\n    def validation_item_name(self) -> str:\n        return self.name\n\n\nclass DimensionActivityMapping(DomaBaseModel):\n    dim_id: Id\n    act_id: Id\n\n    main_table_id: Optional[Id]\n    joined_table_id: Optional[Id]\n\n    main_column: Optional[str]\n    joined_column: Optional[str]\n\n    @property\n    def dimension(self) -> Optional["Dimension"]:\n        return repository().get_entity("dimension", self.dim_id)\n\n    @property\n    def activity(self) -> Optional["Activity"]:\n        return repository().get_entity("activity", self.act_id)\n\n    @property\n    def main_table(self) -> Optional["DbTable"]:\n        return self._find_table(self.main_table_id)\n\n    @property\n    def joined_table(self) -> Optional["DbTable"]:\n        return self._find_table(self.joined_table_id)\n\n    @staticmethod\n    def _find_table(uid: Id) -> Optional["DbTable"]:\n        for table in repository().selected_tables:\n            if uid == table.uid:\n                return table\n\n    def validate_entity(self) -> None:\n        if not bool(self.main_table_id) or not bool(self.main_column):\n            raise ValueError(f"A main table must be set.")\n\n        if not bool(self.joined_table_id) or not bool(self.joined_column):\n            raise ValueError(f"A joined table must be set.")\n\n    def validation_item_name(self) -> str:\n        return f"{self.dimension.name} <=> {self.activity.name}"\n\n\nclass Dimension(Entity):\n    @classmethod\n    def get_type(cls) -> EntityType:\n        return "dimension"\n\n    @staticmethod\n    def create(name: str) -> "Dimension":\n        dim = Dimension(name=name)\n        repository().dimensions.append(dim)\n        return dim\n\n    def delete(self) -> None:\n        repo = repository()\n        for activity in self.activities:\n            activity.unassign_dimension(self)\n\n        if table := self.db_table:\n            table.delete()\n\n        repo.dimensions.remove(self)\n\n    @property\n    def activities(self) -> List["Activity"]:\n        act_ids = [mapping.act_id for mapping in repository().dim_act_mappings if mapping.dim_id == self.uid]\n        return [act for act in repository().activities if act.uid in act_ids]\n\n    @property\n    def db_table(self) -> Optional["DbTable"]:\n        for table in repository().selected_tables:\n            if table.entity_uid == self.uid and table.entity_type == "dimension":\n                return table\n\n    def validate_entity(self) -> None:\n        super().validate_entity()\n\n        if self.db_table is None:\n            raise ValueError(f"Missing table for dimension {self.name}")\n\n\nclass Activity(Entity):\n    @classmethod\n    def get_type(cls) -> EntityType:\n        return "activity"\n\n    @staticmethod\n    def create(name: str) -> "Activity":\n        act = Activity(name=name)\n        repository().activities.append(act)\n        return act\n\n    def delete(self) -> None:\n        for dimension in self.dimensions:\n            self.unassign_dimension(dimension)\n\n        if table := self.db_table:\n            table.delete()\n\n        for metric in self.metrics:\n            metric.delete()\n\n        repository().activities.remove(self)\n\n    @property\n    def dimensions(self) -> List[Dimension]:\n        dim_ids = [mapping.dim_id for mapping in repository().dim_act_mappings if mapping.act_id == self.uid]\n        return [dim for dim in repository().dimensions if dim.uid in dim_ids]\n\n    def assign_dimension(self, dim_id: Id) -> None:\n        mapping = DimensionActivityMapping(act_id=self.uid, dim_id=dim_id)\n        repository().dim_act_mappings.append(mapping)\n\n    def unassign_dimension(self, dim: Dimension) -> None:\n        repository().dim_act_mappings = [\n            mapping\n            for mapping in repository().dim_act_mappings\n            if mapping.act_id != self.uid or mapping.dim_id != dim.uid\n        ]\n        for metric in self.metrics:\n            metric.unassign_dimension(dim)\n\n    @property\n    def metrics(self) -> List["Metric"]:\n        return [metric for metric in repository().metrics if metric.act_id == self.uid]\n\n    @property\n    def mappings(self) -> List[DimensionActivityMapping]:\n        return [mapping for mapping in repository().dim_act_mappings if mapping.act_id == self.uid]\n\n    @property\n    def db_table(self) -> Optional["DbTable"]:\n        for table in repository().selected_tables:\n            if table.entity_uid == self.uid and table.entity_type == "activity":\n                return table\n\n    def validate_tables_and_mapping(self) -> List[str]:\n        """Return a list of error messages for each missing tables or mappings in the activity."""\n        errors = []\n        if self.db_table is None:\n            errors.append(f"Missing table for activity {self.name}")\n\n        for dim in self.dimensions:\n            if dim.db_table is None:\n                errors.append(f"Missing table for dimension {dim.name}")\n\n        for mapping in self.mappings:\n            try:\n                mapping.validate_entity()\n            except ValueError:\n                errors.append(f"Missing mapping: {mapping.validation_item_name()}")\n\n        return errors\n\n\nclass DbTable(DomaBaseModel):\n    uid: Id = Field(default_factory=get_uuid4)\n\n    table_name: str\n    table_schema: str\n    table_database: str\n\n    entity_uid: Id\n    entity_type: EntityType\n\n    columns: Dict[str, str] = {}\n\n    @staticmethod\n    def create(\n        table_name: str,\n        table_schema: str,\n        table_database: str,\n        columns: Dict[str, str],\n        entity: Entity,\n    ) -> "DbTable":\n        entity_uid = entity.uid\n        entity_type = entity.get_type()\n        tbl = DbTable(\n            table_name=table_name,\n            table_schema=table_schema,\n            table_database=table_database,\n            columns=columns,\n            entity_uid=entity_uid,\n            entity_type=entity_type,\n        )\n        repository().selected_tables.append(tbl)\n        return tbl\n\n    @property\n    def full_name(self) -> str:\n        return f"{self.table_database}.{self.table_schema}.{self.table_name}"\n\n    @property\n    def entity(self) -> Union[Activity, Dimension]:\n        return repository().get_entity(self.entity_type, self.entity_uid)\n\n    def delete(self) -> None:\n        repository().selected_tables.remove(self)\n        repository().dim_act_mappings = [\n            mapping\n            for mapping in repository().dim_act_mappings\n            if self.uid in [mapping.main_table_id, mapping.joined_table_id]\n        ]\n\n        # Remove table columns from metrics\n        entity = self.entity\n\n        if isinstance(entity, Activity):\n            for metric in entity.metrics:\n                metric.columns = None\n\n        if isinstance(entity, Dimension):\n            for activity in entity.activities:\n                for metric in activity.metrics:\n                    metric.unassign_dimension(entity)\n\n    def validate_entity(self) -> None:\n        if snowflake_session().table(self.full_name).first() is None:\n            raise ValueError(f"Table \'{self.full_name}\' is empty or does not exist.")\n\n    def validation_item_name(self) -> str:\n        return self.full_name\n\n    def columns_of_type(self, data_types: List[db.DataType]) -> List[str]:\n        """Return the columns having one of the given types."""\n        return [col for col, data_type in self.columns.items() if data_type in data_types]\n')
    __stickytape_write_module('data/metric.py', b'import datetime\nfrom typing import Dict, List, Literal, Optional, Set, Tuple, get_args\n\nimport pandas as pd\nimport snowflake.snowpark.functions as f\nimport streamlit as st\nfrom pydantic import Field\nfrom snowflake import snowpark\n\nfrom data.helpers import repository\nfrom data.models import Activity, Dimension, DimensionActivityMapping, Entity, EntityType, Id\nfrom range_analysis_app import db\nfrom std_graphlib import TopologicalSorter\nfrom utils import BaseModelCacheKey, groupby\n\nAggMethod = Literal["sum", "count", "avg"]\nAGG_METHODS = get_args(AggMethod)\n\n\nclass MetricColumns(BaseModelCacheKey):\n    dimensions: List[str]\n    metric_column: str\n    timestamp: str\n    agg_method: Literal["sum", "count", "avg"]\n\n    labels: Dict[str, str] = Field(default_factory=dict)\n\n    def remove_dimension_columns(self, dim_columns: Set[str]) -> None:\n        for dim_col in set(self.dimensions) & dim_columns:\n            self.dimensions.remove(dim_col)\n            self.labels.pop(dim_col, None)\n\n\nclass Metric(Entity):\n    act_id: Optional[Id]\n    columns: Optional[MetricColumns]\n\n    @classmethod\n    def get_type(cls) -> EntityType:\n        return "metric"\n\n    def validate_entity(self) -> None:\n        if not bool(self.columns):\n            raise ValueError(f"Metric columns must be set.")\n\n    @staticmethod\n    def create(name: str, act_id: Optional[str] = None) -> "Metric":\n        met = Metric(name=name, act_id=act_id)\n        repository().metrics.append(met)\n        return met\n\n    def delete(self) -> None:\n        repository().metrics.remove(self)\n\n    @property\n    def activity(self) -> Optional[Activity]:\n        return repository().get_entity("activity", self.act_id)\n\n    def unassign_dimension(self, dimension: Dimension):\n        dim_table = dimension.db_table\n        if not self.columns or not dim_table:\n            return\n\n        dim_columns = set(dim_table.columns.keys())\n        self.columns.remove_dimension_columns(dim_columns)\n\n    def dataframe(self) -> snowpark.DataFrame:\n        """Return a dataframe based on the metric activity joined with its dimensions."""\n        if not self.columns:\n            raise ValueError(f"No columns in metric {self.name}.")\n\n        if self.activity.validate_tables_and_mapping():\n            raise ValueError(f"Missing tables and mappings in activity {self.activity.name}.")\n\n        mappings = sorted_mappings(self.activity.mappings)\n\n        snowpark_tables: Dict[str, snowpark.Table] = {}\n        for mapping in mappings:\n            for table in [mapping.main_table, mapping.joined_table]:\n                if table.uid not in snowpark_tables:\n                    snowpark_tables[table.uid] = db.snowflake_session().table(table.full_name)\n\n        act_table = snowpark_tables[self.activity.db_table.uid]\n\n        result_dataframe = act_table\n\n        for mapping in sorted_mappings(self.activity.mappings):\n            main_table = snowpark_tables[mapping.main_table_id]\n            joined_table = snowpark_tables[mapping.joined_table_id]\n\n            result_dataframe = result_dataframe.join(\n                joined_table,\n                how="left",\n                on=main_table.col(mapping.main_column) == joined_table.col(mapping.joined_column),\n            )\n\n        def col_ref(column_name: str) -> snowpark.Column:\n            """Return a reference for the column in one of the given tables eg. my_table.my_column"""\n            for table in snowpark_tables.values():\n                if column_name in table.columns:\n                    return table.col(column_name)\n\n        cols = self.columns\n\n        return result_dataframe.select(\n            [\n                *(col_ref(col).alias(col) for col in cols.dimensions),\n                f.to_date(col_ref(cols.timestamp)).alias(cols.timestamp),\n                col_ref(cols.metric_column).alias(cols.metric_column),\n            ]\n        )\n\n    @st.cache_data\n    def dimension_values(self, dimension: str, limit: int = 1000) -> List[str]:\n        assert dimension in self.columns.dimensions\n\n        rows = (\n            self.dataframe()\n            .group_by(dimension)\n            .agg(f.sum(self.columns.metric_column).alias("total"))\n            .sort(f.col("total").desc_nulls_last())\n            .limit(limit)\n            .collect()\n        )\n        return [row[dimension.upper()] for row in rows if row[dimension.upper()] is not None]\n\n    @st.cache_data\n    def preview_dataframe(self, limit: int = 10) -> pd.DataFrame:\n        data = pd.DataFrame(self.dataframe().limit(limit).collect())\n\n        columns = [self.columns.metric_column] + self.columns.dimensions\n        renames = {col: f"{col} ({label})" for col, label in self.columns.labels.items()}\n        return data.set_index(self.columns.timestamp)[columns].rename(columns=renames)\n\n    @st.cache_data\n    def date_range(self) -> Tuple[datetime.date, datetime.date]:\n        """Return the (min_date, max_date) of the metric."""\n        row = (\n            self.dataframe()\n            .select(f.min(self.columns.timestamp).alias("min_date"), f.max(self.columns.timestamp).alias("max_date"))\n            .first()\n        )\n        return row.MIN_DATE, row.MAX_DATE\n\n\ndef sorted_mappings(mappings: List[DimensionActivityMapping]) -> List[DimensionActivityMapping]:\n    """Return the mappings sorted by the order in which tables must be joined in SQL."""\n    # A graph is a dict of { node => preceding nodes }, so here we have { table to join => tables to load beforehand }\n    tables_graph = {\n        joined_table_id: [mapping.main_table_id for mapping in mappings]\n        for joined_table_id, mappings in groupby(mappings, key=lambda mapping: mapping.joined_table_id).items()\n    }\n    sorted_table_ids = list(TopologicalSorter(tables_graph).static_order())\n\n    def sort_by_main_table(mapping: DimensionActivityMapping) -> int:\n        return sorted_table_ids.index(mapping.main_table_id)\n\n    return sorted(mappings, key=sort_by_main_table)\n')
    __stickytape_write_module('std_graphlib.py', b'# Backport of the `graphlib` library that is available in python 3.9:\n# => https://docs.python.org/3/library/graphlib.html\n#\n# TODO: replace by `graphlib` from std once in python 3.9\n\n\n__all__ = ["TopologicalSorter", "CycleError"]\n\n_NODE_OUT = -1\n_NODE_DONE = -2\n\n\nclass _NodeInfo:\n    __slots__ = "node", "npredecessors", "successors"\n\n    def __init__(self, node):\n        # The node this class is augmenting.\n        self.node = node\n\n        # Number of predecessors, generally >= 0. When this value falls to 0,\n        # and is returned by get_ready(), this is set to _NODE_OUT and when the\n        # node is marked done by a call to done(), set to _NODE_DONE.\n        self.npredecessors = 0\n\n        # List of successor nodes. The list can contain duplicated elements as\n        # long as they\'re all reflected in the successor\'s npredecessors attribute.\n        self.successors = []\n\n\nclass CycleError(ValueError):\n    """Subclass of ValueError raised by TopologicalSorter.prepare if cycles\n    exist in the working graph.\n\n    If multiple cycles exist, only one undefined choice among them will be reported\n    and included in the exception. The detected cycle can be accessed via the second\n    element in the *args* attribute of the exception instance and consists in a list\n    of nodes, such that each node is, in the graph, an immediate predecessor of the\n    next node in the list. In the reported list, the first and the last node will be\n    the same, to make it clear that it is cyclic.\n    """\n\n    pass\n\n\nclass TopologicalSorter:\n    """Provides functionality to topologically sort a graph of hashable nodes"""\n\n    def __init__(self, graph=None):\n        self._node2info = {}\n        self._ready_nodes = None\n        self._npassedout = 0\n        self._nfinished = 0\n\n        if graph is not None:\n            for node, predecessors in graph.items():\n                self.add(node, *predecessors)\n\n    def _get_nodeinfo(self, node):\n        if (result := self._node2info.get(node)) is None:\n            self._node2info[node] = result = _NodeInfo(node)\n        return result\n\n    def add(self, node, *predecessors):\n        """Add a new node and its predecessors to the graph.\n\n        Both the *node* and all elements in *predecessors* must be hashable.\n\n        If called multiple times with the same node argument, the set of dependencies\n        will be the union of all dependencies passed in.\n\n        It is possible to add a node with no dependencies (*predecessors* is not provided)\n        as well as provide a dependency twice. If a node that has not been provided before\n        is included among *predecessors* it will be automatically added to the graph with\n        no predecessors of its own.\n\n        Raises ValueError if called after "prepare".\n        """\n        if self._ready_nodes is not None:\n            raise ValueError("Nodes cannot be added after a call to prepare()")\n\n        # Create the node -> predecessor edges\n        nodeinfo = self._get_nodeinfo(node)\n        nodeinfo.npredecessors += len(predecessors)\n\n        # Create the predecessor -> node edges\n        for pred in predecessors:\n            pred_info = self._get_nodeinfo(pred)\n            pred_info.successors.append(node)\n\n    def prepare(self):\n        """Mark the graph as finished and check for cycles in the graph.\n\n        If any cycle is detected, "CycleError" will be raised, but "get_ready" can\n        still be used to obtain as many nodes as possible until cycles block more\n        progress. After a call to this function, the graph cannot be modified and\n        therefore no more nodes can be added using "add".\n        """\n        if self._ready_nodes is not None:\n            raise ValueError("cannot prepare() more than once")\n\n        self._ready_nodes = [i.node for i in self._node2info.values() if i.npredecessors == 0]\n        # ready_nodes is set before we look for cycles on purpose:\n        # if the user wants to catch the CycleError, that\'s fine,\n        # they can continue using the instance to grab as many\n        # nodes as possible before cycles block more progress\n        cycle = self._find_cycle()\n        if cycle:\n            raise CycleError(f"nodes are in a cycle", cycle)\n\n    def get_ready(self):\n        """Return a tuple of all the nodes that are ready.\n\n        Initially it returns all nodes with no predecessors; once those are marked\n        as processed by calling "done", further calls will return all new nodes that\n        have all their predecessors already processed. Once no more progress can be made,\n        empty tuples are returned.\n\n        Raises ValueError if called without calling "prepare" previously.\n        """\n        if self._ready_nodes is None:\n            raise ValueError("prepare() must be called first")\n\n        # Get the nodes that are ready and mark them\n        result = tuple(self._ready_nodes)\n        n2i = self._node2info\n        for node in result:\n            n2i[node].npredecessors = _NODE_OUT\n\n        # Clean the list of nodes that are ready and update\n        # the counter of nodes that we have returned.\n        self._ready_nodes.clear()\n        self._npassedout += len(result)\n\n        return result\n\n    def is_active(self):\n        """Return ``True`` if more progress can be made and ``False`` otherwise.\n\n        Progress can be made if cycles do not block the resolution and either there\n        are still nodes ready that haven\'t yet been returned by "get_ready" or the\n        number of nodes marked "done" is less than the number that have been returned\n        by "get_ready".\n\n        Raises ValueError if called without calling "prepare" previously.\n        """\n        if self._ready_nodes is None:\n            raise ValueError("prepare() must be called first")\n        return self._nfinished < self._npassedout or bool(self._ready_nodes)\n\n    def __bool__(self):\n        return self.is_active()\n\n    def done(self, *nodes):\n        """Marks a set of nodes returned by "get_ready" as processed.\n\n        This method unblocks any successor of each node in *nodes* for being returned\n        in the future by a call to "get_ready".\n\n        Raises :exec:`ValueError` if any node in *nodes* has already been marked as\n        processed by a previous call to this method, if a node was not added to the\n        graph by using "add" or if called without calling "prepare" previously or if\n        node has not yet been returned by "get_ready".\n        """\n\n        if self._ready_nodes is None:\n            raise ValueError("prepare() must be called first")\n\n        n2i = self._node2info\n\n        for node in nodes:\n            # Check if we know about this node (it was added previously using add()\n            if (nodeinfo := n2i.get(node)) is None:\n                raise ValueError(f"node {node!r} was not added using add()")\n\n            # If the node has not being returned (marked as ready) previously, inform the user.\n            stat = nodeinfo.npredecessors\n            if stat != _NODE_OUT:\n                if stat >= 0:\n                    raise ValueError(f"node {node!r} was not passed out (still not ready)")\n                elif stat == _NODE_DONE:\n                    raise ValueError(f"node {node!r} was already marked done")\n                else:\n                    assert False, f"node {node!r}: unknown status {stat}"\n\n            # Mark the node as processed\n            nodeinfo.npredecessors = _NODE_DONE\n\n            # Go to all the successors and reduce the number of predecessors, collecting all the ones\n            # that are ready to be returned in the next get_ready() call.\n            for successor in nodeinfo.successors:\n                successor_info = n2i[successor]\n                successor_info.npredecessors -= 1\n                if successor_info.npredecessors == 0:\n                    self._ready_nodes.append(successor)\n            self._nfinished += 1\n\n    def _find_cycle(self):\n        n2i = self._node2info\n        stack = []\n        itstack = []\n        seen = set()\n        node2stacki = {}\n\n        for node in n2i:\n            if node in seen:\n                continue\n\n            while True:\n                if node in seen:\n                    # If we have seen already the node and is in the\n                    # current stack we have found a cycle.\n                    if node in node2stacki:\n                        return stack[node2stacki[node] :] + [node]\n                    # else go on to get next successor\n                else:\n                    seen.add(node)\n                    itstack.append(iter(n2i[node].successors).__next__)\n                    node2stacki[node] = len(stack)\n                    stack.append(node)\n\n                # Backtrack to the topmost stack entry with\n                # at least another successor.\n                while stack:\n                    try:\n                        node = itstack[-1]()\n                        break\n                    except StopIteration:\n                        del node2stacki[stack.pop()]\n                        itstack.pop()\n                else:\n                    break\n        return None\n\n    def static_order(self):\n        """Returns an iterable of nodes in a topological order.\n\n        The particular order that is returned may depend on the specific\n        order in which the items were inserted in the graph.\n\n        Using this method does not require to call "prepare" or "done". If any\n        cycle is detected, :exc:`CycleError` will be raised.\n        """\n        self.prepare()\n        while self.is_active():\n            node_group = self.get_ready()\n            yield from node_group\n            self.done(*node_group)\n')
    __stickytape_write_module('data/repository.py', b'from typing import TYPE_CHECKING, Dict, List, Optional, Union\n\nfrom pydantic import Field\nfrom pydantic.main import BaseModel\nfrom typing_extensions import Annotated\n\nfrom data.metric import Metric\nfrom data.models import Activity, DbTable, Dimension, DimensionActivityMapping, Entity, EntityType, Id\nfrom range_analysis_app.exploration.range_comparison_exploration import RangeComparisonVisualization\nfrom range_analysis_app.exploration.time_series_exploration import TimeSeriesVisualization\nfrom widgets.kpis import TotalPurchaseValue\nfrom widgets.outlier_detection_widgets import PurchaseAnomaliesHeatmap\nfrom widgets.product_category_distribution_widgets import (\n    ProductCategoriesComparisonBarGraph,\n    ProductCategoryDistributionPieChart,\n)\nfrom widgets.purchasing_trend_over_time_widgets import PurchasingTrendsLineChart\n\nENTITY_TYPE_TO_COLLECTION: Dict[EntityType, str] = {\n    "dimension": "dimensions",\n    "activity": "activities",\n    "metric": "metrics",\n}\n\nVisualization = Annotated[\n    Union[TimeSeriesVisualization, RangeComparisonVisualization],\n    Field(..., discriminator="viz_type"),\n]\n\nWidgetType = Annotated[\n    Union[\n        ProductCategoryDistributionPieChart,\n        ProductCategoriesComparisonBarGraph,\n        PurchaseAnomaliesHeatmap,\n        PurchasingTrendsLineChart,\n        TotalPurchaseValue,\n    ],\n    Field(..., discriminator="widget_type"),\n]\n\n\nclass Repository(BaseModel):\n    dimensions: List[Dimension] = []\n    activities: List[Activity] = []\n    metrics: List[Metric] = []\n    selected_tables: List[DbTable] = []\n    dim_act_mappings: List[DimensionActivityMapping] = []\n    visualizations: Dict[int, Visualization] = {}\n    widgets: List[WidgetType] = []\n\n    def get_entity(self, entity_type: EntityType, uid: Id) -> Optional[Entity]:\n        collection_name = ENTITY_TYPE_TO_COLLECTION.get(entity_type)\n        if collection_name is None:\n            return None\n\n        for e in getattr(self, collection_name):\n            if e.uid == uid:\n                return e\n        return None\n')
    __stickytape_write_module('range_analysis_app/exploration/__init__.py', b'')
    __stickytape_write_module('range_analysis_app/exploration/range_comparison_exploration.py', b'import calendar\nfrom ast import Dict\nfrom typing import Literal\n\nimport pandas as pd\nimport plotly.graph_objects as go\nimport snowflake.snowpark.functions as f\nimport streamlit as st\nfrom streamlit.delta_generator import DeltaGenerator\n\nfrom data.metric import Metric\nfrom range_analysis_app.definitions.period import Period\nfrom range_analysis_app.definitions.range_comparison import RANGE_PERIODICITIES, RangeComparison, RangePeriodicity\nfrom range_analysis_app.exploration.plotly_styles import (\n    BOXPLOT_STYLE,\n    COLORS,\n    PLOT_LAYOUT,\n    PLOTLY_CONFIG,\n    SCATTER_STYLE,\n)\nfrom range_analysis_app.exploration.time_series_exploration import multiselect_dimension_filters\nfrom range_analysis_app.exploration.visualization import VisualizationModel\n\n\ndef range_comparison_exploration(metric: Metric, content_col: DeltaGenerator, side_col: DeltaGenerator) -> None:\n    """Component to create a range comparison and visualize it."""\n    with side_col:\n        comparison = generate_range_comparison(metric)\n\n        viz = RangeComparisonVisualization(comparison=comparison)\n        viz.button_add_to_dashboard()\n\n    with content_col:\n        viz.visualize()\n\n\ndef generate_range_comparison(metric: Metric) -> RangeComparison:\n    range_periodicity = st.selectbox("Range Periodicity", RANGE_PERIODICITIES, index=RANGE_PERIODICITIES.index("month"))\n\n    dates = fetch_dates(metric)\n    last_year = pd.Period(dates.end.year, freq="Y") - 1\n\n    current_period_date = st.date_input(\n        "Current Period Date",\n        value=dates.end,\n        min_value=dates.start,\n        max_value=dates.end,\n    )\n    historical_start, historical_end = st.date_input(\n        "Historical Dates",\n        value=(last_year.start_time.date(), last_year.end_time.date()),\n        min_value=dates.start,\n        max_value=dates.end,\n    )\n\n    filters = multiselect_dimension_filters(metric)\n\n    return RangeComparison(\n        metric=metric,\n        range_periodicity=range_periodicity,\n        historical_dates=Period(start=pd.Timestamp(historical_start), end=pd.Timestamp(historical_end)),\n        current_period_date=pd.Timestamp(current_period_date),\n        filters=filters,\n    )\n\n\n@st.cache_data\ndef fetch_ranges(comparison: RangeComparison) -> pd.DataFrame:\n    return pd.DataFrame(\n        comparison.dataframe()\n        .select(\n            "range_period",\n            "current_value",\n            "minimum",\n            "lower_quartile",\n            "median",\n            "upper_quartile",\n            "maximum",\n        )\n        .sort("range_period")\n        .collect()\n    ).rename(columns=str.lower)\n\n\n@st.cache_data\ndef fetch_dates(metric: Metric) -> Period:\n    row = (\n        metric.dataframe()\n        .select(\n            f.min(metric.columns.timestamp).alias("min_date"),\n            f.max(metric.columns.timestamp).alias("max_date"),\n        )\n        .first()\n    )\n\n    return Period(start=row.MIN_DATE, end=row.MAX_DATE)\n\n\nclass RangeComparisonVisualization(VisualizationModel):\n    viz_type: Literal["range_comparison"] = "range_comparison"\n    comparison: RangeComparison\n\n    def visualize(self) -> None:\n        """Visualize the range comparison with a boxplots of historical ranges and line of current range."""\n        ranges = fetch_ranges(self.comparison)\n\n        if len(ranges) == 0:\n            st.text("No result on this date range")\n            return\n\n        range_periodicity = self.comparison.range_periodicity\n        filters = self.comparison.filters\n\n        PERIOD_NAMES = {\n            "day_of_week": [\n                "Sunday",\n                "Monday",\n                "Tuesday",\n                "Wednesday",\n                "Thursday",\n                "Friday",\n                "Saturday",\n            ],\n            "month": calendar.month_name,\n        }\n        if range_periodicity in PERIOD_NAMES:\n            ranges = ranges.assign(\n                range_period=ranges["range_period"].map(dict(enumerate(PERIOD_NAMES[range_periodicity])))\n            )\n\n        metric_label = self.comparison.metric.name\n        current_period_label = current_period_text(range_periodicity, self.comparison.current_period())\n\n        title = f"{metric_label} in {current_period_label} compared to historical"\n        if not filters.is_empty():\n            title += "".join(" - " + ", ".join(values) for values in filters.mapping.values())\n\n        fig = go.Figure(\n            layout=dict(\n                title=title,\n                xaxis_title="",\n                yaxis_title=metric_label,\n            ),\n        )\n        fig.update_xaxes(type="category")\n        fig.add_trace(\n            go.Box(\n                name="Historical",\n                x=ranges["range_period"],\n                lowerfence=ranges["minimum"],\n                q1=ranges["lower_quartile"],\n                median=ranges["median"],\n                q3=ranges["upper_quartile"],\n                upperfence=ranges["maximum"],\n                **BOXPLOT_STYLE,\n            )\n        )\n        fig.add_trace(\n            go.Scatter(\n                name=current_period_label.capitalize(),\n                x=ranges["range_period"],\n                y=ranges["current_value"],\n                **SCATTER_STYLE,\n                marker_size=10 if len(ranges) < 100 else 6,\n                mode="markers",\n                line_color=COLORS["highlight"],\n            )\n        )\n        fig.update_layout(PLOT_LAYOUT)\n\n        st.plotly_chart(fig, config=PLOTLY_CONFIG, use_container_width=True)\n\n\ndef current_period_text(periodicity: RangePeriodicity, current_period: Period):\n    PERIODICITY_FORMATS: Dict[RangePeriodicity, str] = {\n        "day_of_week": "week %W of %Y",\n        "day_of_month": "days of %b %Y",\n        "day_of_year": "days of %Y",\n        "month": "months of %Y",\n        "quarter": "quarters of %Y",\n    }\n    if periodicity == "week":\n        return f"weeks of {current_period.start.isocalendar()[0]}"\n        #                  ^ when the first week is in the end of the previous year\n\n    return current_period.start.strftime(PERIODICITY_FORMATS[periodicity])\n')
    __stickytape_write_module('range_analysis_app/definitions/__init__.py', b'')
    __stickytape_write_module('range_analysis_app/definitions/period.py', b'import datetime\nfrom typing import Literal\n\nimport pandas as pd\n\nfrom utils import BaseModelCacheKey\n\nPeriodFrequency = Literal["W", "M", "Y", "year-weeks"]\n\n\nclass Period(BaseModelCacheKey):\n    """Period of time between two dates."""\n\n    start: datetime.date\n    end: datetime.date\n\n    @staticmethod\n    def from_pandas(period: pd.Period) -> "Period":\n        return Period(start=period.start_time, end=period.end_time)\n\n    def from_year(year: int) -> "Period":\n        return Period(start=datetime.date(year, 1, 1), end=datetime.date(year, 12, 31))\n\n    @staticmethod\n    def from_date(date: datetime.date, freq: PeriodFrequency) -> "Period":\n        if freq == "year-weeks":\n            return period_of_year_weeks(date.year)\n\n        return Period.from_pandas(pd.Timestamp(date).to_period(freq))\n\n\ndef period_of_year_weeks(year: int) -> Period:\n    """Return the start and end date of the first and last weeks of the given year."""\n    year_period = pd.Period(year, freq="Y")\n\n    first_week = year_period.start_time.to_period("W")\n    last_week = year_period.end_time.to_period("W")\n\n    if first_week.week != 1:  # the last week of previous year\n        first_week += 1\n\n    if last_week.week == 1:  # the first week of next year\n        last_week -= 1\n\n    return Period(start=first_week.start_time.floor("D"), end=last_week.end_time.floor("D"))\n')
    __stickytape_write_module('range_analysis_app/definitions/range_comparison.py', b'from datetime import date\nfrom typing import Any, Dict, List, Literal, get_args\n\nimport pandas as pd\nimport snowflake.snowpark as snowpark\nimport snowflake.snowpark.functions as f\n\nfrom data.metric import Metric\nfrom range_analysis_app import db\nfrom range_analysis_app.definitions.definition import DataframeDefinition\nfrom range_analysis_app.definitions.period import Period, PeriodFrequency\nfrom range_analysis_app.definitions.time_series import TimeSeries\n\nRangePeriodicity = Literal["day_of_month", "day_of_week", "day_of_year", "week", "month", "quarter"]\nRANGE_PERIODICITIES = get_args(RangePeriodicity)\n\n\nclass RangeComparison(DataframeDefinition):\n    metric: Metric\n    range_periodicity: RangePeriodicity\n    historical_dates: Period\n    current_period_date: date\n    filters: db.Filters\n\n    def dataframe(self) -> snowpark.DataFrame:\n        metric_col = self.metric.columns.metric_column\n        historical_ranges = aggregate_over_ranges(\n            self.metric,\n            self.range_periodicity,\n            date_range=self.historical_dates,\n            aggregations=[\n                f.min(metric_col).alias("minimum"),\n                f.percentile_cont(0.25).within_group(metric_col).alias("lower_quartile"),\n                f.median(metric_col).alias("median"),\n                f.percentile_cont(0.75).within_group(metric_col).alias("upper_quartile"),\n                f.max(metric_col).alias("maximum"),\n            ],\n            filters=self.filters,\n        )\n\n        current_ranges = aggregate_over_ranges(\n            self.metric,\n            self.range_periodicity,\n            date_range=self.current_period(),\n            aggregations=[\n                f.any_value(metric_col).alias("current_value"),\n            ],\n            filters=self.filters,\n        )\n\n        return historical_ranges.join(current_ranges, on="range_period", how="left")\n\n    def current_period(self) -> Period:\n        PERIOD_FREQUENCIES: Dict[RangePeriodicity, PeriodFrequency] = {\n            "day_of_week": "W",\n            "day_of_month": "M",\n            "day_of_year": "Y",\n            "week": "year-weeks",\n            "month": "Y",\n            "quarter": "Y",\n        }\n        return Period.from_date(self.current_period_date, freq=PERIOD_FREQUENCIES[self.range_periodicity])\n\n\nRANGE_PERIODICITIES_CONFIGS: Dict[RangePeriodicity, Dict[str, Any]] = {\n    "day_of_week": {"period_function": f.dayofweek, "grain": "day"},\n    "day_of_month": {"period_function": f.dayofmonth, "grain": "day"},\n    "day_of_year": {"period_function": f.dayofyear, "grain": "day"},\n    "week": {"period_function": f.weekofyear, "grain": "week"},\n    "month": {"period_function": f.month, "grain": "month"},\n    "quarter": {"period_function": f.quarter, "grain": "quarter"},\n}\n\n\ndef aggregate_over_ranges(\n    metric: Metric,\n    range_periodicity: RangePeriodicity,\n    date_range: Period,\n    aggregations: List[f.Column],\n    filters: db.Filters,\n) -> snowpark.DataFrame:\n    """Aggregate the given metric over the periods of a periodicity.\n\n    For example, to aggregate each month between 2010 and 2020, outputting 12 rows.\n    """\n\n    range_config = RANGE_PERIODICITIES_CONFIGS[range_periodicity]\n    period_column = range_config["period_function"]("period").alias("range_period")\n\n    cols = metric.columns\n    return (\n        TimeSeries(metric=metric, grain=range_config["grain"])\n        .dataframe()\n        .filter(filters.to_condition())\n        .filter(f.col("period").between(date_range.start, date_range.end))\n        .select([cols.metric_column, period_column])\n        .group_by([period_column.get_name()])\n        .agg(aggregations)\n        .sort(period_column.get_name())\n    )\n')
    __stickytape_write_module('range_analysis_app/definitions/definition.py', b'import hashlib\nfrom abc import abstractmethod\nfrom typing import Generic, Literal, TypeVar\n\nimport snowflake.snowpark as snowpark\nfrom pydantic import BaseModel\n\nimport range_analysis_app.db as db\nfrom utils import BaseModelCacheKey\n\n\ndef hash_json(model: BaseModel) -> int:\n    """Return a constant hash representing the given Pydantic model."""\n    return int(hashlib.sha1(model.json().encode()).hexdigest()[:10], 16)\n\n\nclass DataframeDefinition(BaseModelCacheKey):\n    """Represent a Snowpark calculation resulting in a dataframe.\n\n    This definition is hashable to be able to cache the result in Streamlit.\n    """\n\n    __hash__ = hash_json\n\n    @abstractmethod\n    def dataframe(self) -> snowpark.DataFrame:\n        ...\n\n\nTDefinition = TypeVar("TDefinition", bound=DataframeDefinition)\n\n\nclass ResultTable(Generic[TDefinition], BaseModel):\n    """Table stored from a Snowpark Dataframe."""\n\n    __hash__ = hash_json\n\n    table_name: str\n    definition: TDefinition\n\n    def table(self) -> snowpark.Table:\n        return db.snowflake_session().table(self.table_name)\n\n\ndef generate_table(\n    definition: TDefinition,\n    table_name: str,\n    table_type: Literal["", "temporary"] = "",\n) -> ResultTable[TDefinition]:\n    """Generate a table for the given dataframe and return the table name."""\n    definition.dataframe().write.save_as_table(table_name, table_type=table_type, mode="overwrite")\n    return ResultTable(table_name=table_name, definition=definition)\n')
    __stickytape_write_module('range_analysis_app/definitions/time_series.py', b'from typing import Literal, get_args\n\nimport snowflake.snowpark as snowpark\nimport snowflake.snowpark.functions as f\n\nfrom data.metric import Metric\nfrom range_analysis_app.definitions.definition import DataframeDefinition\n\nGrain = Literal["day", "week", "month", "quarter", "year"]\nGRAINS = get_args(Grain)\n\n\nclass TimeSeries(DataframeDefinition):\n    metric: Metric\n    grain: Grain\n\n    def dataframe(self) -> snowpark.DataFrame:\n        cols = self.metric.columns\n        period_column = f.date_trunc(self.grain, cols.timestamp).alias("period")\n\n        return (\n            self.metric.dataframe()\n            .select(cols.dimensions + [cols.metric_column, period_column])\n            .group_by(cols.dimensions + [period_column.get_name()])\n            .agg(getattr(f, cols.agg_method)(cols.metric_column).alias(cols.metric_column))\n        )\n')
    __stickytape_write_module('range_analysis_app/exploration/plotly_styles.py', b'import plotly.express as px\nimport plotly.graph_objects as go\n\nCOLORS = {\n    "text": "#3e49a7",\n    "background": "#f0f2f6",\n    "plot-background": "#ffffff",\n    "selected-background": "#fff0f0",\n    "line-color": "#00a9de",\n    "line-background": "#9cdcf7",\n    "highlight": "#ff2255",\n    "indicator-text": "#ffffff",\n    "indicator-background": "#2d3579",\n    "indicator-increasing": "#2fed6a",\n    "indicator-decreasing": "#fd0101",\n}\n\n# Doc: https://plotly.com/python/configuration-options\nPLOTLY_CONFIG = {\n    "displayModeBar": False,\n}\n\n# Doc: https://plotly.com/python-api-reference/generated/plotly.graph_objects.layout.html#plotly.graph_objects.layout.Shape\nPLOT_BORDER = go.layout.Shape(\n    type="rect",\n    xref="paper",\n    yref="paper",\n    x0=0,\n    y0=0,\n    x1=1,\n    y1=1,\n    line={"width": 1, "color": COLORS["text"]},\n)\n\n# Doc: https://plotly.com/python-api-reference/generated/plotly.graph_objects.Layout.html\nPLOT_LAYOUT = go.Layout(\n    height=420,\n    margin={"t": 40, "b": 6, "l": 6, "r": 40},\n    font={\n        "family": "Source Sans Pro",\n        "color": COLORS["text"],\n    },\n    showlegend=True,\n    paper_bgcolor=COLORS["background"],\n    plot_bgcolor=COLORS["plot-background"],\n    title={\n        "font": {"color": COLORS["text"]},\n        "x": 0.02,\n        "y": 0.98,\n        "xanchor": "left",\n        "yanchor": "top",\n    },\n    xaxis={\n        "title": {"font": {"color": COLORS["text"], "size": 14}, "standoff": 16},\n        "showgrid": True,\n        "color": COLORS["text"],\n        "ticks": "inside",\n        "tickcolor": COLORS["text"],\n        "tickfont": {"size": 12, "color": COLORS["text"]},\n        "dtick": "M12",\n    },\n    yaxis={\n        "title": {"font": {"color": COLORS["text"]}, "standoff": 16},\n        "showgrid": True,\n        "color": COLORS["text"],\n        "ticks": "inside",\n        "tickcolor": COLORS["text"],\n        "tickfont": {"size": 14, "color": COLORS["text"]},\n    },\n    legend={\n        "font": {"color": COLORS["text"]},\n        "x": 1,\n        "y": 1.125,\n        "xanchor": "right",\n        "yanchor": "top",\n        "orientation": "h",\n        # "xanchor": "left",\n        # "orientation": "v",\n    },\n)\n\n\n# Doc: https://plotly.com/python-api-reference/generated/plotly.graph_objects.Box.html\nBOXPLOT_STYLE = go.Box(\n    fillcolor=COLORS["line-background"],\n    line={"color": COLORS["line-color"]},\n)\n\n\nSCATTER_STYLE = go.Scatter(\n    hovertemplate="<b>%{y}</b><br>%{x}<extra></extra>",\n    # margin={"line": {"width": 0}, "size": 6},\n    line={"width": 1, "color": COLORS["line-color"]},\n)\n\nBAR_STYLE = go.Bar(\n    hovertemplate="<b>%{y}</b><br>%{x}<extra></extra>",\n)\n\n# Doc: https://plotly.com/python/reference/pie\nPIE_STYLE = go.Pie(\n    marker_colors=px.colors.sequential.Blues,\n    hoverinfo="label",\n)\nPIE_LAYOUT = go.Layout(\n    showlegend=False,\n    # legend={"xanchor": "left", "orientation": "v"}\n)\n\nHEATMAP_STYLE = go.Heatmap(\n    # colorscale=px.colors.sequential.Blues,\n    autocolorscale=True,\n    hovertemplate="<b>%{z}</b><br>%{y} - %{x}<extra></extra>",\n    texttemplate="%{z}",\n)\n\n\n# Style of an example chart\nEXAMPLE_LAYOUT = go.Layout(\n    height=150,\n    margin={"t": 15, "b": 15, "l": 15, "r": 15},\n    showlegend=False,\n    xaxis={"title": None},\n    yaxis={"title": None},\n)\nEXAMPLE_COLORS = px.colors.sequential.Blues_r\n\n\n# Styles of a selected chart\nSELECTED_STYLE = go.Layout(\n    title={"font": {"color": COLORS["highlight"]}},\n    paper_bgcolor=COLORS["selected-background"],\n    # shapes=[PLOT_BORDER.update(go.layout.Shape(line={"width": 2, "color": COLORS["highlight"]}))],\n)\n\nINDICATOR_LAYOUT = go.Layout(\n    height=100,\n    font_family="Source Sans Pro",\n    font={"color": COLORS["indicator-text"]},\n    paper_bgcolor=COLORS["indicator-background"],\n    margin={"t": 150, "l": 12},\n)\nINDICATOR_STYLE = go.Indicator(\n    mode="number+delta",\n    title={\n        "align": "left",\n        "font": {"size": 16},\n    },\n    align="left",\n    number={\n        "font": {"size": 36},\n        "valueformat": ",.0f",\n    },\n    delta={\n        "font": {"size": 14},\n        "valueformat": ",.0f",\n        "position": "bottom",\n        "increasing": {\n            "color": COLORS["indicator-increasing"],\n            "symbol": "\xe2\x86\x91 ",\n        },\n        "decreasing": {\n            "color": COLORS["indicator-decreasing"],\n            "symbol": "\xe2\x86\x93 ",\n        },\n    },\n)\nINDICATOR_SELECTED_STYLE = {\n    "paper_bgcolor": COLORS["highlight"],\n}\n')
    __stickytape_write_module('range_analysis_app/exploration/time_series_exploration.py', b'from typing import Literal\n\nimport pandas as pd\nimport plotly.graph_objects as go\nimport snowflake.snowpark.functions as f\nimport streamlit as st\nfrom streamlit.delta_generator import DeltaGenerator\n\nimport range_analysis_app.db as db\nfrom data.metric import Metric\nfrom range_analysis_app.definitions.time_series import GRAINS, TimeSeries\nfrom range_analysis_app.exploration.range_comparison_exploration import PLOT_LAYOUT, PLOTLY_CONFIG, SCATTER_STYLE\nfrom range_analysis_app.exploration.visualization import VisualizationModel\n\n\ndef time_series_exploration(metric: Metric, content_col: DeltaGenerator, side_col: DeltaGenerator) -> None:\n    """Component to create a time series and visualize it."""\n    with side_col:\n        grain = st.selectbox("Periodicity", GRAINS, index=GRAINS.index("month"))\n        time_series = TimeSeries(metric=metric, grain=grain)\n\n        filters = multiselect_dimension_filters(metric)\n        viz = TimeSeriesVisualization(time_series=time_series, filters=filters)\n        viz.button_add_to_dashboard()\n\n    with content_col:\n        viz.visualize()\n\n\nclass TimeSeriesVisualization(VisualizationModel):\n    viz_type: Literal["time_series"] = "time_series"\n    time_series: TimeSeries\n    filters: db.Filters\n\n    def visualize(self) -> None:\n        df = fetch_time_series(self.time_series, self.filters)\n\n        metric = self.time_series.metric\n        metric_column = metric.columns.metric_column.lower()\n\n        title = f"{metric.name} per {self.time_series.grain}"\n        if not self.filters.is_empty():\n            title += "".join(" - " + ", ".join(values) for values in self.filters.mapping.values())\n\n        y_axis_title = metric.name\n        x_axis_title = ""\n\n        fig = (\n            go.Figure(\n                layout=dict(\n                    title=title,\n                    xaxis_title=x_axis_title,\n                    yaxis_title=y_axis_title,\n                )\n            )\n            .add_trace(\n                go.Scatter(\n                    name="Values",\n                    x=df["period"],\n                    y=df[metric_column],\n                    **SCATTER_STYLE,\n                    mode="lines+markers",\n                )\n            )\n            .update_layout(PLOT_LAYOUT)\n        )\n\n        st.plotly_chart(fig, config=PLOTLY_CONFIG, use_container_width=True)\n\n\n@st.cache_data\ndef fetch_time_series(time_series: TimeSeries, filters: db.Filters) -> pd.DataFrame:\n    metric_column = time_series.metric.columns.metric_column\n    return pd.DataFrame(\n        time_series.dataframe()\n        .filter(filters.to_condition())\n        .group_by("period")\n        .agg(f.sum(metric_column).alias(metric_column))\n        .sort("period")\n        .collect()\n    ).rename(columns=str.lower)\n\n\ndef multiselect_dimension_filters(metric: Metric) -> db.Filters:\n    st.markdown("##### Filters")\n\n    filters = db.Filters()\n    for dimension in metric.columns.dimensions:\n        label = metric.columns.labels.get(dimension, dimension)\n\n        values = st.multiselect(label, metric.dimension_values(dimension))\n        if values:\n            filters.add(dimension, values)\n\n    return filters\n')
    __stickytape_write_module('range_analysis_app/exploration/visualization.py', b'from abc import abstractmethod\n\nimport streamlit as st\nfrom pydantic import BaseModel\n\nfrom data.helpers import repository\nfrom range_analysis_app.definitions.definition import hash_json\n\n\nclass VisualizationModel(BaseModel):\n    __hash__ = hash_json\n\n    @abstractmethod\n    def visualize():\n        ...\n\n    def button_add_to_dashboard(self) -> None:\n        """button to add the current visualization to the dashboard."""\n        if hash(self) in repository().visualizations:\n            st.info("Added in dashboard")\n            return\n\n        def add_to_dashboard():\n            repository().visualizations[hash(self)] = self\n\n        st.button("Add to dashboard", on_click=add_to_dashboard)\n')
    __stickytape_write_module('widgets/__init__.py', b'\n')
    __stickytape_write_module('widgets/kpis.py', b'import datetime\nfrom datetime import date\nfrom typing import Dict, Literal, Tuple\n\nimport plotly.graph_objs as go\nimport snowflake.snowpark.functions as f\nimport streamlit as st\nfrom typing_extensions import Literal\n\nfrom data.helpers import repository\nfrom data.metric import Metric\nfrom range_analysis_app import db\nfrom range_analysis_app.definitions.period import Period\nfrom range_analysis_app.definitions.time_series import GRAINS, Grain, TimeSeries\nfrom range_analysis_app.exploration.plotly_styles import INDICATOR_LAYOUT, INDICATOR_STYLE\nfrom widgets.example_charts import example_kpi_metric\nfrom widgets.parameters import metric_selection, multiselect_dimension_filters, year_range_slider\nfrom widgets.widget import Widget, WidgetSize\n\n\nclass TotalPurchaseValue(Widget):\n    widget_kind: Literal["chart", "kpi", "text"] = "kpi"\n    widget_type: Literal["TotalPurchaseValue"] = "TotalPurchaseValue"\n    size: WidgetSize = "small"\n\n    # Parameters\n    metric: Metric\n    grain: Grain\n\n    # Filters\n    period: Period\n    filters: db.Filters = db.Filters()\n\n    @staticmethod\n    def default() -> Widget:\n        metric = repository().metrics[0]\n\n        min_date, max_date = metric.date_range()\n        return TotalPurchaseValue(\n            metric=metric,\n            grain="month",\n            period=Period(start=min_date, end=max_date),\n        )\n\n    @staticmethod\n    def description() -> str:\n        return "Total Purchase Value"\n\n    @staticmethod\n    def example_figure() -> go.Figure:\n        return example_kpi_metric()\n\n    def default_title(self) -> str:\n        _, max_date = self.metric.date_range()\n        last_period = min(max_date, self.period.end)\n        return f"Total {period_text(self.grain, last_period)}"\n\n    def edit_parameters(self) -> None:\n        title = st.text_input("Title", value=self.title, placeholder=self.default_title())\n        metric = metric_selection(self.metric)\n        grain = st.selectbox("Periodicity", GRAINS, index=GRAINS.index(self.grain))\n\n        st.markdown("#### Filters")\n        start_year, end_year = year_range_slider(self.metric, (self.period.start.year, self.period.end.year))\n        filters = multiselect_dimension_filters(self.metric, self.filters)\n\n        def apply_changes() -> None:\n            self.title = title\n            self.metric = metric\n            self.grain = grain\n            self.period = Period(start=date(start_year, 1, 1), end=date(end_year, 12, 31))\n            self.filters = filters\n\n        st.button("Apply", use_container_width=True, on_click=apply_changes)\n\n    def figure(self) -> go.Figure:\n        current, previous = fetch_total_values(self.metric, self.grain, self.filters, self.period)\n\n        return go.Figure(\n            layout=INDICATOR_LAYOUT,\n            data=go.Indicator(\n                value=current["VALUE"],\n                delta=go.indicator.Delta(reference=previous["VALUE"]),\n                title={"text": self.displayed_title()},\n            ).update(INDICATOR_STYLE),\n        )\n\n\n@st.cache_data\ndef fetch_total_values(metric: Metric, grain: Grain, filters: db.Filters, period: Period) -> Tuple[float, float]:\n    metric_column = metric.columns.metric_column\n\n    current, previous = (\n        TimeSeries(metric=metric, grain=grain)\n        .dataframe()\n        .filter(filters.to_condition())\n        .filter(f.col("period").between(period.start, period.end))\n        .group_by("period")\n        .agg(f.sum(metric_column).alias("value"))\n        .sort("period", ascending=False)\n        .first(2)\n    )\n    return current, previous\n\n\ndef period_text(grain: Grain, period: datetime.date):\n    GRAIN_FORMATS: Dict[Grain, str] = {\n        "day": "%Y-%m-%d",\n        "week": "Week %W",\n        "month": "%b %Y",\n        "quarter": "Q%q",\n        "year": "%Y",\n    }\n    return period.strftime(GRAIN_FORMATS[grain])\n')
    __stickytape_write_module('widgets/example_charts.py', b'import itertools\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.graph_objs as go\n\nfrom range_analysis_app.exploration.plotly_styles import (\n    EXAMPLE_COLORS,\n    EXAMPLE_LAYOUT,\n    INDICATOR_LAYOUT,\n    INDICATOR_STYLE,\n    PLOT_LAYOUT,\n)\n\n\ndef example_pie_chart() -> go.Figure:\n    df = pd.DataFrame(\n        {\n            "Product Category": list("ABCDE"),\n            "Percentage": np.random.randint(100, size=5),\n        }\n    )\n    return (\n        px.pie(df, names="Product Category", values="Percentage", title=" ", color_discrete_sequence=EXAMPLE_COLORS)\n        .update_layout(PLOT_LAYOUT)\n        .update_layout(EXAMPLE_LAYOUT)\n    )\n\n\ndef example_bar_comparison_chart() -> go.Figure:\n    return (\n        go.Figure(\n            layout=go.Layout(title=""),\n            data=[\n                go.Bar(name="2022", x=list("ABCDE"), y=np.random.randint(75, size=5) + 25),\n                go.Bar(name="2023", x=list("ABCDE"), y=np.random.randint(75, size=5) + 25),\n            ],\n        )\n        .update_layout(PLOT_LAYOUT)\n        .update_layout(EXAMPLE_LAYOUT)\n    )\n\n\ndef example_bar_chart() -> go.Figure:\n    df = pd.DataFrame(\n        {\n            "Product Category": list("ABCDEFGHIJ"),\n            "Purchasing Volume": np.random.randint(100, size=10),\n        }\n    )\n    return (\n        px.bar(df, x="Product Category", y="Purchasing Volume", title=" ", color_discrete_sequence=EXAMPLE_COLORS)\n        .update_layout(PLOT_LAYOUT)\n        .update_layout(EXAMPLE_LAYOUT)\n    )\n\n\ndef example_box_chart() -> go.Figure:\n    df = pd.DataFrame(\n        {\n            "Product Category": itertools.chain(*([category] * 10 for category in list("ABCDEFGHIJ"))),\n            "Purchase Amount": np.random.randint(100, size=100),\n        }\n    )\n    return (\n        px.box(df, x="Product Category", y="Purchase Amount", title=" ", color_discrete_sequence=EXAMPLE_COLORS)\n        .update_layout(PLOT_LAYOUT)\n        .update_layout(EXAMPLE_LAYOUT)\n    )\n\n\ndef example_line_chart() -> go.Figure:\n    df = pd.DataFrame(\n        {\n            "month": pd.date_range(end="2023-01-01", periods=30, freq="MS"),\n            "trend": np.random.randint(100, size=30) + 50,\n        }\n    )\n    return (\n        px.line(df, x="month", y="trend", title=" ", range_y=[0, 200], color_discrete_sequence=EXAMPLE_COLORS)\n        .update_layout(PLOT_LAYOUT)\n        .update_layout(EXAMPLE_LAYOUT)\n    )\n\n\ndef example_area_chart() -> go.Figure:\n    df = pd.DataFrame(\n        {\n            "quarter": pd.date_range(end="2023-01-01", periods=24, freq="Q"),\n            "trend": np.random.randint(100, size=24) + 50,\n        }\n    )\n    return (\n        px.area(df, x="quarter", y="trend", title=" ", color_discrete_sequence=EXAMPLE_COLORS)\n        .update_layout(PLOT_LAYOUT)\n        .update_layout(EXAMPLE_LAYOUT)\n    )\n\n\ndef example_heatmap_chart() -> go.Figure:\n    scores = np.concatenate((np.random.randint(50, size=45), np.random.randint(10, size=5) + 90))\n    np.random.shuffle(scores)\n\n    df = pd.DataFrame(\n        {\n            "X": list("ABCDE") * 10,\n            "Y": itertools.chain(*([category] * 10 for category in list("ABCDE"))),\n            "score": scores,\n        }\n    )\n    return (\n        go.Figure(\n            layout={"title": " "},\n            data=go.Heatmap(\n                x=df["X"],\n                y=df["Y"],\n                z=df["score"],\n                # colorscale=list(reversed(EXPRESS_COLORS)),\n                autocolorscale=True,\n                hovertext=df["X"] + " - " + df["Y"] + ": " + df["score"].astype(str),\n                hoverinfo="text",\n                colorbar=dict(title="Score"),\n            ),\n        )\n        .update_layout(PLOT_LAYOUT)\n        .update_layout(EXAMPLE_LAYOUT)\n    )\n\n\ndef example_kpi_metric() -> go.Figure:\n    return go.Figure(\n        layout=INDICATOR_LAYOUT,\n        data=go.Indicator(\n            value=30000,\n            delta=go.indicator.Delta(reference=30000 * 0.90),\n            title={"text": "Average per Month"},\n        ).update(INDICATOR_STYLE),\n    )\n')
    __stickytape_write_module('widgets/parameters.py', b'from typing import Tuple\n\nimport streamlit as st\n\nfrom data.helpers import repository\nfrom data.metric import Metric\nfrom range_analysis_app import db\n\n\ndef year_range_slider(metric: Metric, value: Tuple[int, int]) -> Tuple[int, int]:\n    """Slider to select the range of years."""\n    min_date, max_date = metric.date_range()\n    return st.slider(\n        "Time Period",\n        min_value=min_date.year,\n        max_value=max_date.year,\n        value=value,\n    )\n\n\ndef metric_selection(metric: Metric) -> Metric:\n    """Selectbox among all metrics."""\n    return st.selectbox(\n        "Metric",\n        options=repository().metrics,\n        index=repository().metrics.index(metric),\n        format_func=lambda metric: metric.name,\n    )\n\n\ndef multiselect_dimension_filters(metric: Metric, current_filters: db.Filters) -> db.Filters:\n    """List of multiselect filters for each dimension in the metric."""\n    filters = db.Filters()\n\n    for dimension in metric.columns.dimensions:\n        label = metric.columns.labels.get(dimension, dimension)\n\n        values = st.multiselect(\n            label,\n            metric.dimension_values(dimension),\n            default=current_filters.mapping.get(dimension),\n        )\n        if values:\n            filters.add(dimension, values)\n\n    return filters\n')
    __stickytape_write_module('widgets/widget.py', b'from abc import abstractmethod\nfrom typing import Literal, Optional\n\nimport plotly.graph_objects as go\n\nfrom range_analysis_app.definitions.time_series import GRAINS\nfrom utils import BaseModelCacheKey\n\nWidgetSize = Literal["small", "medium", "large"]\n\n\nclass Widget(BaseModelCacheKey):\n    widget_kind: Literal["chart", "kpi", "text"] = "chart"\n    size: WidgetSize = "medium"\n\n    title: str = ""\n\n    @staticmethod\n    @abstractmethod\n    def default() -> "Widget":\n        """Default instance of the widget"""\n\n    @staticmethod\n    @abstractmethod\n    def description() -> str:\n        """Description of the widget in the gallery."""\n\n    @abstractmethod\n    def default_title(self) -> str:\n        """Default title inferred from parameters."""\n\n    def displayed_title(self) -> str:\n        """Either the title defined by the user or the one inferred from parameters."""\n        return (self.title or self.default_title()).strip()\n\n    @staticmethod\n    @abstractmethod\n    def example_figure() -> go.Figure:\n        """Return an example plotly figure showing how the widget look like."""\n\n    @abstractmethod\n    def edit_parameters(self) -> None:\n        """Render a form for editing the widget parameters."""\n\n    @abstractmethod\n    def figure(self) -> go.Figure:\n        """Return the plotly figure with the defined parameters."""\n')
    __stickytape_write_module('widgets/outlier_detection_widgets.py', b'import calendar\nfrom datetime import date\nfrom textwrap import dedent\nfrom typing import Literal, Optional, Tuple\n\nimport pandas as pd\nimport plotly.graph_objs as go\nimport snowflake.snowpark.functions as f\nimport streamlit as st\n\nfrom data.helpers import repository\nfrom data.metric import Metric\nfrom range_analysis_app import db\nfrom range_analysis_app.definitions.period import Period\nfrom range_analysis_app.definitions.range_comparison import RANGE_PERIODICITIES, RANGE_PERIODICITIES_CONFIGS\nfrom range_analysis_app.exploration.plotly_styles import HEATMAP_STYLE, PLOT_LAYOUT\nfrom widgets.example_charts import example_heatmap_chart\nfrom widgets.parameters import metric_selection, multiselect_dimension_filters, year_range_slider\nfrom widgets.product_category_distribution_widgets import retrieve_category_column\nfrom widgets.widget import Widget, WidgetSize\n\n\nclass PurchaseAnomaliesHeatmap(Widget):\n    widget_kind: Literal["chart", "kpi", "text"] = "chart"\n    widget_type: Literal["PurchaseAnomaliesHeatmap"] = "PurchaseAnomaliesHeatmap"\n    size: WidgetSize = "large"\n\n    # Parameters\n    metric: Metric\n    x_col: str\n    y_col: str\n\n    # Filters\n    period: Period\n    filters: db.Filters = db.Filters()\n\n    @staticmethod\n    def default() -> Widget:\n        metric = repository().metrics[0]\n        min_date, max_date = metric.date_range()\n\n        return PurchaseAnomaliesHeatmap(\n            metric=metric,\n            x_col="month",\n            y_col=retrieve_category_column(metric),  # TODO: define category in configs\n            period=Period(start=min_date, end=max_date),\n        )\n\n    @staticmethod\n    def description() -> str:\n        return "Visualizes purchase anomalies"\n\n    @staticmethod\n    def example_figure() -> go.Figure:\n        return example_heatmap_chart()\n\n    def default_title(self) -> str:\n        return "Purchase Anomalies - Product"\n\n    def edit_parameters(self) -> None:\n        title = st.text_input("Title", value=self.title, placeholder=self.default_title())\n        metric = metric_selection(self.metric)\n\n        # dim_columns = self.metric.columns.dimensions + list(RANGE_PERIODICITIES)\n        # x_col = st.selectbox("X column", options=dim_columns, index=dim_columns.index(self.x_col))\n        # y_col = st.selectbox("Y column", options=dim_columns, index=dim_columns.index(self.y_col))\n        x_col = st.selectbox("Period", options=RANGE_PERIODICITIES, index=RANGE_PERIODICITIES.index(self.x_col))\n\n        st.markdown("#### Filters")\n        start_year, end_year = year_range_slider(self.metric, (self.period.start.year, self.period.end.year))\n        filters = multiselect_dimension_filters(self.metric, self.filters)\n\n        def apply_changes() -> None:\n            self.title = title\n            self.metric = metric\n            self.x_col = x_col\n            # self.y_col = y_col\n            self.period = Period(start=date(start_year, 1, 1), end=date(end_year, 12, 31))\n            self.filters = filters\n\n        st.button("Apply", use_container_width=True, on_click=apply_changes)\n\n    def figure(self) -> go.Figure:\n        metric = self.metric\n\n        df = fetch_correlation(self.metric, self.x_col, self.y_col, self.filters, self.period)\n\n        x = df[self.x_col.upper()]\n        y = df[self.y_col.upper()]\n        z = df[metric.columns.metric_column]\n\n        if self.x_col in PERIOD_NAMES:\n            x = x.map(dict(enumerate(PERIOD_NAMES[self.x_col])))\n\n        return go.Figure(\n            layout=go.Layout(title=self.displayed_title()),\n            data=go.Heatmap(x=x, y=y, z=z).update(HEATMAP_STYLE),\n        ).update_layout(PLOT_LAYOUT)\n\n\n@st.cache_data\ndef fetch_correlation(\n    metric: Metric,\n    x_column: str,\n    y_column: str,\n    filters: db.Filters,\n    period: Period,\n) -> pd.DataFrame:\n    """Fetch the dataframe with [X, Y, Metric] columns."""\n    cols = metric.columns\n\n    def category_column(col: str) -> f.Column:\n        """Either use the given dimension column or the truncated date column"""\n        if col in RANGE_PERIODICITIES_CONFIGS:\n            config = RANGE_PERIODICITIES_CONFIGS[col]\n            return config["period_function"](cols.timestamp).alias(col)\n\n        return f.col(col)\n\n    x_col = category_column(x_column)\n    y_col = category_column(y_column)\n\n    return (\n        metric.dataframe()\n        .select([x_col, y_col, cols.metric_column])\n        .filter(filters.to_condition())\n        .filter(f.col(cols.timestamp).between(period.start, period.end))\n        .group_by([x_col.get_name(), y_col.get_name()])\n        .agg(getattr(f, cols.agg_method)(cols.metric_column).alias(cols.metric_column))\n        .sort([x_col.get_name(), y_col.get_name()])\n        .to_pandas()\n    )\n\n\nPERIOD_NAMES = {\n    "day_of_week": [\n        "Sunday",\n        "Monday",\n        "Tuesday",\n        "Wednesday",\n        "Thursday",\n        "Friday",\n        "Saturday",\n    ],\n    "month": calendar.month_name,\n    "quarter": ["", "Q1", "Q2", "Q3", "Q4"],\n}\n')
    __stickytape_write_module('widgets/product_category_distribution_widgets.py', b'from datetime import date\nfrom typing import Literal, get_args\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport snowflake.snowpark.functions as f\nimport streamlit as st\n\nfrom data.helpers import repository\nfrom data.metric import Metric\nfrom range_analysis_app import db\nfrom range_analysis_app.definitions.period import Period\nfrom range_analysis_app.exploration.plotly_styles import BAR_STYLE, EXAMPLE_COLORS, PIE_LAYOUT, PIE_STYLE, PLOT_LAYOUT\nfrom widgets.example_charts import example_bar_comparison_chart, example_pie_chart\nfrom widgets.parameters import metric_selection, multiselect_dimension_filters, year_range_slider\nfrom widgets.widget import Widget, WidgetSize\n\nViewingMode = Literal["percent", "value"]\nVIEWING_MODES = get_args(ViewingMode)\n\n\nclass ProductCategoryDistributionPieChart(Widget):\n    widget_kind: Literal["chart", "kpi", "text"] = "chart"\n    widget_type: Literal["ProductCategoryDistributionPieChart"] = "ProductCategoryDistributionPieChart"\n    size: WidgetSize = "small"\n\n    # Parameters\n    metric: Metric\n    category_column: str\n    textinfo: Literal["percent", "value"] = "percent"\n\n    # Filters\n    period: Period\n    filters: db.Filters = db.Filters()\n\n    @staticmethod\n    def default() -> Widget:\n        metric = repository().metrics[0]\n        min_date, max_date = metric.date_range()\n\n        return ProductCategoryDistributionPieChart(\n            metric=metric,\n            category_column=retrieve_category_column(metric),  # TODO: define category in configs\n            period=Period(start=min_date, end=max_date),\n        )\n\n    @staticmethod\n    def description() -> str:\n        return "Purchase Behaviour - Product"\n\n    @staticmethod\n    def example_figure() -> go.Figure:\n        return example_pie_chart()\n\n    def default_title(self) -> str:\n        return "Product Categories"\n\n    def edit_parameters(self) -> None:\n        title = st.text_input("Title", value=self.title, placeholder=self.default_title())\n        metric = metric_selection(self.metric)\n        textinfo = st.radio(\n            "Viewing mode",\n            VIEWING_MODES,\n            index=VIEWING_MODES.index(self.textinfo),\n            horizontal=True,\n            format_func=str.capitalize,\n        )\n\n        st.markdown("#### Filters")\n        start_year, end_year = year_range_slider(self.metric, (self.period.start.year, self.period.end.year))\n        filters = multiselect_dimension_filters(self.metric, self.filters)\n\n        def apply_changes() -> None:\n            self.title = title\n            self.metric = metric\n            self.textinfo = textinfo\n            self.period = Period(start=date(start_year, 1, 1), end=date(end_year, 12, 31))\n            self.filters = filters\n\n        st.button("Apply", use_container_width=True, on_click=apply_changes)\n\n    def figure(self) -> go.Figure:\n        metric = repository().metrics[0]\n        df = fetch_categories(self.metric, self.category_column, self.period, self.filters)\n\n        return (\n            go.Figure(\n                layout=go.Layout(title=self.displayed_title()),\n                data=go.Pie(\n                    labels=df[self.category_column],\n                    values=df[metric.columns.metric_column],\n                    textinfo=self.textinfo,\n                ).update(PIE_STYLE),\n            )\n            .update_layout(PLOT_LAYOUT)\n            .update_layout(PIE_LAYOUT)\n        )\n\n\nclass ProductCategoriesComparisonBarGraph(Widget):\n    widget_kind: Literal["chart", "kpi", "text"] = "chart"\n    widget_type: Literal["ProductCategoriesComparisonBarGraph"] = "ProductCategoriesComparisonBarGraph"\n    size: WidgetSize = "medium"\n\n    # Parameters\n    metric: Metric\n    category_column: str\n    year_a: int\n    year_b: int\n\n    # Filters\n    filters: db.Filters = db.Filters()\n\n    @staticmethod\n    def default() -> Widget:\n        metric = repository().metrics[0]\n        _, max_date = metric.date_range()\n\n        metric.columns.dimensions\n        return ProductCategoriesComparisonBarGraph(\n            metric=metric,\n            category_column=retrieve_category_column(metric),  # TODO: define category in configs\n            year_a=max_date.year - 1,\n            year_b=max_date.year,\n        )\n\n    @staticmethod\n    def description() -> str:\n        return "Bar Graph of Top Product Categories"\n\n    @staticmethod\n    def example_figure() -> go.Figure:\n        return example_bar_comparison_chart()\n\n    def default_title(self) -> str:\n        return "Top Product Categories"\n\n    def edit_parameters(self) -> None:\n        title = st.text_input("Title", value=self.title, placeholder=self.default_title())\n        metric = metric_selection(self.metric)\n\n        min_date, max_date = metric.date_range()\n        years = list(range(min_date.year, max_date.year + 1))\n\n        cols = st.columns([1, 1])\n        year_a = cols[0].selectbox("Comparing year", years, index=years.index(self.year_a))\n        year_b = cols[1].selectbox("with year", years, index=years.index(self.year_b))\n\n        st.markdown("#### Filters")\n        filters = multiselect_dimension_filters(self.metric, self.filters)\n\n        def apply_changes() -> None:\n            self.title = title\n            self.metric = metric\n            self.year_a = year_a\n            self.year_b = year_b\n            self.filters = filters\n\n        st.button("Apply", use_container_width=True, on_click=apply_changes)\n\n    def figure(self) -> go.Figure:\n        df_a = fetch_categories(self.metric, self.category_column, Period.from_year(self.year_a), self.filters)\n        df_b = fetch_categories(self.metric, self.category_column, Period.from_year(self.year_b), self.filters)\n\n        metric_column = self.metric.columns.metric_column\n        return go.Figure(\n            layout=go.Layout(title=self.displayed_title()),\n            data=[\n                go.Bar(name=self.year_a, x=df_a[self.category_column], y=df_a[metric_column]).update(BAR_STYLE),\n                go.Bar(name=self.year_b, x=df_b[self.category_column], y=df_b[metric_column]).update(BAR_STYLE),\n            ],\n        ).update_layout(PLOT_LAYOUT)\n\n\n@st.cache_data\ndef fetch_categories(metric: Metric, category_column: str, period: Period, filters: db.Filters) -> pd.DataFrame:\n    cols = metric.columns\n    return (\n        metric.dataframe()\n        .filter(filters.to_condition())\n        .filter(f.col(cols.timestamp).between(period.start, period.end))\n        .group_by(category_column)\n        .agg(f.sum(cols.metric_column).alias(cols.metric_column))\n        .to_pandas()\n    )\n\n\ndef retrieve_category_column(metric: Metric) -> str:\n    assert len(metric.columns.dimensions) > 0, "Metric must have a category column."\n\n    CATEGORY_WORDS = ["CATEGORY", "KIND", "TYPE", "CAT"]\n\n    for word in CATEGORY_WORDS:\n        for col in metric.columns.dimensions:\n            if word in col.upper():\n                return col\n\n    return metric.columns.dimensions[0]\n')
    __stickytape_write_module('widgets/purchasing_trend_over_time_widgets.py', b'from datetime import date\nfrom typing import Literal\n\nimport pandas as pd\nimport plotly.graph_objs as go\nimport snowflake.snowpark.functions as f\nimport streamlit as st\nfrom typing_extensions import Literal\n\nfrom data.helpers import repository\nfrom data.metric import Metric\nfrom range_analysis_app import db\nfrom range_analysis_app.definitions.period import Period\nfrom range_analysis_app.definitions.time_series import GRAINS, Grain, TimeSeries\nfrom range_analysis_app.exploration.plotly_styles import PLOT_LAYOUT, SCATTER_STYLE\nfrom widgets.example_charts import example_line_chart\nfrom widgets.parameters import metric_selection, multiselect_dimension_filters, year_range_slider\nfrom widgets.widget import Widget, WidgetSize\n\n\nclass PurchasingTrendsLineChart(Widget):\n    widget_kind: Literal["chart", "kpi", "text"] = "chart"\n    widget_type: Literal["MonthlyPurchasingTrendsLineChart"] = "MonthlyPurchasingTrendsLineChart"\n    size: WidgetSize = "medium"\n\n    # Parameters\n    metric: Metric\n    grain: Grain\n\n    # Filters\n    period: Period\n    filters: db.Filters = db.Filters()\n\n    @staticmethod\n    def default() -> Widget:\n        metric = repository().metrics[0]\n\n        min_date, max_date = metric.date_range()\n\n        return PurchasingTrendsLineChart(\n            metric=metric,\n            grain="month",\n            period=Period(start=min_date, end=max_date),\n        )\n\n    @staticmethod\n    def description() -> str:\n        return "Line Chart of Purchasing Trends"\n\n    @staticmethod\n    def example_figure() -> go.Figure:\n        return example_line_chart()\n\n    def default_title(self) -> str:\n        title = f"{self.metric.name} per {self.grain}"\n        if not self.filters.is_empty():\n            title += "".join(" - " + ", ".join(values) for values in self.filters.mapping.values())\n        return title\n\n    def edit_parameters(self) -> None:\n        title = st.text_input("Title", value=self.title, placeholder=self.default_title())\n        metric = metric_selection(self.metric)\n        grain = st.selectbox("Periodicity", GRAINS, index=GRAINS.index(self.grain))\n\n        st.markdown("#### Filters")\n        start_year, end_year = year_range_slider(self.metric, (self.period.start.year, self.period.end.year))\n        filters = multiselect_dimension_filters(self.metric, self.filters)\n\n        def apply_changes() -> None:\n            self.title = title\n            self.metric = metric\n            self.grain = grain\n            self.period = Period(start=date(start_year, 1, 1), end=date(end_year, 12, 31))\n            self.filters = filters\n\n        st.button("Apply", use_container_width=True, on_click=apply_changes)\n\n    def figure(self) -> go.Figure:\n        df = fetch_time_series(self.metric, self.grain, self.filters, self.period)\n\n        metric = self.metric\n        metric_column = metric.columns.metric_column.lower()\n\n        return go.Figure(\n            layout=go.Layout(title=self.displayed_title()),\n            data=go.Scatter(\n                name="Values",\n                x=df["period"],\n                y=df[metric_column],\n                mode="lines+markers",\n            ).update(SCATTER_STYLE),\n        ).update_layout(PLOT_LAYOUT)\n\n\n@st.cache_data\ndef fetch_time_series(metric: Metric, grain: Grain, filters: db.Filters, period: Period) -> pd.DataFrame:\n    metric_column = metric.columns.metric_column\n\n    return pd.DataFrame(\n        TimeSeries(metric=metric, grain=grain)\n        .dataframe()\n        .filter(filters.to_condition())\n        .filter(f.col("period").between(period.start, period.end))\n        .group_by("period")\n        .agg(f.sum(metric_column).alias(metric_column))\n        .sort("period")\n        .collect()\n    ).rename(columns=str.lower)\n')
    __stickytape_write_module('widgets/widget_gallery.py', b'from typing import Callable, List, Type\n\nimport streamlit as st\n\nfrom range_analysis_app.exploration.plotly_styles import PLOTLY_CONFIG\nfrom widgets.kpis import TotalPurchaseValue\nfrom widgets.outlier_detection_widgets import PurchaseAnomaliesHeatmap\nfrom widgets.product_category_distribution_widgets import (\n    ProductCategoriesComparisonBarGraph,\n    ProductCategoryDistributionPieChart,\n)\nfrom widgets.purchasing_trend_over_time_widgets import PurchasingTrendsLineChart\nfrom widgets.widget import Widget\n\n\ndef widget_gallery(on_add: Callable[[Type[Widget]], None]) -> None:\n    """Gallery of widgets to display in the sidebar."""\n    render_widget_category(\n        title="Product Category Distribution",\n        widget_classes=[ProductCategoryDistributionPieChart],\n        on_add=on_add,\n    )\n    render_widget_category(\n        title="Purchasing Trends Over Time",\n        widget_classes=[PurchasingTrendsLineChart, ProductCategoriesComparisonBarGraph],\n        on_add=on_add,\n    )\n    render_widget_category(\n        title="Outlier Detection",\n        widget_classes=[PurchaseAnomaliesHeatmap],\n        on_add=on_add,\n    )\n    render_widget_category(\n        title="KPIs",\n        widget_classes=[TotalPurchaseValue],\n        on_add=on_add,\n    )\n\n\ndef render_widget_category(\n    title: str,\n    widget_classes: List[Type[Widget]],\n    on_add: Callable[[Type[Widget]], None],\n):\n    if len(widget_classes) == 0:\n        return\n\n    st.markdown(f"## {title}")\n\n    for widget_class in widget_classes:\n        st.markdown(f"**{widget_class.description()}**")\n        cols = st.columns([1, 22, 1])\n        cols[1].plotly_chart(widget_class.example_figure(), config=PLOTLY_CONFIG, use_container_width=True)\n\n        def add_widget(widget_class: Type[Widget]):\n            return lambda: on_add(widget_class)\n\n        st.button(\n            "\\+ Add widget",\n            key=f"add-{widget_class.__name__}",\n            use_container_width=True,\n            on_click=add_widget(widget_class),\n        )\n')
    __stickytape_write_module('app_pages/help_page.py', b'from typing import Dict, Type\n\nimport streamlit as st\n\nfrom app_pages.data_loading import DataLoadingPage\nfrom app_pages.doma_entities_page import DomaEntitiesPage\nfrom app_pages.mapping import MappingPage\nfrom app_pages.metrics_creation import MetricsCreationPage\nfrom app_pages.page import Page\nfrom app_pages.summary_settings_page import SummarySettingsPage\n\n\nclass HelpPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "Help"\n\n    def run(self) -> None:\n        st.title("Help")\n')
    __stickytape_write_module('app_pages/data_loading.py', b'from typing import Dict, List, Optional\n\nimport streamlit as st\n\nfrom app_pages.page import Page\nfrom data.db import fetch_databases, fetch_schemas, fetch_tables\nfrom data.helpers import repository\nfrom data.models import DbTable, Entity\nfrom data.repository import Repository\nfrom range_analysis_app import db\n\n\nclass DataLoadingPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "This page allows users to select the tables and map they to DOMA entities"\n\n    def run(self) -> None:\n        repo = repository()\n\n        dimensions_labels = {str(dim.uid): f\'Dimension: "{dim.name}"\' for dim in repo.dimensions}\n        activities_labels = {str(act.uid): f\'Activity: "{act.name}"\' for act in repo.activities}\n        labels = {**dimensions_labels, **activities_labels}\n        entities: List[Entity] = [*repo.dimensions, *repo.activities]\n\n        if len(entities) == 0:\n            st.warning("You don\'t have Dimensions or Activities. Please go to the Entities page and configure it")\n            return\n\n        st.markdown("#### Source")\n\n        st.markdown(\n            """\n            1. Select your database and schema.\n            2. Select the appropriate source table for each DOMA entity.\n            3. Press "Apply" to save changes.\n            """\n        )\n\n        self.mapping(repo, entities, labels)\n\n    def mapping(self, repo: Repository, entities: List[Entity], labels: Dict[str, str]):\n        cols = st.columns([1, 1])\n        selected_database = cols[0].selectbox("Database", key="selected_database", options=fetch_databases())\n        selected_schema = cols[1].selectbox("Schema", key="selected_schema", options=fetch_schemas(selected_database))\n\n        tables = fetch_tables(selected_database, selected_schema)\n\n        for idx, entity in enumerate(entities):\n            self.mapping_row(entity, idx, labels, repo, selected_database, selected_schema, tables)\n\n        if st.button("Apply"):\n            self._apply_changes(entities, repo, selected_database, selected_schema)\n\n    def mapping_row(self, entity, idx, labels, repo, selected_database, selected_schema, tables):\n        mapping = self._find_table(entity, repo)\n        selected_table_idx = None\n        if mapping is None:\n            selected_table_idx = 0\n        elif mapping.table_schema == selected_schema and mapping.table_database == selected_database:\n            selected_table_idx = tables.index(mapping.table_name)\n        cols = st.columns([3, 1, 6, 2])\n\n        cols[0].markdown(f"### {labels[entity.uid]}")\n        cols[1].markdown(f"### -----\xe2\x86\x92")\n        key = f"tbl_{idx}"\n        if selected_table_idx is not None:\n            cols[2].selectbox(\n                "Source table",\n                key=key,\n                options=tables,\n                index=selected_table_idx,\n                label_visibility="collapsed",\n            )\n        else:\n            sub_cols = cols[2].columns([10, 1])\n            sub_cols[0].write("")\n            sub_cols[0].write(mapping.full_name)\n            # still looking for better replacement of readonly select\n            # sub_cols[0].text_input(\n            #     "Source table",\n            #     value=mapping.full_name,\n            #     key=key, disabled=True,\n            #     label_visibility="collapsed",\n            # )\n            sub_cols[1].button("\xe2\x9d\x8c", key=f"remove_{key}", on_click=lambda: self._remove_mapping(repo, entity))\n\n    def _apply_changes(self, entities, repo, selected_database, selected_schema):\n        for idx, entity in enumerate(entities):\n            mapping = self._find_table(entity, repo)\n            key = f"tbl_{idx}"\n            if key not in st.session_state:\n                continue\n            selected_table_name = st.session_state[key]\n\n            full_selected_table = f"{selected_database}.{selected_schema}.{selected_table_name}"\n\n            if mapping is None:\n                columns = db.fetch_columns(full_selected_table)\n                DbTable.create(\n                    table_database=selected_database,\n                    table_schema=selected_schema,\n                    table_name=selected_table_name,\n                    columns=columns,\n                    entity=entity,\n                )\n            else:\n                if mapping.table_schema == selected_schema and mapping.table_database == selected_database:\n                    columns = db.fetch_columns(full_selected_table)\n                    mapping.table_name = selected_table_name\n                    mapping.columns = columns\n\n    def _find_table(self, entity, repo) -> Optional[DbTable]:\n        for tbl in repo.selected_tables:\n            if entity.uid == tbl.entity_uid:\n                return tbl\n\n        return None\n\n    def _remove_mapping(self, repo: Repository, entity: Entity):\n        mapping = self._find_table(entity, repo)\n        if mapping is None:\n            return\n        mapping.delete()\n')
    __stickytape_write_module('data/db.py', b'from typing import List\n\nimport streamlit as st\n\nfrom range_analysis_app.db import snowflake_session\n\n\n@st.cache_data\ndef fetch_databases() -> List[str]:\n    return [row.name for row in snowflake_session().sql(f"show databases").collect()]\n\n\n@st.cache_data\ndef fetch_schemas(database: str = "") -> List[str]:\n    return [row.name for row in snowflake_session().sql(f"show schemas in database {database}").collect()]\n\n\n@st.cache_data\ndef fetch_tables(database: str, schema: str) -> List[str]:\n    return [row.name for row in snowflake_session().sql(f"show tables in schema {database}.{schema}").collect()]\n')
    __stickytape_write_module('app_pages/doma_entities_page.py', b'from typing import Callable, List\n\nimport streamlit as st\nfrom streamlit.delta_generator import DeltaGenerator\nfrom streamlit.runtime.state import WidgetCallback\n\nfrom app_pages.page import Page\nfrom data.helpers import repository\nfrom data.metric import Metric\nfrom data.models import Activity, Dimension, Entity, EntityType, Id\nfrom diagrams.entity_diagram import graphviz_entities\n\n\ndef render_dimensions(i: int, item: Dimension, item_type: EntityType) -> DeltaGenerator:\n    col1, col2, col3 = st.columns([10, 1, 1])\n    with col1:\n        name_value = item.name\n        name_key = f"[{item_type}][{i}][name]"\n\n        def get_handler(dim: Dimension, key: str) -> WidgetCallback:\n            def h() -> None:\n                dim.name = st.session_state[key]\n\n            return h\n\n        st.text_input(\n            name_key,\n            label_visibility="collapsed",\n            value=name_value,\n            key=name_key,\n            on_change=get_handler(item, name_key),\n        )\n    with col2:\n        st.write("")\n    return col3\n\n\ndef render_activities(i: int, item: Activity, item_type: EntityType) -> DeltaGenerator:\n    col1, col2, col3, col4 = st.columns([5, 5, 1, 1])\n    with col1:\n        name_value = item.name\n        name_key = f"[{item_type}][{i}][name]"\n\n        def get_handler(act: Activity, key: str) -> WidgetCallback:\n            def h() -> None:\n                act.name = st.session_state[key]\n\n            return h\n\n        st.text_input(\n            name_key,\n            label_visibility="collapsed",\n            value=name_value,\n            key=name_key,\n            on_change=get_handler(item, name_key),\n        )\n\n    with col2:\n        selected_dimensions_key = f"[{item_type}][{i}][dimensions]"\n        selected_dimensions: List[Dimension] = item.dimensions\n\n        dim_map = {dim.uid: dim.name for dim in repository().dimensions}\n\n        options = [dim.uid for dim in repository().dimensions]\n\n        def format_dim(dim_uid: Id) -> str:\n            return dim_map[dim_uid]\n\n        selected_dim_ids = [dim.uid for dim in selected_dimensions]\n\n        def update_activity(dim_ids: List[Id]) -> None:\n            item_dim_ids = [dim.uid for dim in item.dimensions]\n            for dim_id in dim_ids:\n                if dim_id not in item_dim_ids:\n                    item.assign_dimension(dim_id)\n                    print("add")\n            for dim in [dim for dim in repository().dimensions]:\n                if dim.uid not in dim_ids:\n                    item.unassign_dimension(dim)\n                    print("remove", dim.uid)\n\n        st.multiselect(\n            selected_dimensions_key,\n            label_visibility="collapsed",\n            options=options,\n            default=selected_dim_ids,\n            key=selected_dimensions_key,\n            on_change=lambda: update_activity(st.session_state[selected_dimensions_key]),\n            format_func=format_dim,\n        )\n    with col3:\n        st.write("")\n    return col4\n\n\ndef render_metrics(i: int, item: Metric, item_type: EntityType) -> DeltaGenerator:\n    col1, col2, col3, col4 = st.columns([5, 5, 1, 1])\n    with col1:\n        name_value = item.name\n        name_key = f"[{item_type}][{i}][name]"\n\n        def get_handler(m: Metric, key: str) -> WidgetCallback:\n            def h() -> None:\n                m.name = st.session_state[key]\n\n            return h\n\n        st.text_input(\n            name_key,\n            label_visibility="collapsed",\n            value=name_value,\n            key=name_key,\n            on_change=get_handler(item, name_key),\n        )\n    with col2:\n        selected_activity_key = f"[{item_type}][{i}][activity]"\n\n        act_map = {str(act.uid): act.name for act in repository().activities}\n\n        options = list(act_map.keys())\n\n        def format_act(act_uid: Id) -> str:\n            return act_map[act_uid]\n\n        def update_metric(act_id: Id) -> None:\n            item.act_id = act_id\n\n        st.selectbox(\n            label=selected_activity_key,\n            label_visibility="collapsed",\n            options=options,\n            format_func=format_act,\n            index=options.index(item.act_id) if item.act_id in options else 0,\n            key=selected_activity_key,\n            on_change=lambda: update_metric(st.session_state[selected_activity_key]),\n        )\n\n    with col3:\n        st.write("")\n    return col4\n\n\nRenderer = Callable[[int, Entity, EntityType], DeltaGenerator]\n\n\ndef render_crud(item_type: EntityType, items: List[Entity], render: Renderer, on_create: WidgetCallback) -> None:\n    for i, item in enumerate(items):\n        last_col = render(i, item, item_type)\n\n        def delete_item(entity: Entity) -> WidgetCallback:\n            return lambda: entity.delete()\n\n        last_col.button(f"Delete", key=f"{item_type}{i}_delete_button", on_click=delete_item(item))\n\n    st.button(f"Add {item_type.capitalize()}", on_click=on_create)\n\n\nclass DomaEntitiesPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "This page provides a brief introduction to the DOMA framework and the purpose of the app."\n\n    def run(self) -> None:\n        st.markdown("### Summary")\n        graphviz_entities()\n\n        st.markdown(\n            """\n            Dimensions are attributes that describe characteristics of an object. \n            They are used to segment and filter data to gain insights into specific aspects of your business.\n\n            Some examples of dimensions might include: User, Location, Product, Category\n            """\n        )\n        render_crud(\n            item_type="dimension",\n            items=repository().dimensions,\n            render=render_dimensions,\n            on_create=lambda: Dimension.create(name=""),\n        )\n\n        st.markdown(\n            """\n            Activities are actions that occur within your business.\n            They are used to measure performance and identify areas for improvement.\n            \n            Some examples of activities might include:\n            Sales, Website Visits, Customer, Service Calls\n            """\n        )\n\n        activities = repository().activities\n        render_crud(\n            item_type="activity",\n            items=activities,\n            render=render_activities,\n            on_create=lambda: Activity.create(name=""),\n        )\n\n        st.markdown(\n            """\n            Metrics are numerical values that represent a specific aspect of your business performance.\n            They are calculated from a combination of dimensions and activities.\n            \n            Some examples of metrics might include: Revenue, Average Order Value, Conversion Rate\n            """\n        )\n        render_crud(\n            item_type="metric",\n            items=repository().metrics,\n            render=render_metrics,\n            on_create=lambda: Metric.create(name="", act_id=activities[0].uid if len(activities) else None),\n        )\n')
    __stickytape_write_module('diagrams/__init__.py', b'')
    __stickytape_write_module('diagrams/entity_diagram.py', b'from typing import Dict, List, Union\n\nimport streamlit as st\n\nfrom data.helpers import repository\nfrom data.metric import Metric\nfrom data.models import Activity, DbTable, Dimension, DimensionActivityMapping\n\nENTITY_COLORS = {"activity": "#ffdd6677", "dimension": "#66dd6677", "metric": "#ff335577"}\n\n\ndef entity_color(entity: Union[Activity, Dimension, Metric]) -> str:\n    return ENTITY_COLORS[type(entity).__name__.lower()]\n\n\ndef graphviz_entities() -> None:\n    """Diagram showing relationships between Dimensions Activities and Metrics."""\n    repo = repository()\n\n    nodes = []\n    nodes.extend(f\'"{dimension.name}" [fillcolor="{entity_color(dimension)}"]\' for dimension in repo.dimensions)\n    nodes.extend(f\'"{activity.name}" [fillcolor="{entity_color(activity)}"]\' for activity in repo.activities)\n    nodes.extend(f\'"{metric.name}" [fillcolor="{entity_color(metric)}"]\' for metric in repo.metrics)\n    nodes = "\\n".join(nodes)\n\n    edges = []\n    for activity in repo.activities:\n        edges.extend(f\'"{dim.name}" -> "{activity.name}"\' for dim in activity.dimensions)\n        edges.extend(f\'"{activity.name}" -> "{metric.name}"\' for metric in activity.metrics)\n    edges = "\\n".join(edges)\n\n    graphviz_dot = f"""\n        digraph G {{\n            graph [fontname="Helvetica,Arial,sans-serif", fontsize=20, layout=dot, newrank=true]\n            node [style="rounded,filled", shape=box]\n            edge [arrowsize=0.75]\n\n            {nodes}\n            {edges}\n        }}\n    """\n\n    st.graphviz_chart(graphviz_dot)\n')
    __stickytape_write_module('app_pages/mapping.py', b'from typing import List\n\nimport streamlit as st\nfrom streamlit.runtime.state import WidgetCallback\n\nfrom app_pages.page import Page\nfrom data.helpers import render_validation_component, repository, validate_state\nfrom data.models import Activity, DbTable, DimensionActivityMapping\nfrom diagrams.mapping_diagram import graphviz_table_relations\nfrom range_analysis_app.db import ID_TYPES\n\n\nclass MappingPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "This page provides an allows to configure relations between Activities and Dimensions"\n\n    def run(self) -> None:\n        self.validations()\n\n        st.markdown("Configure relations between Activities and Dimensions.")\n\n        for act_idx, activity in enumerate(repository().activities):\n            if act_idx != 0:\n                st.markdown("---")\n\n            self.render_activity(activity)\n\n    @staticmethod\n    def validations():\n        repo = repository()\n\n        if len(repo.activities) == 0 or len(repo.dimensions) == 0:\n            st.warning("No Activities or Dimensions have been defined. Please go to Entities to define some.")\n            st.stop()\n\n        # TODO: Share state validation between pages\n        dimensions_valid, dimensions_message = validate_state(repo.dimensions, "dimension", validate_each=True)\n        activities_valid, activities_message = validate_state(repo.activities, "activity", validate_each=True)\n\n        if not dimensions_valid or not activities_valid:\n            st.warning("Missing tables for Activities AND Dimensions. Please go to the RAW data page to select tables.")\n\n            render_validation_component(dimensions_valid, dimensions_message)\n            render_validation_component(activities_valid, activities_message)\n            st.stop()\n\n    def render_activity(self, activity: Activity) -> None:\n        activity_table = activity.db_table\n        assert activity_table, "Activity is validated to have a table"\n\n        dimension_tables: List[DbTable] = [dim.db_table for dim in activity.dimensions]\n        assert all(dimension_tables), "Dimensions are validated to have tables"\n\n        st.markdown(f"### Activity {activity.name} (`{activity_table.table_name}`):")\n        if len(activity.mappings) == 0:\n            st.markdown("No dimension to associate.")\n            return\n\n        for mapping in activity.mappings:\n            self.render_mapping(mapping, dimension_tables)\n\n        graphviz_table_relations(activity)\n\n    def render_mapping(self, mapping: DimensionActivityMapping, dimension_tables: List[DbTable]) -> None:\n        activity = mapping.activity\n        dimension = mapping.dimension\n\n        activity_table = activity.db_table\n        mapping_key = f"{activity.uid}_{dimension.uid}"\n\n        current_dim_table = dimension.db_table\n\n        cols = st.columns([3, 3, 1, 3, 3])\n\n        # Default mapping\n        if mapping.main_table is None or mapping.joined_table is None:\n            mapping.main_table_id = activity_table.uid\n            mapping.joined_table_id = current_dim_table.uid\n            mapping.main_column = activity_table.columns_of_type(ID_TYPES)[0]\n            mapping.joined_column = current_dim_table.columns_of_type(ID_TYPES)[0]\n\n        def set_mapping_field(field: str, state_key: str) -> WidgetCallback:\n            return lambda: setattr(mapping, field, st.session_state[state_key])\n\n        with cols[0]:\n            st.selectbox(\n                "Dimension table",\n                options=[mapping.joined_table.table_name],\n                key=f"dimension_{mapping_key}",\n                disabled=True,\n            )\n\n        with cols[1]:\n            joined_table = current_dim_table\n            joined_columns = joined_table.columns_of_type(ID_TYPES)\n\n            st.selectbox(\n                f"Column in `{joined_table.table_name}`",\n                options=joined_columns,\n                index=joined_columns.index(mapping.joined_column),\n                # format_func=lambda col: f"{joined_table.table_name}.{col}",\n                key=f"joined_column_{mapping_key}",\n                on_change=set_mapping_field("joined_column", f"joined_column_{mapping_key}"),\n            )\n\n        with cols[2]:\n            st.markdown("###")\n            st.markdown("#### -----\xe2\x86\x92")\n\n        with cols[3]:\n            other_dim_tables = [dim_table for dim_table in dimension_tables if dim_table != current_dim_table]\n\n            def update_main_table() -> None:\n                selected_table: DbTable = st.session_state[f"main_table_{mapping_key}"]\n                mapping.main_table_id = selected_table.uid\n                mapping.main_column = selected_table.columns_of_type(ID_TYPES)[0]\n\n            tables = [activity_table] + other_dim_tables\n\n            st.selectbox(\n                "Associated with table",\n                options=tables,\n                index=tables.index(mapping.main_table),\n                format_func=lambda table: table.table_name,\n                key=f"main_table_{mapping_key}",\n                on_change=update_main_table,\n            )\n\n        with cols[4]:\n            main_table = mapping.main_table\n            main_columns = main_table.columns_of_type(ID_TYPES)\n\n            st.selectbox(\n                f"Column in `{main_table.table_name}`",\n                options=main_columns,\n                index=main_columns.index(mapping.main_column),\n                # format_func=lambda col: f"{main_table.table_name}.{col}",\n                key=f"main_column_{mapping_key}",\n                on_change=set_mapping_field("main_column", f"main_column_{mapping_key}"),\n            )\n')
    __stickytape_write_module('diagrams/mapping_diagram.py', b'import streamlit as st\n\nfrom data.models import Activity, DbTable, DimensionActivityMapping\nfrom diagrams.entity_diagram import ENTITY_COLORS\n\n\ndef graphviz_table_relations(activity: Activity) -> None:\n    """Entity-Relationship-Diagram between Activity and Dimension tables."""\n    act_table = activity.db_table\n    tables = [act_table] + [dim.db_table for dim in activity.dimensions]\n\n    table_nodes = "\\n".join(_graphviz_table_node(table) for table in tables)\n    table_edges = "\\n".join(_graphviz_edge(mapping) for mapping in activity.mappings)\n\n    graphviz_dot = f"""\n        digraph G {{\n            graph [fontname="Helvetica,Arial,sans-serif", fontsize=20, layout=dot, rankdir=LR, newrank=true]\n            node [style=filled, shape=rect, shape=plaintext]\n            edge [arrowsize=0.75]\n\n            {table_nodes}\n            {table_edges}\n        }}\n    """\n\n    st.graphviz_chart(graphviz_dot, use_container_width=True)\n\n\ndef _graphviz_table_node(table: DbTable) -> str:\n    """Return a html table representing the table and its columns."""\n    col_table_rows = "\\n".join(f"""<tr><td port="{col}">{col}</td></tr>""" for col, _ in table.columns.items())\n    bgcolor = ENTITY_COLORS[table.entity_type]\n\n    label_table = f"""\n        <table border="0" cellborder="1" cellspacing="0">\n            <tr><td bgcolor="{bgcolor}" align="CENTER"><b>{table.table_name}</b></td></tr>\n            {col_table_rows}\n        </table>\n    """\n    return f"""{table.table_name} [fillcolor="#ffffff00" label=<{label_table}>];"""\n\n\ndef _graphviz_edge(mapping: DimensionActivityMapping) -> str:\n    """Return the edge between tables of the mapping."""\n    main_table = mapping.main_table.table_name\n    joined_table = mapping.joined_table.table_name\n\n    return f"{joined_table}:{mapping.joined_column} -> {main_table}:{mapping.main_column}"\n')
    __stickytape_write_module('app_pages/metrics_creation.py', b'from collections import ChainMap\nfrom typing import Dict, List\n\nimport pandas as pd\nimport streamlit as st\nfrom streamlit.runtime.state import WidgetCallback\n\nimport range_analysis_app.db as db\nfrom app_pages.page import Page\nfrom data.helpers import repository\nfrom data.metric import AGG_METHODS, Metric, MetricColumns\nfrom range_analysis_app.components.collapse import collapse\nfrom utils import chunks\n\n\nclass MetricsCreationPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "This page provides a form where users can define metrics as answers to business questions."\n\n    def run(self) -> None:\n        metrics = repository().metrics\n\n        for metric in metrics:\n            if metric != metrics[0]:\n                st.markdown("---")\n\n            form_metric_definition(metric)\n\n\ndef form_metric_definition(metric: Metric) -> None:\n    st.markdown(f"### Metric {metric.name}")\n\n    activity = metric.activity\n    errors = activity.validate_tables_and_mapping()\n    if errors:\n        for error in errors:\n            st.error(error)\n        return\n\n    columns: Dict[str, db.DataType] = {\n        **activity.db_table.columns,\n        **ChainMap(*[dim.db_table.columns for dim in activity.dimensions]),\n    }\n\n    def columns_of_types(data_types: List[db.DataType]) -> List[str]:\n        """Return the columns having one of the given types."""\n        return [col for col, data_type in columns.items() if data_type in data_types]\n\n    metric_columns = columns_of_types(["FIXED", "REAL", "INTEGER"]) or ["<no number column>"]\n    timestamp_columns = columns_of_types(db.DATETIME_TYPES) or ["<no timestamp column>"]\n\n    left, right = st.columns([6, 6])\n\n    with right:\n        if metric.columns is None:\n            metric.columns = MetricColumns(\n                dimensions=[],\n                metric_column=metric_columns[0],\n                timestamp=timestamp_columns[0],\n                agg_method="sum",\n            )\n\n        def set_columns_field(field: str, state_key: str) -> WidgetCallback:\n            return lambda: setattr(metric.columns, field, st.session_state[state_key])\n\n        def set_columns_field_and_update_dimensions(field: str, state_key: str) -> WidgetCallback:\n            def set_column():\n                column = st.session_state[state_key]\n                metric.columns.remove_dimension_columns({column})\n                setattr(metric.columns, field, column)\n\n            return set_column\n\n        st.markdown("##### Columns")\n\n        cols = st.columns([1, 1, 1])\n        cols[0].selectbox(\n            "Metric",\n            metric_columns,\n            index=metric_columns.index(metric.columns.metric_column),\n            key=f"metric-column-{metric.uid}",\n            on_change=set_columns_field_and_update_dimensions("metric_column", f"metric-column-{metric.uid}"),\n            help="Column containing a quantitative value to aggregate",\n        )\n        cols[1].selectbox(\n            "Timestamp",\n            timestamp_columns,\n            index=timestamp_columns.index(metric.columns.timestamp),\n            key=f"timestamp-{metric.uid}",\n            on_change=set_columns_field_and_update_dimensions("timestamp", f"timestamp-{metric.uid}"),\n            help="Column containing the date of the event",\n        )\n        cols[2].selectbox(\n            "Aggregate Method",\n            AGG_METHODS,\n            index=AGG_METHODS.index(metric.columns.agg_method),\n            key=f"agg-method-{metric.uid}",\n            on_change=set_columns_field("agg_method", f"agg-method-{metric.uid}"),\n            help="Method in which the metric column will be aggregated",\n        )\n        dimensions = st.multiselect(\n            "Dimensions",\n            [col for col in columns if col not in [metric.columns.timestamp, metric.columns.metric_column]],\n            default=metric.columns.dimensions,\n            key=f"dimensions-{metric.uid}",\n            on_change=set_columns_field("dimensions", f"dimensions-{metric.uid}"),\n            help="Columns that will be used as filters",\n        )\n\n        if dimensions:\n            form_column_labels(metric, dimensions)\n\n    with left:\n        st.markdown("##### Preview (first 10)")\n        st.dataframe(metric.preview_dataframe(), use_container_width=True)\n\n\ndef form_column_labels(metric: Metric, dimensions: List[str]) -> None:\n    """Text inputs for editing column labels."""\n    st.markdown("##### Labels")\n\n    def set_label(dimension: str, key: str) -> WidgetCallback:\n        def update_labels() -> None:\n            if value := st.session_state[key].strip():\n                metric.columns.labels.update(**{dimension: value})\n            else:\n                metric.columns.labels.pop(dimension, None)\n\n        return update_labels\n\n    NB_COLS = 3\n    for dims in chunks(dimensions, chunk_size=NB_COLS):\n        for dim, col in zip(dims, st.columns([1] * NB_COLS)):\n            col.text_input(\n                f"Label for `{dim}`",\n                value=metric.columns.labels.get(dim, ""),\n                placeholder=dim,\n                key=f"label-{dim}-{metric.uid}",\n                on_change=set_label(dim, f"label-{dim}-{metric.uid}"),\n            )\n')
    __stickytape_write_module('range_analysis_app/components/collapse.py', b'from typing import Optional\n\nimport streamlit as st\n\n\ndef collapse(label: str = "", open_by_default: str = False, key: Optional[str] = None) -> bool:\n    if not key:\n        key = f"collapse-{label}"\n\n    is_open = st.session_state.get(key, open_by_default)\n\n    def switch() -> None:\n        st.session_state[key] = not is_open\n\n    caret = "\xe2\xac\x86\xef\xb8\x8f" if is_open else "\xe2\xac\x87\xef\xb8\x8f"\n    st.button(f"{caret} {label}", on_click=switch, key=f"btn-{key}")\n\n    return is_open\n')
    __stickytape_write_module('app_pages/summary_settings_page.py', b'import streamlit as st\n\nfrom app_pages.page import Page\nfrom data.helpers import render_validation_component, repository, validate_state\n\n\nclass SummarySettingsPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "Summary of settings"\n\n    def run(self) -> None:\n        _repo = repository()\n\n        _repo_dimensions = validate_state(_repo.dimensions, "dimension", validate_each=False)\n        _repo_activities = validate_state(_repo.activities, "activity", validate_each=False)\n        _repo_metrics = validate_state(_repo.metrics, "metric", validate_each=False)\n        _repo_each_dimensions = validate_state(_repo.dimensions, "dimension", validate_each=True)\n        _repo_each_activities = validate_state(_repo.activities, "activity", validate_each=True)\n        _repo_dim_act_mappings = validate_state(_repo.dim_act_mappings, "mapping column", permit_empty=True)\n        _repo_each_metrics = validate_state(_repo.metrics, "metric", validate_each=True)\n\n        col_entities, col_raw_data, col_mapping, col_metrics = st.columns(4)\n\n        with col_entities:\n            st.subheader("1. Entities")\n\n            render_validation_component(*_repo_dimensions)\n            render_validation_component(*_repo_activities)\n            render_validation_component(*_repo_metrics)\n\n        with col_raw_data:\n            st.subheader("2. RAW ERP Data")\n\n            render_validation_component(*_repo_each_dimensions)\n            render_validation_component(*_repo_each_activities)\n\n        with col_mapping:\n            st.subheader("3. Mapping")\n\n            render_validation_component(*_repo_dim_act_mappings)\n\n        with col_metrics:\n            st.subheader("4. Metrics")\n\n            render_validation_component(*_repo_each_metrics)\n')
    __stickytape_write_module('app_pages/settings_page.py', b'from typing import Dict, Type\n\nimport streamlit as st\n\nfrom app_pages.data_loading import DataLoadingPage\nfrom app_pages.doma_entities_page import DomaEntitiesPage\nfrom app_pages.mapping import MappingPage\nfrom app_pages.metrics_creation import MetricsCreationPage\nfrom app_pages.page import Page\nfrom app_pages.summary_settings_page import SummarySettingsPage\n\n\nclass SettingsPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "Settings"\n\n    def run(self) -> None:\n        settings_pages: Dict[str, Type[Page]] = {\n            "Summary": SummarySettingsPage,\n            "Entities": DomaEntitiesPage,\n            "RAW Data": DataLoadingPage,\n            "Mapping": MappingPage,\n            "Metrics": MetricsCreationPage,\n        }\n\n        for tab, page in zip(st.tabs(settings_pages.keys()), settings_pages.values()):\n            with tab:\n                page().run()\n')
    __stickytape_write_module('streamlit_in_snowflake.py', b'import base64\nfrom pathlib import Path\n\nimport streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.exceptions import SnowparkSessionException\n\n\n@st.cache_resource\ndef is_inside_snowflake() -> bool:\n    """Return true if Streamlit is executed within Snowflake."""\n    try:\n        _ = get_active_session()\n        return True\n    except SnowparkSessionException:\n        return False\n\n\ndef st_image_base64(image_file: str, folder: Path = Path("images"), **kwargs):\n    """Streamlit image using base64 that can be displayed within Snowflake."""\n    if is_inside_snowflake():\n        folder = Path(".")\n\n    content_bytes = (folder / image_file).read_bytes()\n\n    mime_type = image_file.split(".")[-1:][0].lower()\n    st.image(f"data:image/{mime_type};base64,{base64.b64encode(content_bytes).decode()}", **kwargs)\n\n\ndef snowflake_ui_styles():\n    """Replicate the CSS styles when Streamlit is executed within Snowflake."""\n    if is_inside_snowflake():\n        return\n\n    st.markdown(\n        """\n        <style>\n        \n        /* Hide "Made by Streamlit" */\n        .appview-container .main footer {\n            display: none;\n        }\n        \n        /* Hide background of the top header bar */\n        .stApp > header, .stApp > header > div[data-testid="stDecoration"] {\n            background: #ffffff00;\n        }\n\n        /* Reduce top padding */\n        .main > .block-container {\n            padding-top: 1rem;  /* default = 6rem */\n        }\n        \n        </style>\n        """,\n        unsafe_allow_html=True,\n    )\n')
    __stickytape_write_module('sample_configuration.py', b'import datetime\n\nimport streamlit as st\n\nsample_data = st.secrets.get("sample-database", {"database": "STREAMLIT_APPS_ADRIEN", "schema": "DATA"})\n\nsample_config = {\n    "dimensions": [\n        {"uid": "be4e41ae-b4fe-499d-b3cd-c871fe30d63e", "name": "Product"},\n        {"uid": "ee27ccc6-d974-47f6-8180-738358d4a5cc", "name": "Category"},\n        {"uid": "c63a8eee-381b-4b67-a23d-41dddcbc89db", "name": "Supplier"},\n        {"uid": "9e84b28a-f02d-46af-9d49-d53c5fae225e", "name": "Customer"},\n    ],\n    "activities": [{"uid": "6881ad6b-3740-4ab3-af76-2f0c32752949", "name": "Purchase orders"}],\n    "metrics": [\n        {\n            "uid": "ca59a704-eceb-4a9b-ad18-5cc01a72a207",\n            "name": "Revenue (USD)",\n            "act_id": "6881ad6b-3740-4ab3-af76-2f0c32752949",\n            "columns": {\n                "dimensions": ["CATEGORY", "REGION"],\n                "metric_column": "PRICE",\n                "timestamp": "PURCHASE_DATE",\n                "agg_method": "sum",\n                "labels": {"PRODUCT_NAME": "Product", "CATEGORY": "Product Category", "REGION": "Region"},\n            },\n        }\n    ],\n    "selected_tables": [\n        {\n            "uid": "b4345c52-5e77-4903-83b1-f92c87723b2c",\n            "table_name": "PRODUCT",\n            "table_schema": "DATA",\n            "table_database": "STREAMLIT_APPS_ADRIEN",\n            "entity_uid": "be4e41ae-b4fe-499d-b3cd-c871fe30d63e",\n            "entity_type": "dimension",\n            "columns": {"PRODUCT_ID": "INTEGER", "PRODUCT_NAME": "TEXT", "CATEGORY_ID": "INTEGER", "PRICE": "INTEGER"},\n        },\n        {\n            "uid": "97135b10-ec63-456b-ad9f-89cb92be935f",\n            "table_name": "CATEGORY",\n            "table_schema": "DATA",\n            "table_database": "STREAMLIT_APPS_ADRIEN",\n            "entity_uid": "ee27ccc6-d974-47f6-8180-738358d4a5cc",\n            "entity_type": "dimension",\n            "columns": {"CATEGORY_ID": "INTEGER", "CATEGORY": "TEXT"},\n        },\n        {\n            "uid": "672b42b1-8fbe-4532-982e-0b58ab99e43a",\n            "table_name": "SUPPLIER",\n            "table_schema": "DATA",\n            "table_database": "STREAMLIT_APPS_ADRIEN",\n            "entity_uid": "c63a8eee-381b-4b67-a23d-41dddcbc89db",\n            "entity_type": "dimension",\n            "columns": {\n                "SUPPLIER_ID": "INTEGER",\n                "SUPPLIER_NAME": "TEXT",\n                "SUPPLIER_PHONE": "TEXT",\n                "SUPPLIER_ATTRIBUTES": "TEXT",\n            },\n        },\n        {\n            "uid": "3f6a2134-4551-4c49-a7e0-5d5e0e625073",\n            "table_name": "CUSTOMER",\n            "table_schema": "DATA",\n            "table_database": "STREAMLIT_APPS_ADRIEN",\n            "entity_uid": "9e84b28a-f02d-46af-9d49-d53c5fae225e",\n            "entity_type": "dimension",\n            "columns": {"CUSTOMER_ID": "INTEGER", "CUSTOMER_NAME": "TEXT", "CUSTOMER_PHONE": "TEXT", "REGION": "TEXT"},\n        },\n        {\n            "uid": "382055a6-a3ce-4321-83b6-d8ce13987685",\n            "table_name": "PURCHASE_ORDER",\n            "table_schema": "DATA",\n            "table_database": "STREAMLIT_APPS_ADRIEN",\n            "entity_uid": "6881ad6b-3740-4ab3-af76-2f0c32752949",\n            "entity_type": "activity",\n            "columns": {\n                "ORDER_ID": "INTEGER",\n                "PURCHASE_DATE": "DATE",\n                "PRODUCT_ID": "INTEGER",\n                "QUANTITY": "INTEGER",\n                "PRICE": "INTEGER",\n                "CUSTOMER_ID": "INTEGER",\n                "SUPPLIER_ID": "INTEGER",\n                "STATUS": "TEXT",\n            },\n        },\n    ],\n    "dim_act_mappings": [\n        {\n            "dim_id": "be4e41ae-b4fe-499d-b3cd-c871fe30d63e",\n            "act_id": "6881ad6b-3740-4ab3-af76-2f0c32752949",\n            "main_table_id": "382055a6-a3ce-4321-83b6-d8ce13987685",\n            "joined_table_id": "b4345c52-5e77-4903-83b1-f92c87723b2c",\n            "main_column": "PRODUCT_ID",\n            "joined_column": "PRODUCT_ID",\n        },\n        {\n            "dim_id": "ee27ccc6-d974-47f6-8180-738358d4a5cc",\n            "act_id": "6881ad6b-3740-4ab3-af76-2f0c32752949",\n            "main_table_id": "b4345c52-5e77-4903-83b1-f92c87723b2c",\n            "joined_table_id": "97135b10-ec63-456b-ad9f-89cb92be935f",\n            "main_column": "CATEGORY_ID",\n            "joined_column": "CATEGORY_ID",\n        },\n        {\n            "dim_id": "c63a8eee-381b-4b67-a23d-41dddcbc89db",\n            "act_id": "6881ad6b-3740-4ab3-af76-2f0c32752949",\n            "main_table_id": "382055a6-a3ce-4321-83b6-d8ce13987685",\n            "joined_table_id": "672b42b1-8fbe-4532-982e-0b58ab99e43a",\n            "main_column": "SUPPLIER_ID",\n            "joined_column": "SUPPLIER_ID",\n        },\n        {\n            "dim_id": "9e84b28a-f02d-46af-9d49-d53c5fae225e",\n            "act_id": "6881ad6b-3740-4ab3-af76-2f0c32752949",\n            "main_table_id": "382055a6-a3ce-4321-83b6-d8ce13987685",\n            "joined_table_id": "3f6a2134-4551-4c49-a7e0-5d5e0e625073",\n            "main_column": "CUSTOMER_ID",\n            "joined_column": "CUSTOMER_ID",\n        },\n    ],\n    "visualizations": {},\n    "widgets": [\n        {\n            "widget_kind": "kpi",\n            "size": "small",\n            "title": "",\n            "widget_type": "TotalPurchaseValue",\n            "metric": {\n                "uid": "ca59a704-eceb-4a9b-ad18-5cc01a72a207",\n                "name": "Revenue (USD)",\n                "act_id": "6881ad6b-3740-4ab3-af76-2f0c32752949",\n                "columns": {\n                    "dimensions": ["CATEGORY", "REGION"],\n                    "metric_column": "PRICE",\n                    "timestamp": "PURCHASE_DATE",\n                    "agg_method": "sum",\n                    "labels": {"PRODUCT_NAME": "Product", "CATEGORY": "Product Category", "REGION": "Region"},\n                },\n            },\n            "grain": "year",\n            "period": {"start": datetime.date(2019, 1, 1), "end": datetime.date(2023, 12, 31)},\n            "filters": {"mapping": {}},\n        },\n        {\n            "widget_kind": "kpi",\n            "size": "small",\n            "title": "",\n            "widget_type": "TotalPurchaseValue",\n            "metric": {\n                "uid": "ca59a704-eceb-4a9b-ad18-5cc01a72a207",\n                "name": "Revenue (USD)",\n                "act_id": "6881ad6b-3740-4ab3-af76-2f0c32752949",\n                "columns": {\n                    "dimensions": ["CATEGORY", "REGION"],\n                    "metric_column": "PRICE",\n                    "timestamp": "PURCHASE_DATE",\n                    "agg_method": "sum",\n                    "labels": {"PRODUCT_NAME": "Product", "CATEGORY": "Product Category", "REGION": "Region"},\n                },\n            },\n            "grain": "month",\n            "period": {"start": datetime.date(2019, 1, 1), "end": datetime.date(2022, 12, 31)},\n            "filters": {"mapping": {}},\n        },\n        {\n            "widget_kind": "kpi",\n            "size": "small",\n            "title": "",\n            "widget_type": "TotalPurchaseValue",\n            "metric": {\n                "uid": "ca59a704-eceb-4a9b-ad18-5cc01a72a207",\n                "name": "Revenue (USD)",\n                "act_id": "6881ad6b-3740-4ab3-af76-2f0c32752949",\n                "columns": {\n                    "dimensions": ["CATEGORY", "REGION"],\n                    "metric_column": "PRICE",\n                    "timestamp": "PURCHASE_DATE",\n                    "agg_method": "sum",\n                    "labels": {"PRODUCT_NAME": "Product", "CATEGORY": "Product Category", "REGION": "Region"},\n                },\n            },\n            "grain": "week",\n            "period": {"start": datetime.date(2019, 1, 1), "end": datetime.date(2022, 12, 31)},\n            "filters": {"mapping": {}},\n        },\n        {\n            "widget_kind": "kpi",\n            "size": "small",\n            "title": "",\n            "widget_type": "TotalPurchaseValue",\n            "metric": {\n                "uid": "ca59a704-eceb-4a9b-ad18-5cc01a72a207",\n                "name": "Revenue (USD)",\n                "act_id": "6881ad6b-3740-4ab3-af76-2f0c32752949",\n                "columns": {\n                    "dimensions": ["CATEGORY", "REGION"],\n                    "metric_column": "PRICE",\n                    "timestamp": "PURCHASE_DATE",\n                    "agg_method": "sum",\n                    "labels": {"PRODUCT_NAME": "Product", "CATEGORY": "Product Category", "REGION": "Region"},\n                },\n            },\n            "grain": "day",\n            "period": {"start": datetime.date(2019, 1, 1), "end": datetime.date(2023, 12, 31)},\n            "filters": {"mapping": {}},\n        },\n        {\n            "widget_kind": "chart",\n            "size": "small",\n            "title": "Distribution 2022",\n            "widget_type": "ProductCategoryDistributionPieChart",\n            "metric": {\n                "uid": "ca59a704-eceb-4a9b-ad18-5cc01a72a207",\n                "name": "Revenue (USD)",\n                "act_id": "6881ad6b-3740-4ab3-af76-2f0c32752949",\n                "columns": {\n                    "dimensions": ["CATEGORY", "REGION"],\n                    "metric_column": "PRICE",\n                    "timestamp": "PURCHASE_DATE",\n                    "agg_method": "sum",\n                    "labels": {"PRODUCT_NAME": "Product", "CATEGORY": "Product Category", "REGION": "Region"},\n                },\n            },\n            "category_column": "CATEGORY",\n            "textinfo": "percent",\n            "period": {"start": datetime.date(2022, 1, 1), "end": datetime.date(2022, 12, 31)},\n            "filters": {"mapping": {}},\n        },\n        {\n            "widget_kind": "chart",\n            "size": "small",\n            "title": "Categories - CA",\n            "widget_type": "ProductCategoryDistributionPieChart",\n            "metric": {\n                "uid": "ca59a704-eceb-4a9b-ad18-5cc01a72a207",\n                "name": "Revenue (USD)",\n                "act_id": "6881ad6b-3740-4ab3-af76-2f0c32752949",\n                "columns": {\n                    "dimensions": ["CATEGORY", "REGION"],\n                    "metric_column": "PRICE",\n                    "timestamp": "PURCHASE_DATE",\n                    "agg_method": "sum",\n                    "labels": {"PRODUCT_NAME": "Product", "CATEGORY": "Product Category", "REGION": "Region"},\n                },\n            },\n            "category_column": "CATEGORY",\n            "textinfo": "value",\n            "period": {"start": datetime.date(2019, 1, 1), "end": datetime.date(2023, 12, 31)},\n            "filters": {"mapping": {"REGION": ["CA"]}},\n        },\n        {\n            "widget_kind": "chart",\n            "size": "medium",\n            "title": "",\n            "widget_type": "MonthlyPurchasingTrendsLineChart",\n            "metric": {\n                "uid": "ca59a704-eceb-4a9b-ad18-5cc01a72a207",\n                "name": "Revenue (USD)",\n                "act_id": "6881ad6b-3740-4ab3-af76-2f0c32752949",\n                "columns": {\n                    "dimensions": ["CATEGORY", "REGION"],\n                    "metric_column": "PRICE",\n                    "timestamp": "PURCHASE_DATE",\n                    "agg_method": "sum",\n                    "labels": {"PRODUCT_NAME": "Product", "CATEGORY": "Product Category", "REGION": "Region"},\n                },\n            },\n            "grain": "month",\n            "period": {"start": datetime.date(2019, 5, 25), "end": datetime.date(2023, 5, 24)},\n            "filters": {"mapping": {}},\n        },\n        {\n            "widget_kind": "chart",\n            "size": "medium",\n            "title": "",\n            "widget_type": "ProductCategoriesComparisonBarGraph",\n            "metric": {\n                "uid": "ca59a704-eceb-4a9b-ad18-5cc01a72a207",\n                "name": "Revenue (USD)",\n                "act_id": "6881ad6b-3740-4ab3-af76-2f0c32752949",\n                "columns": {\n                    "dimensions": ["CATEGORY", "REGION"],\n                    "metric_column": "PRICE",\n                    "timestamp": "PURCHASE_DATE",\n                    "agg_method": "sum",\n                    "labels": {"PRODUCT_NAME": "Product", "CATEGORY": "Product Category", "REGION": "Region"},\n                },\n            },\n            "category_column": "CATEGORY",\n            "year_a": 2019,\n            "year_b": 2021,\n            "filters": {"mapping": {}},\n        },\n        {\n            "widget_kind": "chart",\n            "size": "large",\n            "title": "",\n            "widget_type": "PurchaseAnomaliesHeatmap",\n            "metric": {\n                "uid": "ca59a704-eceb-4a9b-ad18-5cc01a72a207",\n                "name": "Revenue (USD)",\n                "act_id": "6881ad6b-3740-4ab3-af76-2f0c32752949",\n                "columns": {\n                    "dimensions": ["CATEGORY", "REGION"],\n                    "metric_column": "PRICE",\n                    "timestamp": "PURCHASE_DATE",\n                    "agg_method": "sum",\n                    "labels": {"PRODUCT_NAME": "Product", "CATEGORY": "Product Category", "REGION": "Region"},\n                },\n            },\n            "x_col": "month",\n            "y_col": "CATEGORY",\n            "period": {"start": datetime.date(2019, 5, 25), "end": datetime.date(2023, 5, 24)},\n            "filters": {"mapping": {}},\n        },\n    ],\n}\n')
    import json
    from pathlib import Path
    
    import numpy as np
    import streamlit as st
    
    # noinspection PyUnresolvedReferences
    import range_analysis_app.components.streamlit_nested_layout  # import for nested columns support
    from app_pages.about_page import AboutPage
    from app_pages.dashboard import DashboardPage
    from app_pages.help_page import HelpPage
    from app_pages.settings_page import SettingsPage
    from data.helpers import save_repo_button
    from data.repository import Repository
    from streamlit_in_snowflake import snowflake_ui_styles, st_image_base64
    
    np.random.seed(seed=42)
    st.set_page_config(layout="wide", initial_sidebar_state="expanded")
    
    snowflake_ui_styles()
    
    st_image_base64("bar.png", use_column_width=True)
    
    title_col, save_button_col = st.columns([9, 3])
    with title_col:
        st_image_base64("banner.png", width=400)
    with save_button_col:
        save_repo_button()
    
    
    # Define the app's pages
    pages = {
        "Dashboard": DashboardPage,
        "Configuration": SettingsPage,
        "Learn": HelpPage,
        "Support": HelpPage,
        "About Maxa": AboutPage,
    }
    
    if st.sidebar.button("Reset to sample Data", use_container_width=True):
        from sample_configuration import sample_config
    
        st.session_state.repository = Repository.parse_obj(sample_config)
    
    
    for tab, page in zip(st.tabs(pages.keys()), pages.values()):
        with tab:
            page().run()
    