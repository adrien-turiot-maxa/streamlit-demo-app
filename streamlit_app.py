#!/usr/bin/env python
import contextlib as __stickytape_contextlib

@__stickytape_contextlib.contextmanager
def __stickytape_temporary_dir():
    import tempfile
    import shutil
    dir_path = tempfile.mkdtemp()
    try:
        yield dir_path
    finally:
        shutil.rmtree(dir_path)

with __stickytape_temporary_dir() as __stickytape_working_dir:
    def __stickytape_write_module(path, contents):
        import os, os.path

        def make_package(path):
            parts = path.split("/")
            partial_path = __stickytape_working_dir
            for part in parts:
                partial_path = os.path.join(partial_path, part)
                if not os.path.exists(partial_path):
                    os.mkdir(partial_path)
                    with open(os.path.join(partial_path, "__init__.py"), "wb") as f:
                        f.write(b"\n")

        make_package(os.path.dirname(path))

        full_path = os.path.join(__stickytape_working_dir, path)
        with open(full_path, "wb") as module_file:
            module_file.write(contents)

    import sys as __stickytape_sys
    __stickytape_sys.path.insert(0, __stickytape_working_dir)

    __stickytape_write_module('range_analysis_app/__init__.py', b'')
    __stickytape_write_module('range_analysis_app/components/__init__.py', b'')
    __stickytape_write_module('range_analysis_app/components/streamlit_nested_layout.py', b'# Implementation from streamlit-nested-layout => https://github.com/joy13975/streamlit-nested-layout\n# This package is not available in the Snowflake Anaconda Channel => https://repo.anaconda.com/pkgs/snowflake/\n\nfrom streamlit.delta_generator import *\nfrom streamlit.delta_generator import _enqueue_message\n\n\ndef _nestable_block(\n    self: "DeltaGenerator",\n    block_proto: Block_pb2.Block = Block_pb2.Block(),\n) -> "DeltaGenerator":\n    # Operate on the active DeltaGenerator, in case we\'re in a `with` block.\n    dg = self._active_dg\n\n    # Prevent nested columns & expanders by checking all parents.\n    block_type = block_proto.WhichOneof("type")\n    # Convert the generator to a list, so we can use it multiple times.\n    # parent_block_types = frozenset(dg._parent_block_types)\n    # if block_type == "column" and block_type in parent_block_types:\n    #     raise StreamlitAPIException(\n    #         "Columns may not be nested inside other columns."\n    #     )\n    # if block_type == "expandable" and block_type in parent_block_types:\n    #     raise StreamlitAPIException(\n    #         "Expanders may not be nested inside other expanders."\n    #     )\n\n    if dg._root_container is None or dg._cursor is None:\n        return dg\n\n    msg = ForwardMsg_pb2.ForwardMsg()\n    msg.metadata.delta_path[:] = dg._cursor.delta_path\n    msg.delta.add_block.CopyFrom(block_proto)\n\n    # Normally we\'d return a new DeltaGenerator that uses the locked cursor\n    # below. But in this case we want to return a DeltaGenerator that uses\n    # a brand new cursor for this new block we\'re creating.\n    block_cursor = cursor.RunningCursor(\n        root_container=dg._root_container,\n        parent_path=dg._cursor.parent_path + (dg._cursor.index,),\n    )\n    block_dg = DeltaGenerator(\n        root_container=dg._root_container,\n        cursor=block_cursor,\n        parent=dg,\n        block_type=block_type,\n    )\n    # Blocks inherit their parent form ids.\n    # NOTE: Container form ids aren\'t set in proto.\n    block_dg._form_data = FormData(current_form_id(dg))\n\n    # Must be called to increment this cursor\'s index.\n    dg._cursor.get_locked_cursor(last_index=None)\n    _enqueue_message(msg)\n\n    caching.save_block_message(\n        block_proto,\n        invoked_dg_id=self.id,\n        used_dg_id=dg.id,\n        returned_dg_id=block_dg.id,\n    )\n\n    return block_dg\n\n\nDeltaGenerator._block = _nestable_block\n')
    __stickytape_write_module('app_pages/__init__.py', b'')
    __stickytape_write_module('app_pages/data_analysis.py', b'import streamlit as st\n\nfrom app_pages.page import Page, button_go_to_page\nfrom data.helpers import repository, save_repo_button\nfrom range_analysis_app.exploration.range_comparison_exploration import range_comparison_exploration\nfrom range_analysis_app.exploration.time_series_exploration import time_series_exploration, render_min_max_metrics, \\\n    render_min_max_metrics_2\n\n\nclass DataAnalysisPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "This page performs the required data analysis based on the metrics defined by the user and displays the results using data visualization tools like Plotly or Matplotlib."\n\n    def run(self) -> None:\n        super().run()\n\n        st.title("Data Analysis and Visualization")\n        button_go_to_page("Home", label="Go Home")\n\n        metrics = [metric for metric in repository().metrics if metric.columns is not None]\n\n        if len(metrics) == 0:\n            st.warning("You don\'t have defined metrics. Please go to the Metric Creation page and create some.")\n            button_go_to_page("Metrics", label="Go to Metrics creation")\n            return\n\n        col1, col2 = st.columns([1, 2])\n\n        with col1:\n            selected_metric = st.selectbox("Metric", metrics, format_func=lambda metric: metric.name)\n            visualizations = {\n                "Time series": time_series_exploration,\n                "Range comparison": range_comparison_exploration,\n            }\n            selected_visualization = st.selectbox("Visualization", visualizations)\n\n        with col2:\n            mcol1, mcol2 = st.columns(2)\n            with mcol1:\n                render_min_max_metrics_2(selected_metric, selected_metric.columns.metric_column)\n            with mcol2:\n                render_min_max_metrics_2(selected_metric, selected_metric.columns.timestamp)\n\n        visualizations[selected_visualization](selected_metric)\n\n        save_repo_button(next_page="Home")\n')
    __stickytape_write_module('app_pages/page.py', b'import uuid\nfrom abc import ABC, abstractmethod\nfrom typing import Optional\n\nimport streamlit as st\n\nfrom utils import navigate_to_page\n\n\nclass Page(ABC):\n    def page_header(self):\n        from app_pages.data_analysis import DataAnalysisPage\n        from app_pages.data_loading import DataLoadingPage\n        from app_pages.doma_entities_page import DomaEntitiesPage\n        from app_pages.mapping import MappingPage\n        from app_pages.metrics_creation import MetricsCreationPage\n\n        # st.button("Home", on_click=navigate_to_page, key=uuid.uuid4().hex, args=[HomePage])\n        st.button("DOMA entities", on_click=navigate_to_page, key=uuid.uuid4().hex, args=[DomaEntitiesPage])\n        st.button("Data Loading", on_click=navigate_to_page, key=uuid.uuid4().hex, args=[DataLoadingPage])\n        st.button("Mapping", on_click=navigate_to_page, key=uuid.uuid4().hex, args=[MappingPage])\n        st.button("Metrics Creation", on_click=navigate_to_page, key=uuid.uuid4().hex, args=[MetricsCreationPage])\n        st.button(\n            "Data Analysis and Visualization", on_click=navigate_to_page, key=uuid.uuid4().hex, args=[DataAnalysisPage]\n        )\n\n    @property\n    @abstractmethod\n    def page_description(self) -> str:\n        pass\n\n    @abstractmethod\n    def run(self) -> None:\n        # self.page_header()\n        pass\n\n\ndef button_go_to_page(page: str, label: Optional[str] = None):\n    st.button(\n        label=label or page,\n        on_click=lambda: st.session_state.update(current_page=page),\n        key=f"btn-go-to-{page}-{label}",\n    )\n')
    __stickytape_write_module('utils.py', b'from typing import Iterable, List, TypeVar\n\nimport streamlit as st\nfrom inflection import pluralize as inflection_pluralize\n\nT = TypeVar("T")\n\n\ndef pluralize(count: int, singular: str) -> str:\n    return f"{count} {inflection_pluralize(singular) if count > 1 else singular}"\n\n\ndef chunks(values: List[T], chunk_size: int) -> Iterable[List[T]]:\n    """Split the given values into multiple chunks of the given size."""\n    for index in range(0, len(values), chunk_size):\n        yield values[index: index + chunk_size]\n\n\ndef navigate_to_page(page):\n    st.empty()\n    _selected_page = page()\n    # st.sidebar.write(_selected_page.page_description)\n    # Call the selected page\'s function\n    _selected_page.run()\n')
    __stickytape_write_module('app_pages/data_loading.py', b'from typing import Dict, List\n\nimport streamlit as st\nfrom streamlit.runtime.state import WidgetCallback\n\nfrom app_pages.page import Page, button_go_to_page\nfrom data.db import fetch_databases, fetch_schemas, fetch_tables\nfrom data.helpers import repository, save_repo_button\nfrom data.models import DbTable, Entity, Id\nfrom data.repository import Repository\nfrom range_analysis_app import db\nfrom range_analysis_app.components.collapse import collapse\n\n\nclass DataLoadingPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "This page allows users to select the tables and map they to DOMA entities"\n\n    def run(self) -> None:\n        super().run()\n\n        st.title("Data Loading")\n        button_go_to_page("Home", label="Go Home")\n\n        repo = repository()\n\n        dimensions_labels = {str(dim.uid): f\'Dimension: "{dim.name}"\' for dim in repo.dimensions}\n        activities_labels = {str(act.uid): f\'Activity: "{act.name}"\' for act in repo.activities}\n        labels = {**dimensions_labels, **activities_labels}\n        entities = repo.dimensions + repo.activities\n        entities_map = {str(e.uid): e for e in entities}\n        mapped_entity_ids = [t.entity_uid for t in repo.selected_tables]\n        unmapped_entities: List[Entity] = [e for e in entities if e.uid not in mapped_entity_ids]\n\n        options = [uid for uid in entities_map.keys() if uid not in mapped_entity_ids]\n\n        if len(entities) == 0:\n            st.warning("You don\'t have Dimensions or Activities. Please go to the Entities page and configure it")\n            button_go_to_page("Entities", label="Go to Entities")\n            return\n\n        st.markdown(\n            """\n            Please select tables corresponding to DOMA entities:\n            1. Choose a Database and Schema.\n            2. Choose a table to map to a DOMA entity.\n            3. Select the appropriate DOMA entity from the \'Map with DOMA Entity\' dropdown.\n            4. Click the \'Add to selected tables \xe2\x9e\xa1\xef\xb8\x8f\' button.\n            5. Verify the table is now listed under \'Selected tables\' on the right side.\n            """\n        )\n\n        left, right = st.columns([2, 2])\n\n        with left:\n            self.source_tables(options=options, labels=labels, entities_map=entities_map)\n        with right:\n            self.selected_tables(repo=repo, unmapped_entities=unmapped_entities)\n\n        save_repo_button(next_page="Mapping")\n\n    def source_tables(self, options: List[str], labels: Dict[str, str], entities_map: Dict[Id, Entity]) -> None:\n        st.markdown("#### Source tables")\n        cols = st.columns([1, 1])\n        selected_database = cols[0].selectbox("Database", key="selected_database", options=fetch_databases())\n        selected_schema = cols[1].selectbox("Schema", key="selected_schema", options=fetch_schemas(selected_database))\n        tables = fetch_tables(selected_database, selected_schema)\n        if len(tables) == 0:\n            st.write(f"There is no tables at `{selected_database}.{selected_schema}`")\n            return\n        selected_table = cols[0].selectbox("Table", key="selected_table", options=tables)\n\n        entity_uid = cols[1].selectbox(\n            label="Map with DOMA Entity",\n            options=options,\n            format_func=lambda uid: labels.get(uid),\n        )\n        full_selected_table = f"{selected_database}.{selected_schema}.{selected_table}"\n\n        columns = db.fetch_columns(full_selected_table)\n\n        st.button(\n            "Add to selected tables \xe2\x9e\xa1\xef\xb8\x8f",\n            disabled=len(options) == 0,\n            on_click=lambda: DbTable.create(\n                table_database=selected_database,\n                table_schema=selected_schema,\n                table_name=selected_table,\n                columns=columns,\n                entity=entities_map[entity_uid],\n            ),\n        )\n\n        st.write(f"Full table name: `{full_selected_table}`")\n        if collapse(f"Preview table"):\n            st.dataframe(db.preview_table(full_selected_table))\n\n    def selected_tables(self, repo: Repository, unmapped_entities: List[Entity]) -> None:\n        if unmapped_entities:\n            st.write("#### Unmapped entities")\n            st.write("\\n".join([f" - {e.name}" for e in unmapped_entities]))\n\n        st.markdown("#### Selected tables")\n        if not repo.selected_tables:\n            st.write("No tables selected yet.")\n            return\n\n        selected_tables: List[DbTable] = repo.selected_tables\n\n        for i, table in enumerate(selected_tables):\n            entity_name = repo.get_entity(table.entity_type, table.entity_uid).name\n            title = f"Table {table.table_name} - {table.entity_type.capitalize()} {entity_name} "\n            with st.expander(title):\n\n                def delete_item(index: int) -> WidgetCallback:\n                    return lambda: selected_tables.pop(index)\n\n                st.button(f"Delete", key=f"remove_btn_{i}", on_click=delete_item(i))\n')
    __stickytape_write_module('data/__init__.py', b'')
    __stickytape_write_module('data/db.py', b'from typing import List\n\nfrom range_analysis_app.caching import cache_in_session_state\nfrom range_analysis_app.db import snowflake_session\n\n\n@cache_in_session_state\ndef fetch_databases() -> List[str]:\n    return [row.name for row in snowflake_session().sql(f"show databases").collect()]\n\n\n@cache_in_session_state\ndef fetch_schemas(database: str = "") -> List[str]:\n    return [row.name for row in snowflake_session().sql(f"show schemas in database {database}").collect()]\n\n\n@cache_in_session_state\ndef fetch_tables(database: str, schema: str) -> List[str]:\n    return [row.name for row in snowflake_session().sql(f"show tables in schema {database}.{schema}").collect()]\n')
    __stickytape_write_module('range_analysis_app/caching.py', b'from collections import defaultdict\nfrom typing import Any, Callable, Dict, TypeVar\n\nimport streamlit as st\nfrom cachetools import LRUCache\n\nCACHE_SIZE = 5\n\n\nTFunction = TypeVar("TFunction", bound=Callable)\n\n\ndef cache_in_session_state(func: TFunction) -> TFunction:\n    """Decorate a function and cache its results in the st.session_state. Arguments must be hashable.\n\n    TODO: Replace by st.cache_data or st.cache_resource once available\n    """\n\n    def cached_func(*args: Any, **kwargs: Any) -> Any:\n        if "caches" not in st.session_state:\n            st.session_state.caches: Dict[str, LRUCache] = defaultdict(lambda: LRUCache(maxsize=CACHE_SIZE))\n\n        function_cache = st.session_state.caches[func.__name__]\n        args_key = (*args, *frozenset(kwargs.items()))\n\n        if function_cache.get(args_key) is None:\n            with st.spinner("Loading..."):\n                function_cache[args_key] = func(*args, **kwargs)\n\n        return function_cache[args_key]\n\n    cached_func.clear_cache = lambda: st.session_state.caches[func.__name__].clear()\n    cached_func.execute_without_cache = func\n\n    return cached_func\n')
    __stickytape_write_module('range_analysis_app/db.py', b'import json\nfrom typing import Dict, Iterable, List, Literal, Tuple, Union, get_args\n\nimport pandas as pd\nimport snowflake.snowpark.functions as f\nimport streamlit as st\nfrom pydantic import BaseModel, Field\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.exceptions import SnowparkSessionException\n\nfrom range_analysis_app.caching import cache_in_session_state\n\n\n@st.cache_resource\ndef snowflake_session() -> Session:\n    try:\n        return get_active_session()\n    except SnowparkSessionException:\n        return Session.builder.configs(st.secrets["snowflake"]).create()\n\n\n@cache_in_session_state\ndef fetch_tables(schema: str = "") -> List[str]:\n    return [row.name for row in snowflake_session().sql(f"show tables in schema {schema}").collect()]\n\n\nDatetimeType = Literal["DATE", "TIME", "TIMESTAMP", "TIMESTAMP_LTZ", "TIMESTAMP_NTZ"]\nDataType = Union[Literal["TEXT", "FIXED", "REAL", "BINARY", "BOOLEAN", "ARRAY", "OBJECT"], DatetimeType]\n\nDATETIME_TYPES: Iterable[DatetimeType] = get_args(DatetimeType)\n\n\n@cache_in_session_state\ndef fetch_columns(table: str) -> Dict[str, DataType]:\n    return {\n        row.column_name: json.loads(row.data_type)["type"]\n        for row in snowflake_session().sql(f"show columns in table {table}").collect()\n    }\n\n\n@cache_in_session_state\ndef preview_table(table: str, limit: int = 5) -> pd.DataFrame:\n    return pd.DataFrame(snowflake_session().table(table).limit(limit).collect())\n\n\nclass Filters(BaseModel):\n    mapping: Dict[str, Tuple[str, ...]] = Field(default_factory=dict)\n\n    def add(self, key: str, values: List[str]) -> None:\n        self.mapping[key] = tuple(values)\n\n    def __hash__(self) -> int:\n        return hash(frozenset(self.mapping.items()))\n\n    def to_condition(self) -> f.Column:\n        """Return a Snowpark filter condition to be used in the `DataFrame.filter` or `DataFrame.where` methods."""\n        condition = f.lit(True)\n        for column, values in self.mapping.items():\n            condition &= f.col(column).in_(values)\n        return condition\n\n\ndef drop_temporary_tables() -> None:\n    for table in fetch_tables.execute_without_cache():\n        if table.startswith("TMP_"):\n            snowflake_session().table(table).drop_table()\n\n    fetch_tables.clear_cache()\n')
    __stickytape_write_module('data/helpers.py', b'from typing import TYPE_CHECKING, List, Optional, Union, Tuple\n\nimport streamlit as st\nfrom inflection import pluralize as inflection_pluralize\nfrom snowflake.snowpark.exceptions import SnowparkSQLException\nfrom snowflake.snowpark.types import StringType, StructField, StructType\n\nfrom app_pages.page import button_go_to_page\nfrom range_analysis_app.db import snowflake_session\nfrom utils import pluralize\n\nif TYPE_CHECKING:\n    from data.repository import Repository\n    from data.models import DomaBaseModel\n\n\ndef create_table_if_not_exists() -> None:\n    session = snowflake_session()\n    try:\n        session.table("doma_entities").collect()\n    except SnowparkSQLException as e:\n        schema = StructType([StructField("json_data", StringType())])\n        df = session.create_dataframe([], schema=schema)\n        df.write.save_as_table("doma_entities", mode="overwrite")\n\n\ndef repository() -> "Repository":\n    if "repository" not in st.session_state:\n        from data.repository import Repository\n\n        create_table_if_not_exists()\n        row = snowflake_session().table("doma_entities").first()\n        # TODO insert {"dimensions": [{"uid": "e9196ec1-7ab1-47f8-b2f4-753c3a83add0", "name": "Product"}], "activities": [{"uid": "3c88a680-eb5c-46b3-a9ec-789a0bf994b8", "name": "Sales"}], "metrics": [{"uid": "d1ba885b-2ab2-4ad6-b06d-aa8d594ebb92", "name": "Revenue", "act_id": "3c88a680-eb5c-46b3-a9ec-789a0bf994b8", "columns": {"dimensions": ["P_NAME", "O_CUSTKEY"], "metric_column": "O_TOTALPRICE", "timestamp": "O_ORDERDATE", "agg_method": "sum", "labels": {"P_NAME": "P_NAME", "O_CUSTKEY": "O_CUSTKEY"}}}], "selected_tables": [{"uid": "f0e58641-0515-4b30-b0b2-1cc3dc4120be", "table_name": "PART", "table_schema": "TPCH_SF1", "table_database": "SNOWFLAKE_SAMPLE_DATA", "entity_uid": "e9196ec1-7ab1-47f8-b2f4-753c3a83add0", "entity_type": "dimension", "columns": {"P_PARTKEY": "FIXED", "P_NAME": "TEXT", "P_MFGR": "TEXT", "P_BRAND": "TEXT", "P_TYPE": "TEXT", "P_SIZE": "FIXED", "P_CONTAINER": "TEXT", "P_RETAILPRICE": "FIXED", "P_COMMENT": "TEXT"}}, {"uid": "b94340b0-a612-47cb-9992-be0cbfbb9e76", "table_name": "ORDERS", "table_schema": "TPCH_SF1", "table_database": "SNOWFLAKE_SAMPLE_DATA", "entity_uid": "3c88a680-eb5c-46b3-a9ec-789a0bf994b8", "entity_type": "activity", "columns": {"O_ORDERKEY": "FIXED", "O_CUSTKEY": "FIXED", "O_ORDERSTATUS": "TEXT", "O_TOTALPRICE": "FIXED", "O_ORDERDATE": "DATE", "O_ORDERPRIORITY": "TEXT", "O_CLERK": "TEXT", "O_SHIPPRIORITY": "FIXED", "O_COMMENT": "TEXT"}}], "dim_act_mappings": [{"dim_id": "e9196ec1-7ab1-47f8-b2f4-753c3a83add0", "act_id": "3c88a680-eb5c-46b3-a9ec-789a0bf994b8", "dimension_column": "P_PARTKEY", "activity_column": "O_ORDERKEY"}]}\n        if row is None:\n            st.session_state.repository = Repository()\n        else:\n            data = row.JSON_DATA\n            st.session_state["prev_repository_json"] = data\n            st.session_state.repository = Repository.parse_raw(data)\n\n    return st.session_state.repository\n\n\ndef save_repo_button(next_page: Optional[str] = None) -> None:\n    """Component for saving the repository and going to the next page."""\n    raw_json = repository().json()\n    prev_raw_json = st.session_state.get("prev_repository_json")\n\n    if prev_raw_json == raw_json:\n        return\n\n    st.write("-----")\n\n    placeholder = st.empty()\n\n    if placeholder.button("Save"):\n        with st.spinner():\n            schema = StructType([StructField("json_data", StringType())])\n            (\n                snowflake_session()\n                .create_dataframe([raw_json], schema=schema)\n                .write.save_as_table("doma_entities", mode="overwrite")\n            )\n\n        st.session_state["prev_repository_json"] = raw_json\n        if next_page:\n            with placeholder:\n                button_go_to_page(next_page, label=f"Go to next page: {next_page}")\n            st.info("Saved. Please go to next page", icon="\xe2\x9c\x94\xef\xb8\x8f")\n        else:\n            placeholder.empty()\n            st.info("Saved.", icon="\xe2\x9c\x94\xef\xb8\x8f")\n\n    else:\n        st.warning(\'You have unsaved changes, please press "Save" before going to next page\', icon="\xe2\x9a\xa0\xef\xb8\x8f")\n\n\ndef validate_state(\n        state: List[Union["DomaBaseModel"]],\n        entity_name: str,\n        permit_empty: bool = False,\n        validate_each: bool = True,\n) -> Tuple[bool, str]:\n    if len(state) == 0 and not permit_empty:\n        return False, f"{entity_name} cannot be empty"\n    else:\n        if validate_each:\n            try:\n                success_message = [f"{pluralize(len(state), entity_name)} has been set"]\n                for x in state:\n                    x.validate_entity()\n                    success_message.append(f"- {x.validation_item_name()}")\n                return True, "\\n".join(success_message)\n            except ValueError as e:\n                return False, str(e)\n        else:\n            return True, "\\n".join(\n                [f"All set for the {inflection_pluralize(entity_name)}"] +\n                [f"- {x.validation_item_name()}" for x in state]\n            )\n\n\ndef render_validation_component(is_success: bool, message: str):\n    if is_success:\n        st.success(message, icon="\xe2\x9c\x94\xef\xb8\x8f")\n    else:\n        st.error(message, icon="\xe2\x9c\x96\xef\xb8\x8f")\n')
    __stickytape_write_module('data/repository.py', b'from typing import TYPE_CHECKING, Dict, List, Optional, Union\n\nfrom pydantic import Field\nfrom pydantic.main import BaseModel\nfrom typing_extensions import Annotated\n\nfrom data.metric import Metric\nfrom data.models import Activity, DbTable, Dimension, DimensionActivityMapping, Entity, EntityType, Id\nfrom range_analysis_app.exploration.range_comparison_exploration import RangeComparisonVisualization\nfrom range_analysis_app.exploration.time_series_exploration import TimeSeriesVisualization\n\nENTITY_TYPE_TO_COLLECTION: Dict[EntityType, str] = {\n    "dimension": "dimensions",\n    "activity": "activities",\n    "metric": "metrics",\n}\n\nVisualization = Annotated[\n    Union[TimeSeriesVisualization, RangeComparisonVisualization],\n    Field(..., discriminator="viz_type"),\n]\n\n\nclass Repository(BaseModel):\n    dimensions: List[Dimension] = []\n    activities: List[Activity] = []\n    metrics: List[Metric] = []\n    selected_tables: List[DbTable] = []\n    dim_act_mappings: List[DimensionActivityMapping] = []\n    visualizations: Dict[int, Visualization] = {}\n\n    def get_entity(self, entity_type: EntityType, uid: Id) -> Optional[Entity]:\n        collection_name = ENTITY_TYPE_TO_COLLECTION.get(entity_type)\n        if collection_name is None:\n            return None\n\n        for e in getattr(self, collection_name):\n            if e.uid == uid:\n                return e\n        return None\n\n    def get_mapping(self, act: Activity, dim: Dimension) -> Optional[DimensionActivityMapping]:\n        for mapping in self.dim_act_mappings:\n            if mapping.dim_id == dim.uid and mapping.act_id == act.uid:\n                return mapping\n        return None\n')
    __stickytape_write_module('data/metric.py', b'from typing import Dict, List, Literal, Optional, Set, get_args\n\nimport snowflake.snowpark.functions as f\nfrom pydantic import BaseModel, Field\nfrom snowflake import snowpark\n\nfrom data.helpers import repository\nfrom data.models import Activity, Dimension, Entity, EntityType\nfrom range_analysis_app import db\nfrom range_analysis_app.caching import cache_in_session_state\nfrom range_analysis_app.definitions.definition import hash_json\n\nAggMethod = Literal["sum", "count", "avg"]\nAGG_METHODS = get_args(AggMethod)\n\n\nclass MetricColumns(BaseModel):\n    __hash__ = hash_json\n\n    dimensions: List[str]\n    metric_column: str\n    timestamp: str\n    agg_method: Literal["sum", "count", "avg"]\n\n    labels: Dict[str, str] = Field(default_factory=dict)\n\n    def remove_dimension_columns(self, dim_columns: Set[str]) -> None:\n        for dim_col in set(self.dimensions) & dim_columns:\n            self.dimensions.remove(dim_col)\n            self.labels.pop(dim_col, None)\n\n\nclass Metric(Entity):\n    act_id: Optional[str]\n    columns: Optional[MetricColumns]\n\n    @classmethod\n    def get_type(cls) -> EntityType:\n        return "metric"\n\n    def validate_entity(self) -> None:\n        if not bool(self.columns):\n            raise ValueError(f"Metric columns must be set.")\n\n    @staticmethod\n    def create(name: str, act_id: Optional[str] = None) -> "Metric":\n        met = Metric(name=name, act_id=act_id)\n        repository().metrics.append(met)\n        return met\n\n    def delete(self) -> None:\n        repository().metrics.remove(self)\n\n    @property\n    def activity(self) -> Optional[Activity]:\n        return repository().get_entity("activity", self.act_id)\n\n    def unassign_dimension(self, dimension: Dimension):\n        dim_table = dimension.db_table\n        if not self.columns or not dim_table:\n            return\n\n        dim_columns = set(dim_table.columns.keys())\n        self.columns.remove_dimension_columns(dim_columns)\n\n    @cache_in_session_state\n    def dataframe(self) -> snowpark.DataFrame:\n        """Return a dataframe based on the metric activity joined with its dimensions."""\n        if not self.columns:\n            raise ValueError(f"No columns in metric {self.name}.")\n\n        if self.activity.validate_tables_and_mapping():\n            raise ValueError(f"Missing tables and mappings in activity {self.activity.name}.")\n\n        act_table = db.snowflake_session().table(self.activity.db_table.full_name)\n        snowpark_tables = [act_table]\n\n        result_dataframe = act_table\n\n        for mapping in self.activity.mappings:\n            dim_table = db.snowflake_session().table(mapping.dimension.db_table.full_name)\n            snowpark_tables.append(dim_table)\n\n            result_dataframe = result_dataframe.join(\n                dim_table,\n                how="left",\n                on=act_table.col(mapping.activity_column) == dim_table.col(mapping.dimension_column),\n            )\n\n        def col_ref(column_name: str) -> snowpark.Column:\n            """Return a reference for the column in one of the given tables eg. my_table.my_column"""\n            for table in snowpark_tables:\n                if column_name in table.columns:\n                    return table.col(column_name)\n\n        cols = self.columns\n\n        return result_dataframe.select(\n            [\n                *(col_ref(col).alias(col) for col in cols.dimensions),\n                f.to_date(col_ref(cols.timestamp)).alias(cols.timestamp),\n                col_ref(cols.metric_column).alias(cols.metric_column),\n            ]\n        )\n\n    @cache_in_session_state\n    def dimension_values(self, dimension: str, limit: int = 1000) -> List[str]:\n        assert dimension in self.columns.dimensions\n\n        rows = (\n            self.dataframe()\n            .group_by(dimension)\n            .agg(f.sum(self.columns.metric_column).alias("total"))\n            .sort(f.col("total").desc_nulls_last())\n            .limit(limit)\n            .collect()\n        )\n        return [row[dimension.upper()] for row in rows if row[dimension.upper()] is not None]\n')
    __stickytape_write_module('data/models.py', b'from abc import ABC, abstractmethod\nfrom typing import TYPE_CHECKING, Dict, List, Literal, NewType, Optional\nfrom uuid import uuid4\n\nfrom pydantic import BaseModel, Field\n\nfrom data.helpers import repository\nfrom range_analysis_app.db import snowflake_session\nfrom range_analysis_app.definitions.definition import hash_json\n\nif TYPE_CHECKING:\n    from data.metric import Metric\n\nId = NewType("Id", str)\n\nEntityType = Literal["dimension", "activity", "metric"]\n\n\ndef get_uuid4() -> Id:\n    return str(uuid4())\n\n\nclass DomaBaseModel(BaseModel, ABC):\n    @abstractmethod\n    def validate_entity(self) -> None:\n        ...\n\n    @abstractmethod\n    def validation_item_name(self) -> str:\n        ...\n\n\nclass Entity(DomaBaseModel, ABC):\n    uid: Id = Field(default_factory=get_uuid4)\n    name: str\n\n    __hash__ = hash_json\n\n    @classmethod\n    @abstractmethod\n    def get_type(cls) -> EntityType:\n        ...\n\n    def validate_entity(self) -> None:\n        if not bool(self.name):\n            raise ValueError(f"An entity name must be set.")\n\n    def validation_item_name(self) -> str:\n        return self.name\n\n\nclass DimensionActivityMapping(DomaBaseModel):\n    dim_id: Id\n    act_id: Id\n    dimension_column: Optional[str]\n    activity_column: Optional[str]\n\n    @property\n    def dimension(self) -> Optional["Dimension"]:\n        return repository().get_entity("dimension", self.dim_id)\n\n    @property\n    def activity(self) -> Optional["Activity"]:\n        return repository().get_entity("activity", self.act_id)\n\n    def validate_entity(self) -> None:\n        if not bool(self.dimension_column):\n            raise ValueError(f"A dimension column must be set.")\n\n        if not bool(self.activity_column):\n            raise ValueError(f"An activity column must be set.")\n\n    def validation_item_name(self) -> str:\n        return f"Dim. {self.dimension_column} <> Act. {self.activity_column}"\n\n\nclass Dimension(Entity):\n    @classmethod\n    def get_type(cls) -> EntityType:\n        return "dimension"\n\n    @staticmethod\n    def create(name: str) -> "Dimension":\n        dim = Dimension(name=name)\n        repository().dimensions.append(dim)\n        return dim\n\n    def delete(self) -> None:\n        repo = repository()\n        for activity in self.activities:\n            activity.unassign_dimension(self)\n\n        if table := self.db_table:\n            table.delete()\n\n        repo.dimensions.remove(self)\n\n    @property\n    def activities(self) -> List["Activity"]:\n        act_ids = [mapping.act_id for mapping in repository().dim_act_mappings if mapping.dim_id == self.uid]\n        return [act for act in repository().activities if act.uid in act_ids]\n\n    @property\n    def db_table(self) -> Optional["DbTable"]:\n        for table in repository().selected_tables:\n            if table.entity_uid == self.uid and table.entity_type == "dimension":\n                return table\n\n    def validate_entity(self) -> None:\n        if self.db_table is None:\n            raise ValueError(f"Missing table for dimension {self.name}")\n\n\nclass Activity(Entity):\n    @classmethod\n    def get_type(cls) -> EntityType:\n        return "activity"\n\n    @staticmethod\n    def create(name: str) -> "Activity":\n        act = Activity(name=name)\n        repository().activities.append(act)\n        return act\n\n    def delete(self) -> None:\n        for dimension in self.dimensions:\n            self.unassign_dimension(dimension)\n\n        if table := self.db_table:\n            table.delete()\n\n        for metric in self.metrics:\n            metric.delete()\n\n        repository().activities.remove(self)\n\n    @property\n    def dimensions(self) -> List[Dimension]:\n        dim_ids = [mapping.dim_id for mapping in repository().dim_act_mappings if mapping.act_id == self.uid]\n        return [dim for dim in repository().dimensions if dim.uid in dim_ids]\n\n    def assign_dimension(self, dim_id: Id) -> None:\n        mapping = DimensionActivityMapping(act_id=self.uid, dim_id=dim_id)\n        repository().dim_act_mappings.append(mapping)\n\n    def unassign_dimension(self, dim: Dimension) -> None:\n        repository().dim_act_mappings = [\n            mapping\n            for mapping in repository().dim_act_mappings\n            if mapping.act_id != self.uid or mapping.dim_id != dim.uid\n        ]\n        for metric in self.metrics:\n            metric.unassign_dimension(dim)\n\n    @property\n    def metrics(self) -> List["Metric"]:\n        return [metric for metric in repository().metrics if metric.act_id == self.uid]\n\n    @property\n    def mappings(self) -> List[DimensionActivityMapping]:\n        return [mapping for mapping in repository().dim_act_mappings if mapping.act_id == self.uid]\n\n    @property\n    def db_table(self) -> Optional["DbTable"]:\n        for table in repository().selected_tables:\n            if table.entity_uid == self.uid and table.entity_type == "activity":\n                return table\n\n    def validate_tables_and_mapping(self) -> List[str]:\n        """Return a list of error messages for each missing tables or mappings in the activity."""\n        errors = []\n        if self.db_table is None:\n            errors.append(f"Missing table for activity {self.name}")\n\n        for dim in self.dimensions:\n            if dim.db_table is None:\n                errors.append(f"Missing table for dimension {dim.name}")\n\n        for mapping in self.mappings:\n            if mapping.activity_column is None or mapping.dimension_column is None:\n                errors.append(f"Missing mapping between activity {self.name} and dimension {mapping.dimension.name}")\n\n        return errors\n\n\nclass DbTable(DomaBaseModel):\n    uid: Id = Field(default_factory=get_uuid4)\n\n    table_name: str\n    table_schema: str\n    table_database: str\n\n    entity_uid: Id\n    entity_type: EntityType\n\n    columns: Dict[str, str] = {}\n\n    @staticmethod\n    def create(\n        table_name: str,\n        table_schema: str,\n        table_database: str,\n        columns: Dict[str, str],\n        entity: Entity,\n    ) -> "DbTable":\n        entity_uid = entity.uid\n        entity_type = entity.get_type()\n        tbl = DbTable(\n            table_name=table_name,\n            table_schema=table_schema,\n            table_database=table_database,\n            columns=columns,\n            entity_uid=entity_uid,\n            entity_type=entity_type,\n        )\n        repository().selected_tables.append(tbl)\n        return tbl\n\n    @property\n    def full_name(self) -> str:\n        return f"{self.table_database}.{self.table_schema}.{self.table_name}"\n\n    def delete(self) -> None:\n        repository().selected_tables.remove(self)\n\n    def validate_entity(self) -> None:\n        if snowflake_session().table(self.full_name).first() is None:\n            raise ValueError(f"Table \'{self.full_name}\' is empty or does not exist.")\n\n    def validation_item_name(self) -> str:\n        return self.full_name\n')
    __stickytape_write_module('range_analysis_app/definitions/__init__.py', b'')
    __stickytape_write_module('range_analysis_app/definitions/definition.py', b'import hashlib\nfrom abc import abstractmethod\nfrom typing import Generic, Literal, TypeVar\n\nimport range_analysis_app.db as db\nimport snowflake.snowpark as snowpark\nfrom pydantic import BaseModel\n\n\ndef hash_json(model: BaseModel) -> int:\n    """Return a constant hash representing the given Pydantic model."""\n    return int(hashlib.sha1(model.json().encode()).hexdigest()[:10], 16)\n\n\nclass DataframeDefinition(BaseModel):\n    """Represent a Snowpark calculation resulting in a dataframe.\n\n    This definition is hashable to be able to cache the result in Streamlit.\n    """\n\n    __hash__ = hash_json\n\n    @abstractmethod\n    def dataframe(self) -> snowpark.DataFrame:\n        ...\n\n\nTDefinition = TypeVar("TDefinition", bound=DataframeDefinition)\n\n\nclass ResultTable(Generic[TDefinition], BaseModel):\n    """Table stored from a Snowpark Dataframe."""\n\n    __hash__ = hash_json\n\n    table_name: str\n    definition: TDefinition\n\n    def table(self) -> snowpark.Table:\n        return db.snowflake_session().table(self.table_name)\n\n\ndef generate_table(\n    definition: TDefinition,\n    table_name: str,\n    table_type: Literal["", "temporary"] = "",\n) -> ResultTable[TDefinition]:\n    """Generate a table for the given dataframe and return the table name."""\n    definition.dataframe().write.save_as_table(table_name, table_type=table_type, mode="overwrite")\n    return ResultTable(table_name=table_name, definition=definition)\n')
    __stickytape_write_module('range_analysis_app/exploration/__init__.py', b'')
    __stickytape_write_module('range_analysis_app/exploration/range_comparison_exploration.py', b'import calendar\nfrom typing import Literal\n\nimport pandas as pd\nimport plotly.graph_objects as go\nimport snowflake.snowpark.functions as f\nimport streamlit as st\n\nfrom data.metric import Metric\nfrom range_analysis_app.caching import cache_in_session_state\nfrom range_analysis_app.definitions.period import Period\nfrom range_analysis_app.definitions.range_comparison import RANGE_PERIODICITIES, RangeComparison\nfrom range_analysis_app.exploration.time_series_exploration import multiselect_dimension_filters\nfrom range_analysis_app.exploration.visualization import VisualizationModel\n\n\ndef range_comparison_exploration(metric: Metric) -> None:\n    """Component to create a range comparison and visualize it."""\n    left, right = st.columns([1, 3])\n\n    with left:\n        comparison = generate_range_comparison(metric)\n\n        viz = RangeComparisonVisualization(comparison=comparison)\n        viz.button_add_to_dashboard()\n\n    with right:\n        viz.visualize()\n\n\ndef generate_range_comparison(metric: Metric) -> RangeComparison:\n    range_periodicity = st.selectbox("Range Periodicity", RANGE_PERIODICITIES, index=RANGE_PERIODICITIES.index("month"))\n\n    dates = fetch_dates(metric)\n    last_year = pd.Period(dates.end.year, freq="Y") - 1\n\n    current_period_date = st.date_input("Current Period Date", dates.end)\n    historical_start, historical_end = st.date_input(\n        "Historical Dates", (last_year.start_time.date(), last_year.end_time.date())\n    )\n\n    filters = multiselect_dimension_filters(metric)\n\n    return RangeComparison(\n        metric=metric,\n        range_periodicity=range_periodicity,\n        historical_dates=Period(start=pd.Timestamp(historical_start), end=pd.Timestamp(historical_end)),\n        current_period_date=pd.Timestamp(current_period_date),\n        filters=filters,\n    )\n\n\n@cache_in_session_state\ndef fetch_ranges(comparison: RangeComparison) -> pd.DataFrame:\n    return pd.DataFrame(\n        comparison.dataframe()\n        .select(\n            "range_period",\n            "current_value",\n            "minimum",\n            "lower_quartile",\n            "median",\n            "upper_quartile",\n            "maximum",\n        )\n        .sort("range_period")\n        .collect()\n    ).rename(columns=str.lower)\n\n\n@cache_in_session_state\ndef fetch_dates(metric: Metric) -> Period:\n    row = (\n        metric.dataframe()\n        .select(\n            f.min(metric.columns.timestamp).alias("min_date"),\n            f.max(metric.columns.timestamp).alias("max_date"),\n        )\n        .first()\n    )\n\n    return Period(start=row.MIN_DATE, end=row.MAX_DATE)\n\n\nclass RangeComparisonVisualization(VisualizationModel):\n    viz_type: Literal["range_comparison"] = "range_comparison"\n    comparison: RangeComparison\n\n    def visualize(self) -> None:\n        """Visualize the range comparison with a boxplots of historical ranges and line of current range."""\n        ranges = fetch_ranges(self.comparison)\n\n        if len(ranges) == 0:\n            st.text("No result on this date range")\n            return\n\n        range_periodicity = self.comparison.range_periodicity\n\n        PERIOD_NAMES = {\n            "day_of_week": [\n                "Sunday",\n                "Monday",\n                "Tuesday",\n                "Wednesday",\n                "Thursday",\n                "Friday",\n                "Saturday",\n            ],\n            "month": calendar.month_name,\n        }\n        if range_periodicity in PERIOD_NAMES:\n            ranges = ranges.assign(\n                range_period=ranges["range_period"].map(dict(enumerate(PERIOD_NAMES[range_periodicity])))\n            )\n\n        dimensions = " - ".join(", ".join(values) for values in self.comparison.filters.mapping.values()) or "All"\n        metric_label = self.comparison.metric.name.capitalize()\n\n        fig = go.Figure(\n            layout=dict(\n                title=f"Time series - {metric_label} - {dimensions}",\n                xaxis_title=range_periodicity,\n                yaxis_title=metric_label,\n                legend_title="Legend",\n                template="plotly_dark",\n                width=1000,\n                height=600,\n            ),\n        )\n        fig.update_xaxes(type="category")\n        fig.add_trace(\n            go.Box(\n                name="Historical",\n                x=ranges["range_period"],\n                lowerfence=ranges["minimum"],\n                q1=ranges["lower_quartile"],\n                median=ranges["median"],\n                q3=ranges["upper_quartile"],\n                upperfence=ranges["maximum"],\n            )\n        )\n        fig.add_trace(\n            go.Scatter(\n                name="Current",\n                x=ranges["range_period"],\n                y=ranges["current_value"],\n                mode="markers",\n                marker_size=10 if len(ranges) < 100 else 6,\n            )\n        )\n\n        st.plotly_chart(fig, use_container_width=True)\n')
    __stickytape_write_module('range_analysis_app/definitions/period.py', b'import datetime\nfrom typing import Literal\n\nimport pandas as pd\nfrom pydantic import BaseModel\n\nPeriodFrequency = Literal["W", "M", "Y", "year-weeks"]\n\n\nclass Period(BaseModel):\n    """Period of time between two dates."""\n\n    start: datetime.date\n    end: datetime.date\n\n    @staticmethod\n    def from_pandas(period: pd.Period) -> "Period":\n        return Period(start=period.start_time, end=period.end_time)\n\n    @staticmethod\n    def from_date(date: datetime.date, freq: PeriodFrequency) -> "Period":\n        if freq == "year-weeks":\n            return period_of_year_weeks(date.year)\n\n        return Period.from_pandas(pd.Timestamp(date).to_period(freq))\n\n\ndef period_of_year_weeks(year: int) -> Period:\n    """Return the start and end date of the first and last weeks of the given year."""\n    year_period = pd.Period(year, freq="Y")\n\n    first_week = year_period.start_time.to_period("W")\n    last_week = year_period.end_time.to_period("W")\n\n    if first_week.week != 1:  # the last week of previous year\n        first_week += 1\n\n    if last_week.week == 1:  # the first week of next year\n        last_week -= 1\n\n    return Period(start=first_week.start_time.floor("D"), end=last_week.end_time.floor("D"))\n')
    __stickytape_write_module('range_analysis_app/definitions/range_comparison.py', b'from datetime import date\nfrom typing import Any, Dict, List, Literal, get_args\n\nimport pandas as pd\nimport snowflake.snowpark as snowpark\nimport snowflake.snowpark.functions as f\n\nfrom data.metric import Metric\nfrom range_analysis_app import db\nfrom range_analysis_app.definitions.definition import DataframeDefinition\nfrom range_analysis_app.definitions.period import Period, PeriodFrequency\nfrom range_analysis_app.definitions.time_series import TimeSeries\n\nRangePeriodicity = Literal["day_of_month", "day_of_week", "day_of_year", "week", "month", "quarter"]\nRANGE_PERIODICITIES = get_args(RangePeriodicity)\n\n\nclass RangeComparison(DataframeDefinition):\n    metric: Metric\n    range_periodicity: RangePeriodicity\n    historical_dates: Period\n    current_period_date: date\n    filters: db.Filters\n\n    def dataframe(self) -> snowpark.DataFrame:\n        metric_col = self.metric.columns.metric_column\n        historical_ranges = aggregate_over_ranges(\n            self.metric,\n            self.range_periodicity,\n            date_range=self.historical_dates,\n            aggregations=[\n                f.min(metric_col).alias("minimum"),\n                f.percentile_cont(0.25).within_group(metric_col).alias("lower_quartile"),\n                f.median(metric_col).alias("median"),\n                f.percentile_cont(0.75).within_group(metric_col).alias("upper_quartile"),\n                f.max(metric_col).alias("maximum"),\n            ],\n            filters=self.filters,\n        )\n\n        PERIOD_FREQUENCIES: Dict[RangePeriodicity, PeriodFrequency] = {\n            "day_of_week": "W",\n            "day_of_month": "M",\n            "day_of_year": "Y",\n            "week": "year-weeks",\n            "month": "Y",\n            "quarter": "Y",\n        }\n        current_period = Period.from_date(self.current_period_date, freq=PERIOD_FREQUENCIES[self.range_periodicity])\n\n        current_ranges = aggregate_over_ranges(\n            self.metric,\n            self.range_periodicity,\n            date_range=current_period,\n            aggregations=[\n                f.any_value(metric_col).alias("current_value"),\n            ],\n            filters=self.filters,\n        )\n\n        return historical_ranges.join(current_ranges, on="range_period", how="left")\n\n\ndef aggregate_over_ranges(\n    metric: Metric,\n    range_periodicity: RangePeriodicity,\n    date_range: Period,\n    aggregations: List[f.Column],\n    filters: db.Filters,\n) -> snowpark.DataFrame:\n    """Aggregate the given metric over the periods of a periodicity.\n\n    For example, to aggregate each month between 2010 and 2020, outputting 12 rows.\n    """\n    RANGE_PERIODICITIES_CONFIGS: Dict[RangePeriodicity, Dict[str, Any]] = {\n        "day_of_week": {"period_function": f.dayofweek, "grain": "day"},\n        "day_of_month": {"period_function": f.dayofmonth, "grain": "day"},\n        "day_of_year": {"period_function": f.dayofyear, "grain": "day"},\n        "week": {"period_function": f.weekofyear, "grain": "week"},\n        "month": {"period_function": f.month, "grain": "month"},\n        "quarter": {"period_function": f.quarter, "grain": "quarter"},\n    }\n\n    range_config = RANGE_PERIODICITIES_CONFIGS[range_periodicity]\n    period_column = range_config["period_function"]("period").alias("range_period")\n\n    cols = metric.columns\n    return (\n        TimeSeries(metric=metric, grain=range_config["grain"])\n        .dataframe()\n        .filter(filters.to_condition())\n        .filter(f.col("period").between(date_range.start, date_range.end))\n        .select([cols.metric_column, period_column])\n        .group_by([period_column.get_name()])\n        .agg(aggregations)\n        .sort(period_column.get_name())\n    )\n')
    __stickytape_write_module('range_analysis_app/definitions/time_series.py', b'from typing import Literal, get_args\n\nimport snowflake.snowpark as snowpark\nimport snowflake.snowpark.functions as f\n\nfrom data.metric import Metric\nfrom range_analysis_app.definitions.definition import DataframeDefinition\n\nGrain = Literal["day", "week", "month", "quarter", "year"]\nGRAINS = get_args(Grain)\n\n\nclass TimeSeries(DataframeDefinition):\n    metric: Metric\n    grain: Grain\n\n    def dataframe(self) -> snowpark.DataFrame:\n        cols = self.metric.columns\n        period_column = f.date_trunc(self.grain, cols.timestamp).alias("period")\n\n        return (\n            self.metric.dataframe()\n            .select(cols.dimensions + [cols.metric_column, period_column])\n            .group_by(cols.dimensions + [period_column.get_name()])\n            .agg(getattr(f, cols.agg_method)(cols.metric_column).alias(cols.metric_column))\n        )\n')
    __stickytape_write_module('range_analysis_app/exploration/time_series_exploration.py', b'from typing import Literal\n\nimport pandas as pd\nimport plotly.graph_objects as go\nimport snowflake.snowpark.functions as f\nimport streamlit as st\n\nimport range_analysis_app.db as db\nfrom data.metric import Metric\nfrom range_analysis_app.caching import cache_in_session_state\nfrom range_analysis_app.definitions.time_series import GRAINS, TimeSeries\nfrom range_analysis_app.exploration.visualization import VisualizationModel\n\n\ndef time_series_exploration(metric: Metric) -> None:\n    """Component to create a time series and visualize it."""\n    left, right = st.columns([1, 3], gap="medium")\n\n    with left:\n        grain = st.selectbox("Periodicity", GRAINS)\n        time_series = TimeSeries(metric=metric, grain=grain)\n\n        filters = multiselect_dimension_filters(metric)\n        viz = TimeSeriesVisualization(time_series=time_series, filters=filters)\n        viz.button_add_to_dashboard()\n\n    with right:\n        viz.visualize()\n\n\nclass TimeSeriesVisualization(VisualizationModel):\n    viz_type: Literal["time_series"] = "time_series"\n    time_series: TimeSeries\n    filters: db.Filters\n\n    def visualize(self) -> None:\n        df = fetch_time_series(self.time_series, self.filters)\n\n        metric = self.time_series.metric\n        metric_column = metric.columns.metric_column.lower()\n\n        dimensions = " - ".join(", ".join(values) for values in self.filters.mapping.values()) or "All"\n        title = f"Time series - {metric.name.capitalize()} - {dimensions}"\n        y_axis_title = metric.name.capitalize()\n        x_axis_title = ""\n\n        fig = (\n            go.Figure()\n            .add_trace(\n                go.Scatter(\n                    name="Values",\n                    x=df["period"],\n                    y=df[metric_column],\n                    hovertemplate="<b>%{x}</b><br>%{fullData.name}: %{y}<extra></extra>",\n                    marker={"line": {"width": 0}, "size": 6},\n                    line={"width": 1},\n                    mode="lines+markers",\n                )\n            )\n            .update_layout(\n                {\n                    "title": {"text": title},\n                    "xaxis": {"title": {"text": x_axis_title}, "dtick": "M12", "showgrid": True},\n                    "yaxis": {"title": {"text": y_axis_title}, "showgrid": True},\n                    "height": 600,\n                }\n            )\n        )\n\n        st.plotly_chart(fig, use_container_width=True)\n\n\ndef render_min_max_metrics(metric: Metric, column_name: str):\n    df_min_max = metric.dataframe().select(\n        f.min(column_name).alias(f"{column_name}_min"),\n        f.max(column_name).alias(f"{column_name}_max"),\n    ).first()\n    st.caption(f"Min/Max: **{df_min_max[f\'{column_name}_MIN\']}** / **{df_min_max[f\'{column_name}_MAX\']}**")\n\n\ndef render_min_max_metrics_2(metric: Metric, column_name: str):\n    df_min_max = metric.dataframe().select(\n        f.min(column_name).alias(f"{column_name}_min"),\n        f.max(column_name).alias(f"{column_name}_max"),\n    ).first()\n    st.text_input("Min/Max", f"{df_min_max[f\'{column_name}_MIN\']} / {df_min_max[f\'{column_name}_MAX\']}", disabled=True)\n\n\n@cache_in_session_state\ndef fetch_time_series(time_series: TimeSeries, filters: db.Filters) -> pd.DataFrame:\n    metric_column = time_series.metric.columns.metric_column\n    return pd.DataFrame(\n        time_series.dataframe()\n        .filter(filters.to_condition())\n        .group_by("period")\n        .agg(f.sum(metric_column).alias(metric_column))\n        .sort("period")\n        .collect()\n    ).rename(columns=str.lower)\n\n\ndef multiselect_dimension_filters(metric: Metric) -> db.Filters:\n    st.markdown("##### Filters")\n\n    filters = db.Filters()\n    for dimension in metric.columns.dimensions:\n        label = metric.columns.labels.get(dimension, dimension)\n\n        values = st.multiselect(label, metric.dimension_values(dimension))\n        if values:\n            filters.add(dimension, values)\n\n    return filters\n')
    __stickytape_write_module('range_analysis_app/exploration/visualization.py', b'from abc import abstractmethod\n\nimport streamlit as st\nfrom pydantic import BaseModel\n\nfrom data.helpers import repository\nfrom range_analysis_app.definitions.definition import hash_json\n\n\nclass VisualizationModel(BaseModel):\n    __hash__ = hash_json\n\n    @abstractmethod\n    def visualize():\n        ...\n\n    def button_add_to_dashboard(self) -> None:\n        """button to add the current visualization to the dashboard."""\n        if hash(self) in repository().visualizations:\n            st.info("Added in dashboard")\n            return\n\n        def add_to_dashboard():\n            repository().visualizations[hash(self)] = self\n\n        st.button("Add to dashboard", on_click=add_to_dashboard)\n')
    __stickytape_write_module('range_analysis_app/components/collapse.py', b'from typing import Optional\n\nimport streamlit as st\n\n\ndef collapse(label: str = "", open_by_default: str = False, key: Optional[str] = None) -> bool:\n    if not key:\n        key = f"collapse-{label}"\n\n    is_open = st.session_state.get(key, open_by_default)\n\n    def switch() -> None:\n        st.session_state[key] = not is_open\n\n    caret = "\xe2\xac\x86\xef\xb8\x8f" if is_open else "\xe2\xac\x87\xef\xb8\x8f"\n    st.button(f"{caret} {label}", on_click=switch, key=f"btn-{key}")\n\n    return is_open\n')
    __stickytape_write_module('app_pages/doma_entities_page.py', b'from typing import Callable, List\n\nimport streamlit as st\nfrom streamlit.delta_generator import DeltaGenerator\nfrom streamlit.runtime.state import WidgetCallback\n\nfrom app_pages.page import Page, button_go_to_page\nfrom data.helpers import repository, save_repo_button\nfrom data.metric import Metric\nfrom data.models import Activity, Dimension, Entity, EntityType, Id\n\n\ndef render_dimensions(i: int, item: Dimension, item_type: EntityType) -> None:\n    col1, col2, col3 = st.columns([10, 1, 1])\n    with col1:\n        name_value = item.name\n        name_key = f"[{item_type}][{i}][name]"\n\n        def get_handler(dim: str, key: str) -> WidgetCallback:\n            def h() -> None:\n                dim.name = st.session_state[key]\n\n            return h\n\n        st.text_input(\n            name_key,\n            label_visibility="collapsed",\n            value=name_value,\n            key=name_key,\n            on_change=get_handler(item, name_key),\n        )\n    with col2:\n        st.write("")\n    return col3\n\n\ndef render_activities(i: int, item: Activity, item_type: EntityType) -> None:\n    col1, col2, col3, col4 = st.columns([5, 5, 1, 1])\n    with col1:\n        name_value = item.name\n        name_key = f"[{item_type}][{i}][name]"\n\n        def get_handler(act: Activity, key: str) -> WidgetCallback:\n            def h() -> None:\n                act.name = st.session_state[key]\n\n            return h\n\n        st.text_input(\n            name_key,\n            label_visibility="collapsed",\n            value=name_value,\n            key=name_key,\n            on_change=get_handler(item, name_key),\n        )\n\n    with col2:\n        selected_dimensions_key = f"[{item_type}][{i}][dimensions]"\n        selected_dimensions: List[Dimension] = item.dimensions\n\n        dim_map = {dim.uid: dim.name for dim in repository().dimensions}\n\n        options = [dim.uid for dim in repository().dimensions]\n\n        def format_dim(dim_uid: Id) -> str:\n            return dim_map[dim_uid]\n\n        selected_dim_ids = [dim.uid for dim in selected_dimensions]\n\n        def update_activity(dim_ids: List[Id]) -> None:\n            item_dim_ids = [dim.uid for dim in item.dimensions]\n            for dim_id in dim_ids:\n                if dim_id not in item_dim_ids:\n                    item.assign_dimension(dim_id)\n                    print("add")\n            for dim in [dim for dim in repository().dimensions]:\n                if dim.uid not in dim_ids:\n                    item.unassign_dimension(dim)\n                    print("remove", dim.uid)\n\n        st.multiselect(\n            selected_dimensions_key,\n            label_visibility="collapsed",\n            options=options,\n            default=selected_dim_ids,\n            key=selected_dimensions_key,\n            on_change=lambda: update_activity(st.session_state[selected_dimensions_key]),\n            format_func=format_dim,\n        )\n    with col3:\n        st.write("")\n    return col4\n\n\ndef render_metrics(i: int, item: Metric, item_type: EntityType) -> None:\n    col1, col2, col3, col4 = st.columns([5, 5, 1, 1])\n    with col1:\n        name_value = item.name\n        name_key = f"[{item_type}][{i}][name]"\n\n        def get_handler(m: Metric, key: str) -> WidgetCallback:\n            def h() -> None:\n                m.name = st.session_state[key]\n\n            return h\n\n        st.text_input(\n            name_key,\n            label_visibility="collapsed",\n            value=name_value,\n            key=name_key,\n            on_change=get_handler(item, name_key),\n        )\n    with col2:\n        selected_activity_key = f"[{item_type}][{i}][activity]"\n\n        act_map = {str(act.uid): act.name for act in repository().activities}\n\n        options = list(act_map.keys())\n\n        def format_act(act_uid: Id) -> str:\n            return act_map[act_uid]\n\n        def update_metric(act_id: Id) -> None:\n            item.act_id = act_id\n\n        st.selectbox(\n            label=selected_activity_key,\n            label_visibility="collapsed",\n            options=options,\n            format_func=format_act,\n            index=options.index(item.act_id) if item.act_id in options else 0,\n            key=selected_activity_key,\n            on_change=lambda: update_metric(st.session_state[selected_activity_key]),\n        )\n\n    with col3:\n        st.write("")\n    return col4\n\n\nRenderer = Callable[[int, Entity, EntityType], DeltaGenerator]\n\n\ndef render_crud(item_type: EntityType, items: List[Entity], render: Renderer, on_create: WidgetCallback) -> None:\n    for i, item in enumerate(items):\n        last_col = render(i, item, item_type)\n\n        def delete_item(entity: Entity) -> WidgetCallback:\n            return lambda: entity.delete()\n\n        last_col.button(f"Delete", key=f"{item_type}{i}_delete_button", on_click=delete_item(item))\n\n    st.button(f"Add {item_type.capitalize()}", on_click=on_create)\n\n\nclass DomaEntitiesPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "This page provides a brief introduction to the DOMA framework and the purpose of the app."\n\n    def run(self) -> None:\n        super().run()\n\n        st.title("Entities")\n        button_go_to_page("Home", label="Go Home")\n\n        st.markdown(\n            """\n            Dimensions are attributes that describe characteristics of an object. \n            They are used to segment and filter data to gain insights into specific aspects of your business.\n\n            Some examples of dimensions might include: User, Location, Product, Category\n            """\n        )\n\n        render_crud(\n            item_type="dimensions",\n            items=repository().dimensions,\n            render=render_dimensions,\n            on_create=lambda: Dimension.create(name=""),\n        )\n\n        st.markdown(\n            """\n            Activities are actions that occur within your business.\n            They are used to measure performance and identify areas for improvement.\n            \n            Some examples of activities might include:\n            Sales, Website Visits, Customer, Service Calls\n            """\n        )\n\n        activities = repository().activities\n        render_crud(\n            item_type="activities",\n            items=activities,\n            render=render_activities,\n            on_create=lambda: Activity.create(name=""),\n        )\n\n        st.markdown(\n            """\n            Metrics are numerical values that represent a specific aspect of your business performance.\n            They are calculated from a combination of dimensions and activities.\n            \n            Some examples of metrics might include: Revenue, Average Order Value, Conversion Rate\n            """\n        )\n        render_crud(\n            item_type="metrics",\n            items=repository().metrics,\n            render=render_metrics,\n            on_create=lambda: Metric.create(name="", act_id=activities[0].uid if len(activities) else None),\n        )\n\n        save_repo_button(next_page="RAW Data")\n')
    __stickytape_write_module('app_pages/mapping.py', b'import streamlit as st\n\nfrom app_pages.page import Page, button_go_to_page\nfrom data.helpers import repository, save_repo_button\nfrom data.models import Activity, DbTable, Dimension, DimensionActivityMapping\nfrom data.repository import Repository\n\n\nclass MappingPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "This page provides an allows to configure relations between Activities and Dimensions"\n\n    def run(self) -> None:\n        super().run()\n\n        st.title("Mapping")\n        button_go_to_page("Home", label="Go Home")\n\n        repo = repository()\n\n        if len(repo.selected_tables) == 0:\n            st.warning("No tables have been selected. Please go to the RAW data page to select tables.")\n            button_go_to_page("RAW Data", label="Go to RAW Data")\n            return\n\n        if len(repo.activities) == 0 or len(repo.dimensions) == 0:\n            st.warning(\n                "No activities or dimensions have been defined. "\n                "Please go to Entities to define activities and dimensions."\n            )\n            button_go_to_page("Entities", label="Go to Entities")\n            return\n\n        st.markdown(\n            """\n            On this page, you can configure the relations between Activities and Dimensions.\n\n            To set up the mappings:\n            1. Find the Activity and Dimension you want to configure under their respective headings.\n            2. For each Activity and Dimension pair, select the appropriate columns from the source tables.\n               These columns should contain matching data to establish the relationship.\n            """\n        )\n\n        for act_idx, activity in enumerate(repo.activities):\n            self.render_activity(repo, act_idx, activity)\n\n        save_repo_button(next_page="Metrics")\n\n    def render_activity(self, repo: Repository, act_idx: int, activity: Activity) -> None:\n        if act_idx:\n            st.write("---")\n        act_associated_table = activity.db_table\n        if act_associated_table is None:\n            return\n\n        st.markdown(f"### Activity {activity.name}:\\nSource table: `{act_associated_table.full_name}`")\n\n        st.write("#### Dimensions:")\n        for dim_idx, dimension in enumerate(activity.dimensions):\n            self.render_dimension(repo, act_idx, activity, act_associated_table, dim_idx, dimension)\n\n    def render_dimension(\n        self,\n        repo: Repository,\n        act_idx: int,\n        activity: Activity,\n        act_associated_table: DbTable,\n        dim_idx: int,\n        dimension: Dimension,\n    ) -> None:\n        dim_associated_table = dimension.db_table\n        mapping = repo.get_mapping(activity, dimension)\n        if dim_associated_table is None or mapping is None:\n            return\n        st.markdown(f"#### - {dimension.name}:\\nSource table: `{dim_associated_table.full_name}`")\n\n        # Add select to configure relation between tables\n        activity_columns = list(act_associated_table.columns.keys())\n        dimension_columns = list(dim_associated_table.columns.keys())\n\n        if mapping.activity_column is None:\n            mapping.activity_column = activity_columns[0]\n\n        if mapping.dimension_column is None:\n            mapping.dimension_column = dimension_columns[0]\n\n        def on_change_activity_column(m: DimensionActivityMapping, v: str) -> None:\n            m.activity_column = v\n\n        def on_change_dimension_column(m: DimensionActivityMapping, v: str) -> None:\n            m.dimension_column = v\n\n        col1, col2, col3 = st.columns([4, 0.5, 4])\n        with col1:\n            st.selectbox(\n                "Activity table column",\n                options=activity_columns,\n                index=activity_columns.index(mapping.activity_column),\n                key=f"activity_column_{act_idx}_{dim_idx}",\n                on_change=lambda: on_change_activity_column(\n                    mapping, st.session_state[f"activity_column_{act_idx}_{dim_idx}"]\n                ),\n            )\n        with col2:\n            st.write("")\n            st.write("\\n\\n\\n\\n## \xe2\x86\x94\xef\xb8\x8f")\n        with col3:\n            st.selectbox(\n                "Dimension table column",\n                options=dimension_columns,\n                index=dimension_columns.index(mapping.dimension_column),\n                key=f"dimension_column_{act_idx}_{dim_idx}",\n                on_change=lambda: on_change_dimension_column(\n                    mapping, st.session_state[f"dimension_column_{act_idx}_{dim_idx}"]\n                ),\n            )\n')
    __stickytape_write_module('app_pages/metrics_creation.py', b'from collections import ChainMap\nfrom typing import Dict, List, Optional\n\nimport streamlit as st\nfrom streamlit.runtime.state import WidgetCallback\n\nimport range_analysis_app.db as db\nfrom app_pages.page import Page, button_go_to_page\nfrom data.helpers import repository, save_repo_button\nfrom data.metric import AGG_METHODS, Metric, MetricColumns\nfrom range_analysis_app.components.collapse import collapse\nfrom range_analysis_app.exploration.time_series_exploration import render_min_max_metrics\nfrom utils import chunks\n\n\nclass MetricsCreationPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "This page provides a form where users can define metrics as answers to business questions."\n\n    def run(self) -> None:\n        super().run()\n\n        st.title("Metrics Creation Page")\n        button_go_to_page("Home", label="Go Home")\n\n        metrics = repository().metrics\n\n        for metric in metrics:\n            if metric != metrics[0]:\n                st.markdown("---")\n\n            form_metric_definition(metric)\n\n        save_repo_button(next_page="Business Use Cases")\n\n\ndef form_metric_definition(metric: Metric) -> None:\n    st.markdown(f"### Metric {metric.name.capitalize()}")\n\n    activity = metric.activity\n    errors = activity.validate_tables_and_mapping()\n    if errors:\n        for error in errors:\n            st.error(error)\n        return\n\n    columns: Dict[str, db.DataType] = {\n        **activity.db_table.columns,\n        **ChainMap(*[dim.db_table.columns for dim in activity.dimensions]),\n    }\n\n    def first_of_type(data_types: List[db.DataType]) -> Optional[str]:\n        """Return the first column having one of the given types."""\n        for col, data_type in columns.items():\n            if data_type in data_types:\n                return col\n\n    left, right = st.columns([3, 2])\n\n    with left:\n        if metric.columns is None:\n            metric.columns = MetricColumns(\n                dimensions=[],\n                metric_column=first_of_type(["FIXED", "REAL"]),\n                timestamp=first_of_type(db.DATETIME_TYPES),\n                agg_method="sum",\n            )\n\n        def set_columns_field(field: str, state_key: str) -> WidgetCallback:\n            return lambda: setattr(metric.columns, field, st.session_state[state_key])\n\n        st.markdown("##### Columns")\n\n        dimensions = st.multiselect(\n            "Dimensions",\n            columns,\n            default=metric.columns.dimensions,\n            key=f"dimensions-{metric.uid}",\n            on_change=set_columns_field("dimensions", f"dimensions-{metric.uid}"),\n        )\n\n        col1, col2, col3 = st.columns(3)\n        with col1:\n            st.selectbox(\n                "Metric",\n                columns,\n                index=list(columns).index(metric.columns.metric_column),\n                key=f"metric-column-{metric.uid}",\n                on_change=set_columns_field("metric_column", f"metric-column-{metric.uid}"),\n            )\n            render_min_max_metrics(metric, metric.columns.metric_column)\n\n        with col2:\n            st.selectbox(\n                "Timestamp",\n                columns,\n                index=list(columns).index(metric.columns.timestamp),\n                key=f"timestamp-{metric.uid}",\n                on_change=set_columns_field("timestamp", f"timestamp-{metric.uid}"),\n            )\n            render_min_max_metrics(metric, metric.columns.timestamp)\n\n        with col3:\n            st.selectbox(\n                "Aggregate Method",\n                AGG_METHODS,\n                index=AGG_METHODS.index(metric.columns.agg_method),\n                key=f"agg-method-{metric.uid}",\n                on_change=set_columns_field("agg_method", f"agg-method-{metric.uid}"),\n            )\n\n        if dimensions:\n            form_column_labels(metric, dimensions)\n\n    with right:\n        st.markdown("##")\n        if collapse("Preview metric table", key=f"preview-metric-{metric.uid}"):\n            st.dataframe(metric.dataframe().limit(5).to_pandas())\n\n\ndef form_column_labels(metric: Metric, dimensions: List[str]) -> None:\n    """Text inputs for editing column labels."""\n    st.markdown("##### Labels")\n\n    def set_label(dimension: str, key: str) -> WidgetCallback:\n        def update_labels() -> None:\n            if value := st.session_state[key].strip():\n                metric.columns.labels.update(**{dimension: value})\n            else:\n                metric.columns.labels.pop(dimension, None)\n\n        return update_labels\n\n    NB_COLS = 3\n    for dims in chunks(dimensions, chunk_size=NB_COLS):\n        for dim, col in zip(dims, st.columns([1] * NB_COLS)):\n            col.text_input(\n                f"Label for `{dim}`",\n                value=metric.columns.labels.get(dim, ""),\n                placeholder=dim,\n                key=f"label-{dim}-{metric.uid}",\n                on_change=set_label(dim, f"label-{dim}-{metric.uid}"),\n            )\n')
    __stickytape_write_module('app_pages/home_page.py', b'import streamlit as st\n\nfrom app_pages.dashboard import DashboardPage\nfrom app_pages.page import Page\nfrom data.helpers import render_validation_component, repository, save_repo_button, validate_state\n\n\nclass HomePage(Page):\n    @property\n    def page_description(self) -> str:\n        return "This is the home page"\n\n    def run(self) -> None:\n        super().run()\n\n        st.title("Maxa ERP Data DEMO")\n\n        st.markdown(\n            """\n            Welcome to the DOMA analysis app!\n            This page allows you to define dimensions, activities and metrics of your business\n            """\n        )\n\n        st.write("-----------------")\n        _repo = repository()\n\n        _repo_dimensions = validate_state(_repo.dimensions, "dimension", validate_each=False)\n        _repo_activities = validate_state(_repo.activities, "activity", validate_each=False)\n        _repo_metrics = validate_state(_repo.metrics, "metric", validate_each=False)\n        _repo_each_dimensions = validate_state(_repo.dimensions, "dimension", validate_each=True)\n        _repo_each_activities = validate_state(_repo.activities, "activity", validate_each=True)\n        _repo_dim_act_mappings = validate_state(_repo.dim_act_mappings, "mapping column", permit_empty=True)\n        _repo_each_metrics = validate_state(_repo.metrics, "metric", validate_each=True)\n\n        setup_completed = all(\n            [\n                _repo_dimensions[0],\n                _repo_activities[0],\n                _repo_metrics[0],\n                _repo_each_dimensions[0],\n                _repo_each_activities[0],\n                _repo_dim_act_mappings[0],\n                _repo_each_metrics[0],\n            ]\n        )\n\n        st.header("Setup")\n        with st.expander(\n            "Setup completed" if setup_completed else "Please complete setup", expanded=not setup_completed\n        ):\n            col_entities, col_raw_data, col_mapping, col_metrics = st.columns(4)\n\n            with col_entities:\n                st.subheader("1. Entities")\n                st.button(\n                    "Edit",\n                    key="Edit Entities",\n                    on_click=lambda: st.session_state.update(current_page="Entities"),\n                )\n\n                render_validation_component(*_repo_dimensions)\n                render_validation_component(*_repo_activities)\n                render_validation_component(*_repo_metrics)\n\n            with col_raw_data:\n                st.subheader("2. RAW ERP Data")\n                st.button(\n                    "Edit",\n                    key="Edit RAW Data",\n                    on_click=lambda: st.session_state.update(current_page="RAW Data"),\n                )\n\n                render_validation_component(*_repo_each_dimensions)\n                render_validation_component(*_repo_each_activities)\n\n            with col_mapping:\n                st.subheader("3. Mapping")\n                st.button(\n                    "Edit",\n                    key="Edit Mapping",\n                    on_click=lambda: st.session_state.update(current_page="Mapping"),\n                )\n\n                render_validation_component(*_repo_dim_act_mappings)\n\n            with col_metrics:\n                st.subheader("4. Metrics")\n                st.button(\n                    "Edit",\n                    key="Edit Metrics",\n                    on_click=lambda: st.session_state.update(current_page="Metrics"),\n                )\n\n                render_validation_component(*_repo_each_metrics)\n\n        st.write("-----------------")\n        # st.header("Dashboard")\n        DashboardPage().run()\n\n        save_repo_button(next_page="Data Loading")\n')
    __stickytape_write_module('app_pages/dashboard.py', b'import itertools\n\nimport streamlit as st\n\nfrom app_pages.page import Page, button_go_to_page\nfrom data.helpers import repository, save_repo_button\n\n\nclass DashboardPage(Page):\n    @property\n    def page_description(self) -> str:\n        return "Dashboard of saved visualizations."\n\n    def run(self) -> None:\n        st.header("Dashboard")\n\n        repo = repository()\n\n        if not repo.visualizations:\n            st.warning("Add visualizations to the dashboard in the Business Use Cases page.")\n            button_go_to_page("Business Use Cases", label="Go to Business Use Cases")\n            return\n\n        for index, (viz_key, visualization) in enumerate(repo.visualizations.items()):\n            if index != 0:\n                st.markdown("---")\n\n            visualization.visualize()\n\n            cols = st.columns([1] * 12)\n\n            def remove_viz(key: int) -> None:\n                return lambda: repo.visualizations.pop(key)\n\n            cols[0].button("Remove", key=f"remove-{viz_key}", on_click=remove_viz(viz_key))\n\n            def move_viz(key: int, index: int) -> None:\n                return lambda: move_visualization(key, index)\n\n            cols[1].button(\n                "\xe2\xac\x86\xef\xb8\x8f",\n                key=f"move-up-{viz_key}",\n                on_click=move_viz(viz_key, index - 1),\n                disabled=index == 0,\n            )\n            cols[2].button(\n                "\xe2\xac\x87\xef\xb8\x8f",\n                key=f"move-down-{viz_key}",\n                on_click=move_viz(viz_key, index + 1),\n                disabled=index == len(repo.visualizations) - 1,\n            )\n\n        save_repo_button()\n\n\ndef move_visualization(key: int, index: int) -> None:\n    """Move the visualization with the given key to the given index."""\n    visualizations = repository().visualizations\n    viz = visualizations.pop(key)\n\n    keys = list(visualizations.keys())\n    values = list(visualizations.values())\n\n    keys.insert(index, key)\n    values.insert(index, viz)\n\n    repository().visualizations = dict(zip(keys, values))\n')
    from pathlib import Path
    
    import streamlit as st
    
    # noinspection PyUnresolvedReferences
    import range_analysis_app.components.streamlit_nested_layout  # import for nested columns support
    from app_pages.data_analysis import DataAnalysisPage
    from app_pages.data_loading import DataLoadingPage
    from app_pages.doma_entities_page import DomaEntitiesPage
    from app_pages.home_page import HomePage
    from app_pages.mapping import MappingPage
    from app_pages.metrics_creation import MetricsCreationPage
    from data.helpers import repository
    from data.repository import Repository
    
    st.set_page_config(layout="wide")
    
    # Define the app's pages
    pages = {
        "Home": HomePage,
        "Entities": DomaEntitiesPage,
        "RAW Data": DataLoadingPage,
        "Mapping": MappingPage,
        "Metrics": MetricsCreationPage,
        "Business Use Cases": DataAnalysisPage,
    }
    # Define the sidebar navigation menu
    st.sidebar.title("Maxa ERP Data DEMO")
    selected_page = st.sidebar.selectbox(
        "Select page",
        list(pages),
        format_func=lambda name: f"{list(pages).index(name) + 1}. {name}",
        key="current_page",
        label_visibility="collapsed",
    )
    selected_page_obj = pages[selected_page]()
    st.sidebar.write(selected_page_obj.page_description)
    
    if st.sidebar.button("Reset to sample Data"):
        st.session_state.repository = Repository.parse_raw(
            '{"dimensions": [{"uid": "cf2f2e9e-d3ce-4fbd-8f50-81d6d2015f67", "name": "Supplier"}, {"uid": "52e75e69-d9c3-49e4-9510-106aa91ca5ad", "name": "Product"}, {"uid": "11729b5b-8403-431f-b38e-f1d0087e31e6", "name": "Customer"}], "activities": [{"uid": "421a4cb4-b8c5-4223-b7a5-b30a480288ad", "name": "Sales"}, {"uid": "41f1fbb9-1eb7-44cb-ad3b-0007c96b16af", "name": "Procurement"}], "metrics": [{"uid": "907a9d8f-9fa4-45cd-b6c1-894c544d166a", "name": "Shipped Quantities", "act_id": "41f1fbb9-1eb7-44cb-ad3b-0007c96b16af", "columns": {"dimensions": ["P_NAME", "S_NAME"], "metric_column": "L_QUANTITY", "timestamp": "L_SHIPDATE", "agg_method": "sum", "labels": {"P_NAME": "Product", "S_NAME": "Supplier"}}}, {"uid": "3d9a7c5a-7cce-4d2a-8185-8474947c4b40", "name": "Revenue", "act_id": "421a4cb4-b8c5-4223-b7a5-b30a480288ad", "columns": {"dimensions": ["C_NAME"], "metric_column": "O_TOTALPRICE", "timestamp": "O_ORDERDATE", "agg_method": "sum", "labels": {"C_NAME": "Customer"}}}], "selected_tables": [{"uid": "311d7842-a58e-424d-9589-1d79a11dc897", "table_name": "SUPPLIER", "table_schema": "TPCH_SF1", "table_database": "SNOWFLAKE_SAMPLE_DATA", "entity_uid": "cf2f2e9e-d3ce-4fbd-8f50-81d6d2015f67", "entity_type": "dimension", "columns": {"S_SUPPKEY": "FIXED", "S_NAME": "TEXT", "S_ADDRESS": "TEXT", "S_NATIONKEY": "FIXED", "S_PHONE": "TEXT", "S_ACCTBAL": "FIXED", "S_COMMENT": "TEXT"}}, {"uid": "c9a26abc-72cc-416f-a700-49d8a2a4e629", "table_name": "ORDERS", "table_schema": "TPCH_SF1", "table_database": "SNOWFLAKE_SAMPLE_DATA", "entity_uid": "421a4cb4-b8c5-4223-b7a5-b30a480288ad", "entity_type": "activity", "columns": {"O_ORDERKEY": "FIXED", "O_CUSTKEY": "FIXED", "O_ORDERSTATUS": "TEXT", "O_TOTALPRICE": "FIXED", "O_ORDERDATE": "DATE", "O_ORDERPRIORITY": "TEXT", "O_CLERK": "TEXT", "O_SHIPPRIORITY": "FIXED", "O_COMMENT": "TEXT"}}, {"uid": "62420350-bb74-48f3-9093-c94c1992799f", "table_name": "LINEITEM", "table_schema": "TPCH_SF1", "table_database": "SNOWFLAKE_SAMPLE_DATA", "entity_uid": "41f1fbb9-1eb7-44cb-ad3b-0007c96b16af", "entity_type": "activity", "columns": {"L_ORDERKEY": "FIXED", "L_PARTKEY": "FIXED", "L_SUPPKEY": "FIXED", "L_LINENUMBER": "FIXED", "L_QUANTITY": "FIXED", "L_EXTENDEDPRICE": "FIXED", "L_DISCOUNT": "FIXED", "L_TAX": "FIXED", "L_RETURNFLAG": "TEXT", "L_LINESTATUS": "TEXT", "L_SHIPDATE": "DATE", "L_COMMITDATE": "DATE", "L_RECEIPTDATE": "DATE", "L_SHIPINSTRUCT": "TEXT", "L_SHIPMODE": "TEXT", "L_COMMENT": "TEXT"}}, {"uid": "b5703d8c-0d84-4b18-b1f0-57cdb7501bf9", "table_name": "CUSTOMER", "table_schema": "TPCH_SF1", "table_database": "SNOWFLAKE_SAMPLE_DATA", "entity_uid": "11729b5b-8403-431f-b38e-f1d0087e31e6", "entity_type": "dimension", "columns": {"C_CUSTKEY": "FIXED", "C_NAME": "TEXT", "C_ADDRESS": "TEXT", "C_NATIONKEY": "FIXED", "C_PHONE": "TEXT", "C_ACCTBAL": "FIXED", "C_MKTSEGMENT": "TEXT", "C_COMMENT": "TEXT"}}, {"uid": "508d5382-e5df-4013-91eb-076b308c0520", "table_name": "PART", "table_schema": "TPCH_SF1", "table_database": "SNOWFLAKE_SAMPLE_DATA", "entity_uid": "52e75e69-d9c3-49e4-9510-106aa91ca5ad", "entity_type": "dimension", "columns": {"P_PARTKEY": "FIXED", "P_NAME": "TEXT", "P_MFGR": "TEXT", "P_BRAND": "TEXT", "P_TYPE": "TEXT", "P_SIZE": "FIXED", "P_CONTAINER": "TEXT", "P_RETAILPRICE": "FIXED", "P_COMMENT": "TEXT"}}], "dim_act_mappings": [{"dim_id": "cf2f2e9e-d3ce-4fbd-8f50-81d6d2015f67", "act_id": "41f1fbb9-1eb7-44cb-ad3b-0007c96b16af", "dimension_column": "S_SUPPKEY", "activity_column": "L_SUPPKEY"}, {"dim_id": "11729b5b-8403-431f-b38e-f1d0087e31e6", "act_id": "421a4cb4-b8c5-4223-b7a5-b30a480288ad", "dimension_column": "C_CUSTKEY", "activity_column": "O_CUSTKEY"}, {"dim_id": "52e75e69-d9c3-49e4-9510-106aa91ca5ad", "act_id": "41f1fbb9-1eb7-44cb-ad3b-0007c96b16af", "dimension_column": "P_PARTKEY", "activity_column": "L_PARTKEY"}], "visualizations": {"357309227134": {"viz_type": "time_series", "time_series": {"metric": {"uid": "907a9d8f-9fa4-45cd-b6c1-894c544d166a", "name": "Shipped Quantities", "act_id": "41f1fbb9-1eb7-44cb-ad3b-0007c96b16af", "columns": {"dimensions": ["P_NAME", "S_NAME"], "metric_column": "L_QUANTITY", "timestamp": "L_SHIPDATE", "agg_method": "sum", "labels": {"P_NAME": "Product", "S_NAME": "Supplier"}}}, "grain": "month"}, "filters": {"mapping": {"P_NAME": ["rose red purple salmon sandy"]}}}, "652236660276": {"viz_type": "range_comparison", "comparison": {"metric": {"uid": "907a9d8f-9fa4-45cd-b6c1-894c544d166a", "name": "Shipped Quantities", "act_id": "41f1fbb9-1eb7-44cb-ad3b-0007c96b16af", "columns": {"dimensions": ["P_NAME", "S_NAME"], "metric_column": "L_QUANTITY", "timestamp": "L_SHIPDATE", "agg_method": "sum", "labels": {"P_NAME": "Product", "S_NAME": "Supplier"}}}, "range_periodicity": "month", "historical_dates": {"start": "1997-01-01", "end": "1997-12-31"}, "current_period_date": "1998-12-01", "filters": {"mapping": {}}}}, "983770255227": {"viz_type": "time_series", "time_series": {"metric": {"uid": "3d9a7c5a-7cce-4d2a-8185-8474947c4b40", "name": "Revenue", "act_id": "421a4cb4-b8c5-4223-b7a5-b30a480288ad", "columns": {"dimensions": ["C_NAME"], "metric_column": "O_TOTALPRICE", "timestamp": "O_ORDERDATE", "agg_method": "sum", "labels": {"C_NAME": "Customer"}}}, "grain": "month"}, "filters": {"mapping": {}}}}}'
        )
    
    
    # Call the selected page's function
    selected_page_obj.run()
    